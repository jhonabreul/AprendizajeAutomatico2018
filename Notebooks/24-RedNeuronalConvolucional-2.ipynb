{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificar dígitos escritos a mano MNIST usando una Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de la Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figuras/ArquitecturaRNC_MNIST.png\" width=\"100%\">\n",
    "\n",
    "Como se puede observar, comenzamos con las imágenes de dígitos en escala de grises MNIST 28 × 28. Luego creamos 32, 5 × 5 filtros convolucionales/canales más activaciones de nodos ReLU. Después de esto, todavía tenemos una altura y un ancho de 28 nodos. A continuación, realizamos una reducción  aplicando una operación de reducción por valor máximo de 2 × 2 con una paso de 2. La capa segunda capa consiste en la misma estructura, pero ahora con 64 filtros/canales y otra reducción por valor máximo con paso de 2. A continuación, aplanamos la salida para obtener una capa completamente conectada con 3164 nodos, seguida de otra capa oculta de 1000 nodos. Estas capas usarán activaciones de nodo ReLU. Finalmente, usamos una capa de clasificación de softmax para dar salida a las probabilidades de los 10 dígitos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrada de datos y Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wladimir/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Hiperparámetors\n",
    "tasa_de_aprendizaje = 0.0001\n",
    "epocas = 10\n",
    "tamaño_lote = 50\n",
    "\n",
    "# declarar los placeholders de datos para el entrenamiento\n",
    "# entrada x - de 28 x 28 pixels = 784 - esta es la imagen aplanada obtenida de \n",
    "# mnist.train.nextbatch()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# reformatear dinamicamente la imagen a dos dimensiones\n",
    "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "# declarar el placeholder de la salida - 10 dígitos\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow tiene un cargador para los datos MNIST. Después de eso, tenemos algunas declaraciones de los hiperparámetros que determinan el comportamiento de optimización (tasa de aprendizaje, tamaño del lote, etc.). A continuación, declaramos un placeholder x para los datos de entrada de la imagen. Los datos de entrada de la imagen se extraerán utilizando la función `mnist.train.nextbatch()`, que proporciona una representación aplanada de la imagen en escala de grises de 28 × 28 = 784 nodos. Sin embargo, antes de poder utilizar estos datos en las funciones de convolución y reducción por valor máximo de TensorFlow, como `conv2d()` y `max_pool()` necesitamos reformar los datos ya que estas funciones solo toman datos 4D.\n",
    "\n",
    "El formato de los datos a suministrar es `[i, j, k, l]` donde `i` es el número de muestras de entrenamiento, `j` es la altura de la imagen, `k` es el peso y `l` es el número del canal. Debido a que tenemos una imagen en escala de grises, `l` siempre será igual a 1 (si tuviéramos una imagen RGB, sería igual a 3). Las imágenes MNIST son 28 x 28, por lo que `j` y `k` son iguales a 28. Cuando reformamos los datos de entrada `x` en `x_shaped`, teóricamente no conocemos el tamaño de la primera dimensión de x, por lo que no se sabe el valor de `i`. Sin embargo, `tf.reshape()` nos permite poner -1 en lugar de `i` y se cambiará dinámicamente en función del número de muestras de entrenamiento a medida que se realiza el entrenamiento. Entonces se usará `[-1, 28, 28, 1]` para el segundo argumento en `tf.reshape()`.\n",
    "\n",
    "Finalmente, necesitamos un placeholder para nuestros datos de entrenamiento de salida, que es un tensor de tamaño `[?, 10]`, donde el 10 representa los 10 dígitos posibles para clasificar. Utilizaremos el `mnist.train.next_batch()` para extraer las etiquetas de los dígitos como un vector único; en otras palabras, el dígito \"3\" se representará como `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir las Capas de Convolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que tenemos que crear un par de capas convolucionales, es mejor crear una función para reducir la repetición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_nueva_capa_conv(datos_entrada, num_canales_entrada, num_filtros, forma_filtro, forma_reducción, nombre):\n",
    "    # fijar la forma de los filtros de ebtrada para tf.nn.conv_2d\n",
    "    forma_filtro_conv = [forma_filtro[0], forma_filtro[1], num_canales_entrada,\n",
    "                      num_filtros]\n",
    "\n",
    "    # inicializar pesos y sesgos para el filtro\n",
    "    pesos = tf.Variable(tf.truncated_normal(forma_filtro_conv, stddev=0.03),\n",
    "                                      name=nombre+'_W')\n",
    "    sesgo = tf.Variable(tf.truncated_normal([num_filtros]), name=nombre+'_b')\n",
    "\n",
    "    # fijar la operación de la capa de convolución\n",
    "    capa_salida = tf.nn.conv2d(datos_entrada, pesos, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # agregar el sesgo\n",
    "    capa_salida += sesgo\n",
    "\n",
    "    # aplicar la funcion de activación no lineal ReLU\n",
    "    capa_salida = tf.nn.relu(capa_salida)\n",
    "\n",
    "    # realizar la reducción por valor máximo\n",
    "    tamaño = [1, forma_reducción[0], forma_reducción[1], 1]\n",
    "    pasos = [1, 2, 2, 1]\n",
    "    capa_salida = tf.nn.max_pool(capa_salida, ksize=tamaño, strides=pasos, \n",
    "                               padding='SAME')\n",
    "\n",
    "    return capa_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear dos Capas Convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear dos capas convolucionales\n",
    "capa1 = crear_nueva_capa_conv(x_shaped, 1, 32, [5, 5], [2, 2], nombre='capa1')\n",
    "capa2 = crear_nueva_capa_conv(capa1, 32, 64, [5, 5], [2, 2], nombre='capa2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capas Completamente Conectadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se discutió previamente, primero tenemos que aplanar la salida de la capa convolucional final. Ahora es una cuadrícula 7 × 7 de nodos con 64 canales, lo que equivale a 3136 nodos por muestra de entrenamiento. Podemos usar `tf.reshape()` para hacer lo que necesitamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplanada = tf.reshape(capa2, [-1, 7 * 7 * 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, tenemos una primera dimensión calculada dinámicamente (el -1 anterior), que corresponde al número de muestras de entrada en el lote de entrenamiento. A continuación, configuramos la primera capa completamente conectada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fijar los pesos y sesgos para esta capa, y después activar con ReLU\n",
    "wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
    "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
    "capa_densa1 = tf.matmul(aplanada, wd1) + bd1\n",
    "capa_densa1 = tf.nn.relu(capa_densa1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente estamos inicializando los pesos de la capa completamente conectada, multiplicándolos por la salida convolucional aplanada, y luego agregando un sesgo. Finalmente, se aplica una activación de ReLU. La siguiente capa está definida por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otra capa con activación softmax\n",
    "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
    "capa_densa2 = tf.matmul(capa_densa1, wd2) + bd2\n",
    "y_ = tf.nn.softmax(capa_densa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta capa se conecta a la salida y, por lo tanto, usamos una activación soft-max para producir los valores de salida predichos `y_`. Ahora hemos definido la estructura básica de nuestra red neuronal convolucional. Vamos a definir ahora la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función de costo de entropía cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos desarrollar nuestra propia función de costo de entropía cruzada, como hicimos anteriormente, basado en el valor `y_`. Sin embargo, debemos tener cuidado con el manejo de los valores de `NaN`. Afortunadamente, TensorFlow proporciona una función práctica que aplica soft-max seguida de pérdida de entropía cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropia_cruzada = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=capa_densa2, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `softmax_cross_entropy_with_logits()` toma dos argumentos: el primero (`logits`) es el resultado de la multiplicación de la matriz de la capa final (más sesgo) y el segundo es el vector objetivo de entrenamiento. La función primero toma el soft-max de la multiplicación de la matriz, luego lo compara con el objetivo de entrenamiento usando `cross-entropy`. El resultado es el cálculo de entropía cruzada por muestra de entrenamiento, por lo que debemos reducir este tensor en un escalar (un valor único). Para hacer esto usamos `tf.reduce_mean()` que toma la media del tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El entrenamiento de la red neuronal convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura esencial del entrenamiento de una red neuronal es:\n",
    "  - Crea un optimizador\n",
    "  - Crear operaciones predicción correcta y evaluación de precisión\n",
    "  - Inicializar las operaciones\n",
    "  - Determine el número de ejecuciones de lotes dentro de una época de entrenamiento\n",
    "  - Para cada época:\n",
    "    - Para cada lote:\n",
    "      - Extraiga los datos del lote\n",
    "      - Ejecute las operaciones optimizador y entropia_cruzada\n",
    "      - Agregar al costo promedio\n",
    "    - Calcule la precisión de la prueba actual\n",
    "    - Imprime algunos resultados\n",
    "  - Calcule la precisión de la prueba final e imprima\n",
    "\n",
    "El código para ejecutar esto es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1 costo = 0.782  exactitud prueba: 0.936\n",
      "Epoca: 2 costo = 0.154  exactitud prueba: 0.970\n",
      "Epoca: 3 costo = 0.095  exactitud prueba: 0.978\n",
      "Epoca: 4 costo = 0.070  exactitud prueba: 0.982\n",
      "Epoca: 5 costo = 0.057  exactitud prueba: 0.985\n",
      "Epoca: 6 costo = 0.049  exactitud prueba: 0.986\n",
      "Epoca: 7 costo = 0.041  exactitud prueba: 0.988\n",
      "Epoca: 8 costo = 0.035  exactitud prueba: 0.985\n",
      "Epoca: 9 costo = 0.029  exactitud prueba: 0.989\n",
      "Epoca: 10 costo = 0.027  exactitud prueba: 0.987\n",
      "\n",
      "Entrenamiento finalizado!\n",
      "0.9874\n"
     ]
    }
   ],
   "source": [
    "# agregar un optimizador\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=tasa_de_aprendizaje).minimize(entropia_cruzada)\n",
    "\n",
    "# definir la operación para evaluar la exactitud\n",
    "prediccion_correcta = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "exactitud = tf.reduce_mean(tf.cast(prediccion_correcta, tf.float32))\n",
    "\n",
    "# fijar la operación de inicialización\n",
    "inicializacion = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # inicializar las variables\n",
    "    sess.run(inicializacion)\n",
    "    total_lotes = int(len(mnist.train.labels) / tamaño_lote)\n",
    "    for epoca in range(epocas):\n",
    "        costo_promedio = 0\n",
    "        for i in range(total_lotes):\n",
    "            lote_x, lote_y = mnist.train.next_batch(batch_size=tamaño_lote)\n",
    "            _, c = sess.run([optimizador, entropia_cruzada], \n",
    "                            feed_dict={x: lote_x, y: lote_y})\n",
    "            costo_promedio += c / total_lotes\n",
    "        exactitud_prueba = sess.run(exactitud, \n",
    "                       feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Epoca:\", (epoca + 1), \"costo =\", \"{:.3f}\".format(costo_promedio), \" exactitud prueba: {:.3f}\".format(exactitud_prueba))\n",
    "\n",
    "    print(\"\\nEntrenamiento finalizado!\")\n",
    "    print(sess.run(exactitud, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
