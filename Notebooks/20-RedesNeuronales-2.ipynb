{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de las Redes Neuronales: Propagación Hacia Atrás \"Backpropagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se explicaran los conceptos de la función de costo logístico y el algoritmo de propagación hacia atrás \"Backpropagation\" que se implemantaron en la clase anterior para aprender los pesos de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular la función de costo logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de costo logístico que se implemento en el metodo `_calcular_costo` es la siguiente:\n",
    "\n",
    "$$J(\\boldsymbol{w})=-\\sum_{i=1}^{n}y^{[i]}log(a^{[i]})+(1-y^{[i]})log(1-a^{[i]})$$\n",
    "\n",
    "Donde, $a^{[i]}$ es la activación sigmoide del i-ésimo ejemplo del conjunto de datos, el cual se calculo en el paso de propagación hacia atrás\n",
    "\n",
    "$$a^{[i]}=\\phi(z^{[i]})$$\n",
    "\n",
    "ahora, se agrega el término de regularización, el cual permite reducir el grado de sobreajuste. El término de regularización L2 se define de la manera siguiente:\n",
    "\n",
    "$$L2=\\lambda\\parallel\\boldsymbol{w}\\parallel_2^2=\\lambda\\sum_{j=1}^{m}w_j^2$$\n",
    "\n",
    "Al agregar el término de regularización L2 a la función de costo se obtiene la siguiente ecuación:\n",
    "\n",
    "$$J(\\boldsymbol{w})=-\\left[\\sum_{i=1}^{n}y^{[i]}log(a^{[i]})+(1-y^{[i]})log(1-a^{[i]})\\right]+\\lambda\\parallel\\boldsymbol{w}\\parallel_2^2$$\n",
    "\n",
    "Debido a que se implementó el Perceptron Multicapa para realizar clasificación múltiple, la salida es un vector de $t$ elementos usando la codificación \"one-hot\", por ejemplo la salida de la capa de salida para un ejemplo de la clase 2:\n",
    "\n",
    "$$a^{(salida)}=\\left[\\\n",
    "\\begin{array}{ll}\n",
    "      0.1 \\\\\n",
    "      0.9 \\\\\n",
    "      \\vdots \\\\\n",
    "      0.3 \\\\\n",
    "\\end{array} \n",
    "\\right], \\ y = \\left[\\\n",
    "\\begin{array}{ll}\n",
    "      0 \\\\\n",
    "      1 \\\\\n",
    "      \\vdots \\\\\n",
    "      0 \\\\\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Por lo que es necesario generalizar la función de costo logístico a todas las $t$ unidades de activación de la red. Por lo que la función de costo sin el término de regularización es:\n",
    "\n",
    "$$J(\\boldsymbol{w})=-\\sum_{i=1}^{n}\\sum_{j=1}^{t}y^{[i]}log(a^{[i]}_j)+(1-y^{[i]}_j)log(1-a^{[i]}_j)$$\n",
    "\n",
    "El siguiente término de regularización generalizada puede parecer un poco complicado en principio, pero aquí solo estamos calculando la suma de todos los pesos de una capa $l$ (sin el término de sesgo):\n",
    "\n",
    "$$\\frac{\\lambda}{2}\\sum_{l=1}^{L-1}\\sum_{i=1}^{u_l}\\sum_{j=1}^{u_{l+1}}(w_{j,i}^{(l)})^2$$\n",
    "\n",
    "Quedando la función de costo logístico:\n",
    "\n",
    "$$J(\\boldsymbol{w})=-\\left[\\sum_{i=1}^{n}\\sum_{j=1}^{t}y^{[i]}log(a^{[i]}_j)+(1-y^{[i]}_j)log(1-a^{[i]}_j)\\right]+\\frac{\\lambda}{2}\\sum_{l=1}^{L-1}\\sum_{i=1}^{u_l}\\sum_{j=1}^{u_{l+1}}(w_{j,i}^{(l)})^2$$\n",
    "\n",
    "Recordar que nuestro objetivo es minimizar la función de costo $J(\\boldsymbol{W})$; por lo tanto, se necesita calcular la derivada parcial de los parámetros $\\boldsymbol{W}$ con respecto a cada peso para cada capa en la red:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{j,i}^{(l)}}J(\\boldsymbol{W})$$\n",
    "\n",
    "A continuación, se presenta el algoritmo de propagación hacia atrás \"backpropagation\", que permite calcular esas derivadas parciales para minimizar la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de Propagación Hacia Atrás \"Backpropagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esencia, podemos pensar en la propagación hacia atrás \"backpropagation\" como un enfoque computacionalmente muy eficiente para calcular las derivadas parciales de una función de costo compleja en redes neuronales multicapa. Aquí, nuestro objetivo es usar esas derivadas\n",
    "para aprender los coeficientes de peso para parametrizar una red neuronal multicapa.\n",
    "\n",
    "El desafío en la parametrización de redes neuronales es que se esta típicamente tratando con una gran cantidad de coeficientes de peso en un espacio de atributos de muchas dimensiones. En contraste con las funciones de costo de neuronales de una sola capa\n",
    "redes como Adaline o regresión logística, la superficie de error de una función de costo de una red neuronal no es convexa o lisa con respecto a los parámetros. Hay muchos baches en esta superficie de costo multi-dimensional (mínimos locales) que tenemos que superar para encontrar el mínimo de la función de costo.\n",
    "\n",
    "La regla de la cadena es un enfoque para calcular la derivada de una función anidada compleja, como $f(g(x))$, de la siguiente manera:\n",
    "\n",
    "$$\\frac{d}{dx}[f(g(x))]=\\frac{df}{dx}\\cdot\\frac{dg}{dx}$$\n",
    "\n",
    "De manera similar, se puede usar la regla de cadena para una composición de función arbitrariamente larga. Por ejemplo, supongamos que tenemos cinco funciones diferentes, $f(x)$, $g(x)$, $h(x)$, $u(x)$ y $v(x)$, y que $F$ sea la composición de la función: $F(x) = f(g(h(u(v(x)))))$. Aplicando la regla de cadena, podemos calcular la derivada de esta función de la siguiente manera:\n",
    "\n",
    "$$\\frac{dF}{dx}=\\frac{d}{dx}F(x)=\\frac{d}{dx}f(g(h(u(v(x)))))=\\frac{df}{dg}\\cdot\\frac{dg}{dh}\\cdot\\frac{dh}{du}\\cdot\\frac{du}{dv}\\cdot\\frac{dv}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando  Redes Neuronales usando Propagación Hacia Atrás \"Backpropagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, explicaremos la matemática de propagación hacia atrás \"backpropagation\" para entender cómo se pueden aprender los pesos en una red neuronal de manera muy eficiente.\n",
    "\n",
    "Anteriormente vimos como calcular el costo como la diferencia entre la activación de la capa de salida y y la etiqueta de clase objetivo. Ahora veremos como trabaja el algoritmo de propagación hacia atrás para actualizar los pesos en nuestro modelo de Perceptrón Multicapa. \n",
    "\n",
    "Primero necesitamos aplicar la propagación hacia adelante para obtener la activación de la capa de salida, que formulamos de la siguiente manera:\n",
    "\n",
    "$$\\boldsymbol{Z}^{(oculta)}=\\boldsymbol{A}^{(entrada)}\\boldsymbol{W}^{(oculta)}\\ \\ (entrada\\ neta\\ a\\ la\\ capa\\ oculta)$$\n",
    "\n",
    "$$\\boldsymbol{A}^{(oculta)}=\\phi(\\boldsymbol{Z}^{(oculta)})\\ \\ (activación\\ de\\ la\\ capa\\ oculta)$$\n",
    "\n",
    "$$\\boldsymbol{Z}^{(salida)}=\\boldsymbol{A}^{(oculta)}\\boldsymbol{W}^{(salida)}\\ \\ (entrada\\ neta\\ a\\ la\\ capa\\ de \\ salida)$$\n",
    "\n",
    "$$\\boldsymbol{A}^{(salida)}=\\phi(\\boldsymbol{Z}^{(salida)})\\ \\ (activación\\ de\\ la\\ capa\\ \\ de\\ salida)$$\n",
    "\n",
    "La siguiente figura muestra este proceso:\n",
    "\n",
    "<img src=\"figuras/propagacionHaciaAdelante.png\" width=\"75%\">\n",
    "\n",
    "En propagación hacia atrás, propagamos el error de derecha a izquierda. Comenzamos por calcular el vector de error de la capa de salida:\n",
    "\n",
    "$$\\boldsymbol{\\delta}^{(salida)}=\\boldsymbol{a}^{(salida)}-y$$\n",
    "\n",
    "Aqui, $\\boldsymbol{y}$ es el vector de las etiquetas de clase verdaderas.\n",
    "\n",
    "Después, se calcula el término de error de la capa oculta:\n",
    "\n",
    "$$\\boldsymbol{\\delta}^{(oculta)}=\\boldsymbol{\\delta}^{(salida)}(\\boldsymbol{W}^{(salida)})^T\\odot\\frac{\\partial \\phi(z^{(oculta)})}{\\partial z^{(oculta)}}$$\n",
    "\n",
    "Donde, $\\odot$ es la multiplicación elemento por elemento y $\\frac{\\partial \\phi(z^{(oculta)})}{\\partial z^{(oculta)}}$ es simplemente la derivada de la función de activación sigmoide:\n",
    "\n",
    "$$\\frac{\\partial \\phi(z^{(oculta)})}{\\partial z^{(oculta)}}=(a^{(oculta)}\\odot(1-a^{(oculta)}))$$\n",
    "\n",
    "Derivación de la derivada de la función de activación sigmoide:\n",
    "\n",
    "$$\\phi'(z)=\\frac{\\partial}{\\partial z}\\left(\\frac{1}{1+e^{-z}}\\right)$$\n",
    "\n",
    "$$=\\frac{e^{-z}}{\\left(1+e^{-z}\\right)^2}$$\n",
    "\n",
    "$$=\\frac{1+e^{-z}}{(1+e^{-z})^2}-\\left(\\frac{1}{1+e^{-z}}\\right)^2$$\n",
    "\n",
    "$$=\\frac{1}{(1+e^{-z})}-\\left(\\frac{1}{1+e^{-z}}\\right)^2$$\n",
    "\n",
    "$$=\\phi(z)-(\\phi(z))^2$$\n",
    "\n",
    "$$=\\phi(z)(1-\\phi(z))$$\n",
    "\n",
    "$$=a(1-a)$$\n",
    "\n",
    "Después, se calcula la $\\delta^{(oculta)}$ de la capa oculta:\n",
    "\n",
    "$$\\boldsymbol{\\delta}^{(oculta)}=\\boldsymbol{\\delta}^{(salida)}(\\boldsymbol{W}^{(salida)})^T\\odot(a^{(oculta)}\\odot(1-a^{(oculta)}))$$\n",
    "\n",
    "Después de obtener los términos $\\delta$, se puede escribir la derivación de la función de costo como:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{i,j}^{(salida)}}J(\\boldsymbol{W})=a_j^{(oculta)}\\delta_i^{(salida)}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{i,j}^{(oculta)}}J(\\boldsymbol{W})=a_j^{(entrada)}\\delta_i^{(oculta)}$$\n",
    "\n",
    "Posteriormente, hay que acumular la derivada parcial de cada nodo en cada capa y el error del nodo en la proxima capa, hay que calcular $\\Delta_{i,j}^{(l)}$ para cada ejemplo del conjunto de entrenamiento. Por lo que es más facíl implementarlo como una versióm vectorizada:\n",
    "\n",
    "$$\\Delta^{(oculta)}=\\Delta^{(oculta)}+(\\boldsymbol{A}^{(entrada)})^T\\delta^{(oculta)}$$\n",
    "\n",
    "$$\\Delta^{(salida)}=\\Delta^{(salida)}+(\\boldsymbol{A}^{(oculta)})^T\\delta^{(salida)}$$\n",
    "\n",
    "Y después que se hayan acumulado las derivadas, se agrega el término de regularización:\n",
    "\n",
    "$$\\Delta^{(l)}:=\\Delta^{(l)}+\\lambda^{(l)}\\ \\ (exceptuando\\ el \\ término\\ de\\ sesgo)$$\n",
    "\n",
    "Por último, después de haber calculado los gradientes, ahora se pueden actualizar los pesos tomando un paso en sentido opuesto del gradiente para cada capa l:\n",
    "\n",
    "$$\\boldsymbol{W}^{(l)}:=\\boldsymbol{W}^{(l)}-\\eta\\Delta^{(l)}$$\n",
    "\n",
    "Para unir todo, resumamos la propagación hacia atrás en el siguiente figura:\n",
    "\n",
    "<img src=\"figuras/propagacionHaciaAtras.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
