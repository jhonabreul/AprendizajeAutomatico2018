{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales\n",
    "\n",
    "En este ejercicio se implementaran las capas convolucionales (CONV) y de agrupación (POOL) en numpy, incluidas tanto la propagación hacia adelante como la propagación hacia atrás. \n",
    "\n",
    "**Notación**:\n",
    "- Superíndice $[l]$ denota un objeto de la capa $l^{th}$. \n",
    "    - Ejemplos: $a^{[4]}$ es la $4^{ta}$ capa de activación. $W^{[5]}$ y $b^{[5]}$ son los parámetros de la $5^{ta}$ capa.\n",
    "\n",
    "\n",
    "- Superíndice $(i)$ denota un objeto del i-ésimo ejemplo. \n",
    "    - Ejemplo: $x^{(i)}$ es el i-ésimo ejemplo de entrenamiento de entrada.\n",
    "    \n",
    "    \n",
    "- Subíndice $i$ denota la i-ésima entrada de un vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota la i-ésima entrada de las activaciones en la capa $l$, asumiendo que esta es una capa completamente conectada.\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$ y $n_C$ denotan respectivamente la altura, el ancho y el número de canales de una capa determinada. Si quieres hacer referencia a una capa específica $l$, también se puede escribir $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "\n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ y $n_{C_{prev}}$ denotan respectivamente la altura, el ancho y el número de canales de la capa anterior. Si hace referencia a una capa específica $l$, esto también podría ser denotado $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Paquetes\n",
    "\n",
    "Primero importemos todos los paquetes que necesitará durante este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Esquema del ejercicio\n",
    "\n",
    "Se implementarán los bloques de construcción de una red neuronal convolucional! Cada función que implementará tendrá instrucciones detalladas que lo guiarán por los pasos necesarios:\n",
    "\n",
    "- Funciones de convolución, que incluyen:\n",
    "     - Relleno con ceros\n",
    "     - Ventana de convolución\n",
    "     - Convolución hacia adelante\n",
    "     - Convolución hacia atrás\n",
    "- Funciones de agrupamiento, que incluyen:\n",
    "     - Agrupando hacia adelante\n",
    "     - Crear máscara\n",
    "     - Distribuir valor\n",
    "     - Agrupando hacia atrás (opcional)\n",
    "    \n",
    "En este ejercicio se implementaran estas funciones desde cero en `numpy`. En el siguiente ejercicio, se usará los equivalentes de TensorFlow de estas funciones para construir el siguiente modelo:\n",
    "\n",
    "<img src=\"figuras/arquitectura_cnn.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**Tenga en cuenta** que para cada función hacia adelante, existe su correspondiente equivalente hacia atrás. Por lo tanto, en cada paso de los módulos hacia adelante almacenará algunos parámetros en un caché. Estos parámetros se usan para calcular gradientes durante la propagación hacia atrás. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Redes Neuronales Convolucionales\n",
    "\n",
    "Aunque los frameworks de programación hacen que las convoluciones sean fáciles de usar, siguen siendo uno de los conceptos más difíciles de entender en Deep Learning. Una capa de convolución transforma un volumen de entrada en un volumen de salida de diferentes tamaños, como se muestra a continuación.\n",
    "\n",
    "<img src=\"figuras/RNC.png\" width=\"75%\">\n",
    "\n",
    "In this part, you will build every step of the convolution layer. You will first implement two helper functions: one for zero padding and the other for computing the convolution function itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Relleno con ceros\n",
    "\n",
    "Relleno con ceros agrega ceros alrededor del borde de la imagen:\n",
    "\n",
    "<img src=\"figuras/relleno_con_ceros.png\" width=\"75%\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 1** </u><font color='purple'>  : **Relleno con ceros**<br> Imagen (3 canales, RGB) con un relleno de 2. </center></caption>\n",
    "\n",
    "Los principales beneficios del relleno son los siguientes:\n",
    "- Permite usar una capa de convolución sin necesariamente contraer la altura y el ancho de los volúmenes. Esto es importante para construir redes más profundas, ya que de lo contrario la altura/ancho se reduciría a medida que se avanza hacia capas más profundas. Un caso especial importante es la convolución \"misma\", en la que el alto/ancho se conserva exactamente después de una capa.\n",
    "- Nos ayuda a mantener más información en el borde de una imagen. Sin relleno, muy pocos valores en la siguiente capa se verían afectados por los píxeles de los bordes de una imagen.\n",
    "\n",
    "Implementar la siguiente función, que rellena todas las imágenes de un lote de ejemplos X con ceros. [Usar np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). Tenga en cuenta si desea rellenar la matriz \"a\" de la forma $(5,5,5,5,5)$ con `pad = 1` para la 2ª dimensión,` pad = 3` para la 4ª dimensión y `pad = 0` para el resto, harías:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓN: relleno_cero\n",
    "\n",
    "def relleno_cero(X, relleno):\n",
    "    \"\"\"\n",
    "    Rellena con ceros todas las imágens del conjunto de datos X. El relleno se aplica\n",
    "    a la altura y el ancho de la imagenT como se muestra en la Figura 1.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- un arreglo numpy python de forma (m, n_H, n_W, n_C) representando un lote de imágenes\n",
    "    relleno -- integer, cantidad de relleno alrededor de cada imagen sobre las dimensions vertical y horizontal\n",
    "    \n",
    "    Retorna:\n",
    "    X_relleno -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_relleno = np.pad(X, ((0,0),(relleno,relleno),(relleno,relleno),(0,0)), 'constant')\n",
    "    \n",
    "    return X_relleno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_relleno.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_relleno[1,1] = [[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d32ada0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAACuCAYAAABUfpQYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngjQzy/B/eIxBW6vwpqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118601160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_relleno = relleno_cero(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_relleno.shape =\", x_relleno.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_relleno[1,1] =\", x_relleno[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_relleno_ceros')\n",
    "axarr[1].imshow(x_relleno[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Paso único de convolución\n",
    "En esta parte, implemente un solo paso de convolución, en el cual aplica el filtro a una posición única de la entrada. Esto se usará para construir una unidad convolucional, que:\n",
    "- Toma un volumen de entrada\n",
    "- Aplica un filtro en cada posición de la entrada\n",
    "- Emite otro volumen (generalmente de diferente tamaño)\n",
    "\n",
    "<h3 style=\"text-align: center;\" markdown=\"1\">`.            Imagen                 Resultado Convolución`</h3>\n",
    "\n",
    "<img src=\"figuras/convolucion.gif\" width=\"50%\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 2** </u><font color='purple'>  : **Operación de Convolución**<br> con un filtro 2x2 y un paso de 1 (paso = cantidad que se mueve la ventana cada vez que se desliza) </center></caption>\n",
    "\n",
    "En una aplicación de visión por computadora, cada valor en la matriz de la izquierda corresponde a un único valor de píxel, y convolvemos un filtro de 3x3 con la imagen multiplicando sus valores elemento por elemento con la matriz original, y luego los sumamos. En este primer paso del ejercicio, implementará un solo paso de convolución, que corresponde a la aplicación de un filtro a solo una de las posiciones para obtener una única salida de valor real.\n",
    "\n",
    "Más adelante, se aplicará esta función a múltiples posiciones de la entrada para implementar la operación convolucional completa.\n",
    "\n",
    "Implementar `conv_paso_sencillo`(). Usar `numpy.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓN: conv_paso_sencillo\n",
    "\n",
    "def conv_paso_sencillo(a_trozo_prev, W, b):\n",
    "    \"\"\"\n",
    "    Aplicar un filtro definido por los parámetros W sobre un trozo (a_trozo_prev) de la salida de activación \n",
    "    de la capa previa.\n",
    "    \n",
    "    Arguments:\n",
    "    a_trozo_prev -- trozo de los datos de entrada con forma (f, f, n_C_prev)\n",
    "    W -- Parámetros de pesos contenidos en la ventana con forma (f, f, n_C_prev)\n",
    "    b -- Parámetros de sesgos contenidos en la ventana con forma (1, 1, 1)\n",
    "    \n",
    "    Retorna:\n",
    "    Z -- un valor escalar, resultado de convolver la ventana deslizante (W, b) sobre un trozo x de los datos de entrada\n",
    "    \"\"\"\n",
    "\n",
    "    # Producto elemento por elemento entre una_rodaja y W. Sumar sesgos.\n",
    "    s = (a_trozo_prev * W) + b\n",
    "    # Sumar sobre todas las entradas del volume s\n",
    "    Z = np.sum(s)\n",
    "    \n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.1602122025\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_paso_sencillo(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Redes neuronales convolucionales - Propagación hacia adelante\n",
    "\n",
    "En la propagación hacia adelante, se tomarán muchos filtros y los convolverá con la entrada. Cada 'convolución' da una matriz de salida de 2D. A continuación, se apilarán estas salidas para obtener un volumen de 3D:\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"figuras/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "Implementar la función siguiente para convolver los filtros W sobre una activación de entrada A_prev. Esta función toma como entrada A_prev, las activaciones producidas por la capa previa (para un lote de m entradas), F filtros/pesos denotados por W, y un vector de sesgo denotado por b, donde cada filtro tiene su propio (único) sesgo. Finalmente, también tiene acceso al diccionario de hiperparámetros que contiene el paso y el relleno. \n",
    "\n",
    "**Pista**:\n",
    "1. Para seleccionar un corte de 2x2 en la esquina superior izquierda de una matriz \"a_prev\" (forma (5,5,3)), se haría:\n",
    "```python\n",
    "a_pedazo_prev = a_prev [0: 2,0: 2 ,:]\n",
    "```\n",
    "Esto será útil cuando defina `a_pedazo_prev` a continuación, usando los índices `inicio/fin` que definirá.\n",
    "2. Para definir a_pedazo, primero deberá definir sus esquinas `vert_inicio`, `vert_fin`, `horiz_inicio` y `horiz_fin`. Esta figura puede ser útil para que encuentre cómo cada esquina se puede definir usando h, w, f y s en el código a continuación.\n",
    "\n",
    "<img src=\"figuras/pedazo_coord.png\" width=\"75%\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 3** </u><font color='purple'>  : **Definición de un trozo de la matriz usando los indices inicio/fin tanto vertical como horizontal (con un filtro 2x2 )** <br> Esta figura mustra un solo canal.  </center></caption>\n",
    "\n",
    "\n",
    "**Recordar**:\n",
    "Las formulas que relacionan  la forma de salida de la convolución con la forma de la entrada son:\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times relleno}{paso} \\rfloor +1 $$\n",
    "\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times relleno}{paso} \\rfloor +1 $$\n",
    "\n",
    "$$ n_C = \\text{número de filtros usados en la convolución}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCiÓN: conv_hacia_adelante\n",
    "\n",
    "def conv_hacia_adelante(A_prev, W, b, hparametros):\n",
    "    \"\"\"\n",
    "    Implemanta la propagación hacia adelate para una función de convolución\n",
    "    \n",
    "    Argumentos:\n",
    "    A_prev -- activaciones de salida de la capa anterior, arreglo numpy de forma (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Pesos, arreglo numpy de forma (f, f, n_C_prev, n_C)\n",
    "    b -- Sesgos, arreglo numpy de forma (1, 1, 1, n_C)\n",
    "    hparametros -- diccionario python conteniendo \"paso\" and \"relleno\"\n",
    "        \n",
    "    Retorna:\n",
    "    Z -- salida convolución, arreglo numpy de forma (m, n_H, n_W, n_C)\n",
    "    cache -- cache de valores necesarios para la función conv_hacia_atras()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leer las dimensiones de la forma de A_prev  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Leer las dimensiones de la forma de W\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Leer la información de \"hparametros\" (≈2 lines)\n",
    "    paso = hparametros['paso']\n",
    "    relleno = hparametros['relleno']\n",
    "    \n",
    "    # Calcular las dimensiones del volumen de salida de la convolución\n",
    "    # Usando la formula dada arriba.\n",
    "    n_H = int((n_H_prev - f + 2*relleno)/(paso)) + 1\n",
    "    n_W = int((n_W_prev - f + 2*relleno)/(paso)) + 1\n",
    "    \n",
    "    # Initializar el volumen de salida Z con ceros.\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Crear A_prev_relleno rellenando A_prev\n",
    "    A_prev_relleno = relleno_cero(A_prev, relleno)\n",
    "    for i in range(m):                                 # repetir sobre el lote de ejemplos de entrenamiento\n",
    "        a_prev_relleno = A_prev_relleno[i,:,:,:]       # Seleccionar la activación con relleno del i-ésimo ejemplo de entrenamineto\n",
    "        for h in range(n_H):                           # repetir sobre el eje vertical del volumen de salida\n",
    "            for w in range(n_W):                       # repetir sobre el eje horizontal del volumen de salida\n",
    "                for c in range(n_C):                   # repetir sobre los canales (= #filtros) del volumen de salida\n",
    "                    \n",
    "                    # Encontrar las esquinas del \"trozo\" actual \n",
    "                    vert_inicio = h\n",
    "                    vert_fin = h + f\n",
    "                    horiz_inicio = w\n",
    "                    horiz_fin = w + f\n",
    "                    # Usar las esquinas para definir el trozo (3D) de a_prev_relleno \n",
    "                    a_trozo_prev = a_prev_relleno[vert_inicio:vert_fin,horiz_inicio:horiz_fin,:]\n",
    "                    # Convolver el trozo (3D) con el filtro W y sesgo b correctos, para obtener una neurona de salida.\n",
    "                    Z[i, h, w, c] = conv_paso_sencillo(a_trozo_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                    \n",
    "    \n",
    "    # Asegurarse que la froma de salida este correcta\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Guardar información en la \"cache\" para la prpagación hacia atras\n",
    "    cache = (A_prev, W, b, hparametros)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.155859324889\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparametros = {\"relleno\" : 2,\n",
    "               \"paso\": 1}\n",
    "\n",
    "Z, cache_conv = conv_hacia_adelante(A_prev, W, b, hparametros)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, la capa de convolución debe también contener una activación, en ese caso, agregaríamos la siguiente línea de código:\n",
    "\n",
    "```python\n",
    "# Convolver el trozo (3D) con el filtro W y sesgo b correctos, para obtener una neurona de salida.\n",
    "Z[i, h, w, c] = conv_paso_sencillo(a_trozo_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "# Aplicar activación\n",
    "A[i, h, w, c] = activacion(Z[i, h, w, c])\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Capa de Reducción \"Pooling\"\n",
    "\n",
    "La capa de reducción \"pooling\" reduce la altura y anchura de la entrada. Ayuda a reducir la computación, como también ayuda al detector de características más invariante a su posición en la entrada. Los dos tipos de capas de reducción son:\n",
    "\n",
    "- Capa de reducción valor máximo: desliza una ventana ($f, f$) sobre la entrada y almacena el valor máximo de la ventana en la salida.\n",
    "\n",
    "- Capa de reducción valor promedio: desliza una ventana ($f, f$) sobre la entrada y almacena el valor promedio de la ventana en la salida.\n",
    "\n",
    "<img src=\"figuras/reduccion_max.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"figuras/reduccion_avg.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "Estas capas de reducción no tienen parámetros que entrenar en la propagación hacia atras. Sin embargo, tienen hiperparámetros como el tamaño de la ventana $f$. Esto especifica el alto y el ancho de la ventana $f \\times f$ sobre la que se calcularía el máximo o el promedio. \n",
    "\n",
    "### 4.1 - Redución hacia adelante\n",
    "Ahora, implementará `reducción_max` y `reduccion_avg`, en la misma función. \n",
    "\n",
    "**Ejercicio**: Implementar la propagación hacia adelante de la capa de reducción.\n",
    "\n",
    "**Recordar**:\n",
    "Como no hay relleno, las fórmulas que vinculan la forma de salida de la reducción a la forma de entrada son:\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{paso} \\rfloor +1 $$\n",
    "\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{paso} \\rfloor +1 $$\n",
    "\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCCIÓN: reduccion_hacia_adelante\n",
    "\n",
    "def reduccion_hacia_adelante(A_prev, hparametros, modo = \"max\"):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante de la capa de reducción\n",
    "    \n",
    "    Argumentos:\n",
    "    A_prev -- Datos de entrada, arreglo numpy de forma (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparametros -- diccionario python conteniendo \"f\" y \"paso\"\n",
    "    modo -- el modo de reducción a utilizar, definido como un string (\"max\" or \"average\")\n",
    "    \n",
    "    Retorna:\n",
    "    A -- salida de la capa de reducción, arreglo numpy de forma (m, n_H, n_W, n_C)\n",
    "    cache -- cache de valores necesarios para la propagación hacia atrás de la capa de reducción,\n",
    "    contiene los datos de entrada y los hparametros \n",
    "    \"\"\"\n",
    "    \n",
    "    # Leer las dimensiones de la forma de la entrads\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Leer los hyperparámetros desde \"hparametros\"\n",
    "    f = hparametros[\"f\"]\n",
    "    paso = hparametros[\"paso\"]\n",
    "    \n",
    "    # Definir las dimensiones de la salida\n",
    "    n_H = int(1 + (n_H_prev - f) / paso)\n",
    "    n_W = int(1 + (n_W_prev - f) / paso)\n",
    "    n_C = n_C_prev\n",
    "  \n",
    "    # Initializar la matriz de salida A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "   \n",
    "    for i in range(m):                          # repetir sobre el lote de ejemplos de entrenamiento\n",
    "        for h in range(n_H):                    # repetir sobre el eje vertical del volumen de salida\n",
    "            for w in range(n_W):                # repetir sobre el eje horizontal del volumen de salida\n",
    "                for c in range(n_C):            # repetir sobre los canales (= #filtros) del volumen de salida\n",
    "                    \n",
    "                    # Encontrar las esquinas del \"trozo\" actual\n",
    "                    vert_inicio = h\n",
    "                    vert_fin = h + f\n",
    "                    horiz_inicio = w\n",
    "                    horiz_fin = w + f\n",
    "                    \n",
    "                    # Usar las esqunas para definir el trozo actual del i-ésimo ejemplo de entrenamiento\n",
    "                    # de A_prev, y canal c.\n",
    "                    a_trozo_prev = A_prev[i,vert_inicio:vert_fin,horiz_inicio:horiz_fin,c]\n",
    "                    \n",
    "                    # Computar la operación de reducción sobre el trozo dependiendo del modo. Usar np.max/np.mean.\n",
    "                    if modo == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_trozo_prev)\n",
    "                    elif modo == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_trozo_prev)\n",
    "\n",
    "    # Almacenar la entrada y los hiperparámetros en el \"cache\" para la reduccion_hacia_atras()\n",
    "    cache = (A_prev, hparametros)\n",
    "\n",
    "    # Asegurarse que la forma de la salida es la corecta\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modo = max\n",
      "A = [[[[ 1.74481176  1.6924546   2.10025514]]]\n",
      "\n",
      "\n",
      " [[[ 1.19891788  1.51981682  2.18557541]]]]\n",
      "\n",
      "modo = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparametros = {\"paso\" : 1, \"f\": 4}\n",
    "\n",
    "A, cache = reduccion_hacia_adelante(A_prev, hparametros)\n",
    "print(\"modo = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = reduccion_hacia_adelante(A_prev, hparametros, modo = \"average\")\n",
    "print(\"modo = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now implemented the forward passes of all the layers of a convolutional network. \n",
    "\n",
    "The remainer of this notebook is optional, and will not be graded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Propagación hacia atrás en la redes neuronales convolucionales\n",
    "\n",
    "En los \"frameworks\" modernos de aprendizaje profundo, solo se tiene que implementar la propagación hacia adelante, y el \"frameworks\" se encarga de la propagación hacia atrás, por lo que no es necesitan preocuparse por los detalles de la propagación hacia atrás. La propagación hacia atrás en las redes convolucionales es complicada.\n",
    "\n",
    "Al igual que en una red neuronal simple (totalmente conectada), donde se usó la propagación hacia atras para calcular los derivadas con respecto al costo para actualizar los parámetros. De manera similar, en las redes neuronales convolucionales se puede calcular las derivadas con respecto al costo para actualizar los parámetros. Las ecuaciones de backprop no son triviales y se presentan brevemente a continuación.\n",
    "\n",
    "### 5.1 - Propagación hacia atrás en la capa convolucional \n",
    "\n",
    "Iniciemos implementado la propagación hacia atrás en una capa convolucional. \n",
    "\n",
    "#### 5.1.1 - Computar dA:\n",
    "Esta es la fórmula para calcular $dA$ con respecto al costo de un determinado filtro $W_c$ y un ejemplo de entrenamiento dado:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "Donde $W_c$ es un filtro y $dZ_{hw}$ es un escalar correspondiente al gradiente del costo con respecto a la salida de la capa convolucional Z en la h-ésima fila y la w-ésima columna (correspondiente al producto punto tomado en el i-ésimo paso a la izquierda y el j-ésimo paso hacia abajo). Tenga en cuenta que en cada momento, multiplicamos el mismo filtro $W_c$ por un $dZ$ diferente cuando se actualiza $dA$. Lo hacemos principalmente porque al calcular la propagación hacia adelante, cada filtro se multiplica y se suma con un a_trozo diferente. Por lo tanto, al calcular la propagación hacia atrás para $dA$, solo estamos agregando los gradientes de todos los a_trozos. \n",
    "\n",
    "En código, dentro del lazo `for` apropiado, la formula se traduce a:\n",
    "```python\n",
    "da_prev_relleno[vert_inicio:vert_fin, horiz_inicio:horiz_fin, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - Computar dW:\n",
    "Esta es la fórmula para calcular $dW_c$ ($dW_c$ es la derivada de un filtro) con respecto a la pérdida:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Donde $a_{trozo}$ corresponde al trozo que se usó para generar la acitivación $Z_{ij}$. Por lo tanto, esto termina dándonos el gradiente de $W$ con respecto a ese trozo. Dado que es el mismo $W$, sumaremos todos los gradientes para obtener $dW$. \n",
    "\n",
    "En código, dentro del lazo `for` apropiado, la formula se traduce a:\n",
    "```python\n",
    "dW[:,:,:,c] += a_trozo * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - Computar db:\n",
    "\n",
    "Esta es la fórmula para calcular $db$ con respecto al costo de el filtro $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "Como ya se habia visto en redes neuronales multicapas, $db$ se calcula sumando $dZ$. En este caso, solo está sumando todos los gradientes de la salida de la convolución (Z) con respecto al costo.\n",
    "\n",
    "En código, dentro del lazo `for` apropiado, la formula se traduce a:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "Implementar la función the `conv_hacia_atras`. SE debe sumar sobre todos los ejemplos de entrenamiento, filtros, alturas y anchuras. A continuación, debe calcular las derivadas utilizando las fórmulas 1, 2 y 3 anteriores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_hacia_atras(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una función de convolución\n",
    "    \n",
    "    Argumentos:\n",
    "    dZ -- gradiente del costo con respecto a la salida de la capa de convolución (Z), arreglo numpy de forma (m, n_H, n_W, n_C)\n",
    "    cache -- cache de los valores necesitados por conv_hacia_atras(), salida de of conv_hacia_adelante\n",
    "    \n",
    "    Retorna:\n",
    "    dA_prev -- gradiente del costo con respecto a la entrada de la capa de convolución (A_prev),\n",
    "               arreglo numpy de forma (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradiente del costo con respecto a los pesos de la capa de convolución (W)\n",
    "          arreglo numpy de forma (f, f, n_C_prev, n_C)\n",
    "    db -- gradiente del costo con respecto a los sesgos de la capa de convolución (b)\n",
    "          arreglo numpy de forma (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leer la información del \"cache\"\n",
    "    (A_prev, W, b, hparametros) = cache\n",
    "    \n",
    "    # Leer las dimensiones de la forma de A_prev\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Leer las dimensiones de la forma de W\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Leer la información de los \"hparametros\"\n",
    "    paso = hparametros['paso']\n",
    "    relleno = hparametros['relleno']\n",
    "    \n",
    "    # Leer las dimensiones de la forma de dZ\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initializar dA_prev, dW, db con las fromas correctas\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Rellenar A_prev y dA_prev\n",
    "    A_prev_relleno = relleno_cero(A_prev, relleno)\n",
    "    dA_prev_relleno =  relleno_cero(dA_prev, relleno)\n",
    "    \n",
    "    for i in range(m):                       # repetir sobre el lote de ejemplos de entrenamiento\n",
    "        \n",
    "        # selecciona el i-ésimo ejemplo de entrenamiento de A_prev_relleno y dA_prev_relleno\n",
    "        a_prev_relleno = A_prev_relleno[i,:,:,:]\n",
    "        da_prev_relleno = dA_prev_relleno[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                    # repetir sobre el eje vertical del volumen de salida\n",
    "            for w in range(n_W):                # repetir sobre el eje horizontal del volumen de salida\n",
    "                for c in range(n_C):            # repetir sobre los canales (= #filtros) del volumen de salida\n",
    "                    \n",
    "                    # Encontrar las esquinas del \"trozo\" actual\n",
    "                    vert_inicio = h\n",
    "                    vert_fin = h + f\n",
    "                    horiz_inicio = w\n",
    "                    horiz_fin = w + f\n",
    "                    \n",
    "                    # Usar las esquinas para definir el trozo de a_prev_relleno\n",
    "                    a_trozo = a_prev_relleno[vert_inicio:vert_fin,horiz_inicio:horiz_fin,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_relleno[vert_inicio:vert_fin, horiz_inicio:horiz_fin, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_trozo * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # fijar el i-ésimo ejemplo de entrenamineto de dA_prev al da_prev_relleno sin rellenar\n",
    "        dA_prev[i, :, :, :] = dA_prev_relleno[i, relleno:-relleno, relleno:-relleno, :]\n",
    "\n",
    "    \n",
    "    # Asegurarse que la forma de la salida es la corecta\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.60899067587\n",
      "dW_mean = 10.5817412755\n",
      "db_mean = 76.3710691956\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_hacia_atras(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Capa de Reducción - propagación hacia atrás\n",
    "\n",
    "A continuación, implementemos la propagación hacia atrás para la capa de reducción, comenzando con la capa reducción por valor m®aximo. Aunque una capa de reducción no tiene parámetros para que propagación hacia atrás actualice, se necesita volver a propagar el gradiente a través de la capa de reducción para calcular los gradientes de las capas anteriores de la capa de reducción. \n",
    "\n",
    "### 5.2.1 Reducción por valor máximo - propagación hacia atrás  \n",
    "\n",
    "Antes de implementar la propagación hacia atrás de la capa de reducción, construir una función auxiliar llamada `crear_mascara_ventana()` que hace lo siguiente: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "Esta función crea una matriz de \"máscara\" que realiza un seguimiento de dónde está el máximo de la matriz. `True` (1) indica la posición del máximo en X, las otras entradas son `False` (0). La propagación hacia atrás para la reducción por valor promedio será similar a esta pero usando una máscara diferente.  \n",
    "\n",
    "Implementar `crear_mascara_ventana(()`. Esta función será útil para reducción hacia atrás. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crear_mascara_ventana(x):\n",
    "    \"\"\"\n",
    "    Crea una mascara a partir de una matriz de entrada x, para identificar la entrada máxima de x.\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- Arreglo de forma (f, f)\n",
    "    \n",
    "    Retorna:\n",
    "    mascara -- Arreglo de la misma forma de la ventana, contiene un True en la posición del máximo de x.\n",
    "    \"\"\"\n",
    "    \n",
    "    mascara = x[:,:] == np.max(x)\n",
    "    \n",
    "    return mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = crear_mascara_ventana(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué hacemos un seguimiento de la posición del máximo? Es porque este es el valor de entrada que finalmente influyó en la salida y, por lo tanto, en el costo. Prpagación hacia atrás está calculando gradientes con respecto al costo, por lo que cualquier cosa que influya en el costo final debería tener un gradiente distinto de cero. Por lo tanto, propagación hacia atrás \"propagará\" el gradiente a este valor de entrada particular que ha influido en el costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 -  Reducción por valor promedio - propagación hacia atrás\n",
    "\n",
    "En la reducciónp por valor máximo, para cada ventana de entrada, toda la \"influencia\" en la salida proviene de un único valor de entrada: el máximo. En la agrupación promedio, cada elemento de la ventana de entrada tiene la misma influencia en la salida. Entonces, para implementar propagación hacia atrás, se implementará una función auxiliar que refleja esto.\n",
    "\n",
    "Por ejemplo, si hicimos reducción por valor promedio en la propagación hacia adelante usando un filtro de $2 \\times 2$, entonces la máscara que se usará para la prpagación hacia atrás se verá como: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "Esto implica que cada posición en la matriz $dZ$ contribuye por igual a la salida porque en la propagación hacia adelante, tomamos un promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribuir_valor(dz, forma):\n",
    "    \"\"\"\n",
    "    Distribuye el valor de entrada en la matriz con dimensión forma\n",
    "    \n",
    "    Argumentos:\n",
    "    dz -- escalar de entrada\n",
    "    forma -- la forma (n_H, n_W) de la matriz de salida para donde queremos distribuir el valor de dz\n",
    "    \n",
    "    Retorna:\n",
    "    a -- Arreglo de tamaño (n_H, n_W) donde queremos que se distribuya el valor de dz\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leer las dimensiones de desde forma\n",
    "    (n_H, n_W) = forma\n",
    "    \n",
    "    # Calcular el valor a distribuir en la matriz\n",
    "    promedio = dz / (n_H * n_W)\n",
    "    \n",
    "    # Crear una matriz donde cada valor es el valor promedio\n",
    "    a = np.full((n_H, n_W), promedio)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valores distribuidos = [[ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribuir_valor(2, (2,2))\n",
    "print('valores distribuidos =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Poniendo todo junto: Reducción propagación hacia atrás\n",
    "\n",
    "Tenemos todo lo que necesita para calcular la propagación hacia atrás en una capa de reducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduccion_hacia_atras(dA, cache, modo = \"max\"):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de la capa de reducción\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente del costo con respecto a la salida de la capa de reducción, con la misma forma que A\n",
    "    cache -- la salida del caché de la propagación hacia adelante de la capa de reducción, contiene la entrada\n",
    "             de la capa y los hiperparámetros\n",
    "    modo -- el modo de reducción que se usará, definido como un string (\"max\" o \"average\")\n",
    "    \n",
    "    Retorna:\n",
    "    dA_prev -- gradiente de costo con respecto a la entrada de la capa de reducción, con la misma forma que A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Leer la información del \"cache\"\n",
    "    (A_prev, hparametros) = cache\n",
    "    \n",
    "    # Leer la información de los \"hparametros\"\n",
    "    paso = hparametros['paso']\n",
    "    f = hparametros['f']\n",
    "    \n",
    "    # Leer las dimensiones de la forma de A_prev y la forma de dA\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initializae dA_prev con ceros\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    for i in range(m):                       # repetir sobre el lote de ejemplos de entrenamiento\n",
    "        \n",
    "        # selecciona el i-ésimo ejemplo de entrenamiento de A_prev\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                    # repetir sobre el eje vertical\n",
    "            for w in range(n_W):                # repetir sobre el eje horizontal\n",
    "                for c in range(n_C):            # repetir sobre los canales\n",
    "                    \n",
    "                    # Encontrar las esquinas del \"trozo\" actual\n",
    "                    vert_inicio = h\n",
    "                    vert_fin = h + f\n",
    "                    horiz_inicio = w\n",
    "                    horiz_fin = w + f\n",
    "                    \n",
    "                    # Computar la propagación hacia atrás de ambos modos.\n",
    "                    if modo == \"max\":\n",
    "                        \n",
    "                        # Usar las esquinas y \"c\" para definir el trozo actual de a_prev\n",
    "                        a_prev_trozo = a_prev[vert_inicio:vert_fin,horiz_inicio:horiz_fin,c]\n",
    "                        # Crear la mascara a partir de a_prev_trozo\n",
    "                        mascara = crear_mascara_ventana(a_prev_trozo)\n",
    "                        \n",
    "                        # Fijar dA_prev igual a dA_prev + (la mascara multiplicada por la entrada corecta de dA)\n",
    "                        dA_prev[i, vert_inicio:vert_fin, horiz_inicio:horiz_fin, c] += mascara * dA[i, h, w, c]\n",
    "            \n",
    "                    elif modo == \"average\":\n",
    "                        \n",
    "                        # Obtener el valor de da desde dA \n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Definir la forma del filtro como fxf\n",
    "                        forma = (f, f)\n",
    "                        # Distribuirlo para obtener la porción correcta de dA_prev. es decir, agregue el valor distribuido de da.\n",
    "                        dA_prev[i, vert_inicio:vert_fin, horiz_inicio:horiz_fin, c] += distribuir_valor(da, forma)\n",
    "\n",
    "    \n",
    "    # Asegurarse que la forma de la salida es la corecta\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modo = max\n",
      "media de dA =  0.145713902729\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "modo = average\n",
      "media of dA =  0.145713902729\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparametros = {\"paso\" : 1, \"f\": 2}\n",
    "A, cache = reduccion_hacia_adelante(A_prev, hparametros)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = reduccion_hacia_atras(dA, cache, modo = \"max\")\n",
    "print(\"modo = max\")\n",
    "print('media de dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = reduccion_hacia_atras(dA, cache, modo = \"average\")\n",
    "print(\"modo = average\")\n",
    "print('media of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
