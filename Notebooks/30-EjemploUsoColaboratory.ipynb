{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqBz-L5USZaB"
   },
   "source": [
    "# Ejemplo del uso de Google Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IQmvXgRSZaC"
   },
   "source": [
    "Ejemplo de cómo usar Google [Colab](https://colab.research.google.com), el servicio gratuito en la nube de Google para desarrolladores de inteligencia artificial. Con Colab, puede desarrollar aplicaciones de aprendizaje profundo usando Tarjetas Graficas GPU de forma gratuita.\n",
    "\n",
    "Con Colab se puede usar el Google Drive para almacenar los Notebook de Jupyter y los datos necesarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3swFSrRSZaE"
   },
   "source": [
    "### Instalar librerías para acceder al Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22572,
     "status": "ok",
     "timestamp": 1525409809350,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "6Yj_6yyMSZaE",
    "outputId": "d29b7b48-5bd7-466b-bd44-1be09a67490f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpg: keybox '/tmp/tmpft4c3y_p/pubring.gpg' created\n",
      "gpg: /tmp/tmpft4c3y_p/trustdb.gpg: trustdb created\n",
      "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
      "gpg: Total number processed: 1\n",
      "gpg:               imported: 1\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n"
     ]
    }
   ],
   "source": [
    "# Instalar la libreria FUSE.\n",
    "# https://github.com/astrada/google-drive-ocamlfuse\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5E1AA3x1SZaH"
   },
   "source": [
    "### Generar los tokens de autorización para Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pMoLvEFTSZaI"
   },
   "outputs": [],
   "source": [
    "# Generar los tokens de autorización para Colab\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHBRGlsESZaL"
   },
   "source": [
    "### Generar credenciales para la librería FUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13211,
     "status": "ok",
     "timestamp": 1525409833473,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "h5g1c4JDSZaM",
    "outputId": "bab252a1-4351-46dc-f570-479fdfaaad04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "# Generar credenciales para la librería FUSE\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZFRxy6BSZaP"
   },
   "source": [
    "### Crear un directorio y montar Google Drive usando ese directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8768,
     "status": "ok",
     "timestamp": 1525409845089,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "a65Jh2d5SZaP",
    "outputId": "3d158245-54e1-4b7c-d9f5-a56750456443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse: mountpoint is not empty\n",
      "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
      "Archivos en Drive:\n",
      "01-Introduccion.ipynb\t\t\t\tFashion-MNIST.ipynb\n",
      "21-RedesNeuronales-3.ipynb\t\t\tfiguras\n",
      "22-RedesNeuronales-4.ipynb\t\t\tGenerador_Cervantes_2.ipynb\n",
      "23-RedNeuronalConvolucional-1.ipynb\t\tkeras-mnist-tutorial\n",
      "24-RedNeuronalConvolucional-2.ipynb\t\tmnist_cnn.py\n",
      "25-RedNeuronalConvolucional-3.ipynb\t\tpesos-v1.01-14.4789.hdf5\n",
      "26-RedNeuronalConvolucional-4 (32d4ba2a).ipynb\tpesos-v1.03-14.3032.hdf5\n",
      "26-RedNeuronalConvolucional-4.ipynb\t\tpesos-v1.20-14.1243.hdf5\n",
      "30.EjemploUsoColaboratory.ipynb\t\t\tpesos-v1.81-13.7262.hdf5\n",
      "Breast_Cancer.ipynb\t\t\t\tPytorch_Tutorial.ipynb\n",
      "deep_learning_turkey.ipynb\t\t\tReconocimientoEmociones\n",
      "DonQuijote.txt\t\t\t\t\tReconocimientoEmosiones.ipynb\n",
      "DonQuixote.txt\t\t\t\t\tTitanic.csv\n",
      "entrenamiento-v1.log\n"
     ]
    }
   ],
   "source": [
    "# Crear un directorio y montar Google Drive usando ese directorio\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "\n",
    "print ('Archivos en Drive:')\n",
    "!ls drive/ColabNotebooks    # ColabNotebooks es el nombre del directorio en Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvou0GtNSZaU"
   },
   "source": [
    "## Ejemplo de una Red Neuronal Convolucional con la data CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ARc7SHVSSZaV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29N4d9DbSZaa"
   },
   "source": [
    "#### Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2517,
     "status": "ok",
     "timestamp": 1525409856877,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "CA2G6gxUSZab",
    "outputId": "deb5b7c6-0c30-46a3-9169-5ca434cfc29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de x_entrenamiento: (50000, 32, 32, 3)\n",
      "50000 ejemplos de entrenamiento\n",
      "10000 ejemplos de prueba\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de datos de entrenamiento y prueba:\n",
    "(x_entrenamiento, y_entrenamiento), (x_prueba, y_prueba) = cifar10.load_data()\n",
    "print('Dimensiones de x_entrenamiento:', x_entrenamiento.shape)\n",
    "print(x_entrenamiento.shape[0], 'ejemplos de entrenamiento')\n",
    "print(x_prueba.shape[0], 'ejemplos de prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlT7YG-pSZaf"
   },
   "source": [
    "#### Convertir las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Rbuu6KFlSZag"
   },
   "outputs": [],
   "source": [
    "# Convertir vectores de clases a matrices binarias.\n",
    "num_clases = 10\n",
    "\n",
    "y_entrenamiento = keras.utils.to_categorical(y_entrenamiento, num_clases)\n",
    "y_prueba = keras.utils.to_categorical(y_prueba, num_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eudjQTRVSZaj"
   },
   "source": [
    "#### Normalizar las entradas a valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GwXLw6A8SZak"
   },
   "outputs": [],
   "source": [
    "x_entrenamiento = x_entrenamiento.astype('float32')\n",
    "x_prueba = x_prueba.astype('float32')\n",
    "x_entrenamiento /= 255\n",
    "x_prueba /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-I6j_vaSZam"
   },
   "source": [
    "#### Definir el esquema de aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vLIoLp_PSZan"
   },
   "outputs": [],
   "source": [
    "# pre-procesamiento de las imágenes para aumentarlas\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # rotar aleatoriamente las imágenes entre 0 y 180 grados \n",
    "    width_shift_range=0.1,  # mover aleatoriamente (horizontalmente) las imágenes (fracción del ancho total)\n",
    "    height_shift_range=0.1,  # mover  aleatoriamente (verticalmente) las imágenes (fracción del alto total)\n",
    "    horizontal_flip=True,  # volterar aleatoriamente las imágenes (horizontalmente)\n",
    "    vertical_flip=False)  # volterar aleatoriamente las imágenes (verticalmente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHeLhwUXSZaq"
   },
   "source": [
    "### Definir la arquitectura de la Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3861,
     "status": "ok",
     "timestamp": 1525409886934,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "DptNZmJ2SZaq",
    "outputId": "7b683f5e-bb56-49fb-be13-cf8e838b2ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "bn_conv1 (BatchNormalization (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "bn_conv2 (BatchNormalization (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "bn_conv3 (BatchNormalization (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "bn_conv4 (BatchNormalization (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "bn_fc1 (BatchNormalization)  (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "salida (Dense)               (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "bn_salida (BatchNormalizatio (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,253,714\n",
      "Trainable params: 1,252,286\n",
      "Non-trainable params: 1,428\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fijar eje para la BatchNormalization en 3\n",
    "bn_axis = 3\n",
    "\n",
    "modelo = Sequential()\n",
    "\n",
    "modelo.add(Conv2D(32, (3, 3), padding='same', input_shape=x_entrenamiento.shape[1:], name='conv1'))\n",
    "modelo.add(BatchNormalization(axis=3, name='bn_conv1'))\n",
    "modelo.add(Activation('relu'))\n",
    "modelo.add(Conv2D(32, (3, 3), name='conv2'))\n",
    "modelo.add(BatchNormalization(axis=3, name='bn_conv2'))\n",
    "modelo.add(Activation('relu'))\n",
    "modelo.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "modelo.add(Conv2D(64, (3, 3), padding='same', name='conv3'))\n",
    "modelo.add(BatchNormalization(axis=3, name='bn_conv3'))\n",
    "modelo.add(Activation('relu'))\n",
    "modelo.add(Conv2D(64, (3, 3), name='conv4'))\n",
    "modelo.add(BatchNormalization(axis=3, name='bn_conv4'))\n",
    "modelo.add(Activation('relu'))\n",
    "modelo.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "modelo.add(Flatten())\n",
    "modelo.add(Dense(512, name='fc1'))\n",
    "modelo.add(BatchNormalization(axis=1, name='bn_fc1'))\n",
    "modelo.add(Activation('relu'))\n",
    "modelo.add(Dense(num_clases, name='salida'))\n",
    "modelo.add(BatchNormalization(axis=1, name='bn_salida'))\n",
    "\n",
    "modelo.add(Activation('softmax'))\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnYt3URzSZat"
   },
   "source": [
    "#### Compilar, Callbacks y Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamaño_lote = 32\n",
    "epocas = 200\n",
    "\n",
    "# inicializar optimizador RMSprop\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Entrenar el modelo usando RMSprop\n",
    "modelo.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 6854
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16231196,
     "status": "ok",
     "timestamp": 1525426123089,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "-Lb0Ii2uSZat",
    "outputId": "ddbd2218-3e05-475d-fa5f-371f60ce55f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1084/1562 [===================>..........] - ETA: 33s - loss: 1.5928 - acc: 0.45001562/1562 [==============================] - 112s 72ms/step - loss: 1.5347 - acc: 0.4756 - val_loss: 1.2971 - val_acc: 0.5675\n",
      "Epoch 2/200\n",
      " 276/1562 [====>.........................] - ETA: 1:26 - loss: 1.3603 - acc: 0.54401516/1562 [============================>.] - ETA: 3s - loss: 1.3077 - acc: 0.56801562/1562 [==============================] - 108s 69ms/step - loss: 1.3060 - acc: 0.5687 - val_loss: 1.1941 - val_acc: 0.6050\n",
      "Epoch 3/200\n",
      " 427/1562 [=======>......................] - ETA: 1:16 - loss: 1.2380 - acc: 0.59381562/1562 [==============================] - 109s 70ms/step - loss: 1.1995 - acc: 0.6088 - val_loss: 1.1012 - val_acc: 0.6397\n",
      "Epoch 4/200\n",
      "  35/1562 [..............................] - ETA: 1:46 - loss: 1.1361 - acc: 0.63041330/1562 [========================>.....] - ETA: 15s - loss: 1.1307 - acc: 0.63481562/1562 [==============================] - 107s 69ms/step - loss: 1.1254 - acc: 0.6367 - val_loss: 1.0013 - val_acc: 0.6838\n",
      "Epoch 5/200\n",
      " 381/1562 [======>.......................] - ETA: 1:18 - loss: 1.0798 - acc: 0.65261562/1562 [==============================] - 108s 69ms/step - loss: 1.0628 - acc: 0.6586 - val_loss: 0.9637 - val_acc: 0.6857\n",
      "Epoch 6/200\n",
      "  18/1562 [..............................] - ETA: 1:46 - loss: 1.0182 - acc: 0.68061261/1562 [=======================>......] - ETA: 19s - loss: 1.0117 - acc: 0.67671562/1562 [==============================] - 107s 68ms/step - loss: 1.0082 - acc: 0.6775 - val_loss: 0.8858 - val_acc: 0.7191\n",
      "Epoch 7/200\n",
      " 354/1562 [=====>........................] - ETA: 1:20 - loss: 0.9924 - acc: 0.66981562/1562 [==============================] - 107s 68ms/step - loss: 0.9695 - acc: 0.6860 - val_loss: 0.9053 - val_acc: 0.7036\n",
      "Epoch 8/200\n",
      "  15/1562 [..............................] - ETA: 1:41 - loss: 0.9586 - acc: 0.69171269/1562 [=======================>......] - ETA: 19s - loss: 0.9340 - acc: 0.70071562/1562 [==============================] - 108s 69ms/step - loss: 0.9325 - acc: 0.7003 - val_loss: 0.8869 - val_acc: 0.7110\n",
      "Epoch 9/200\n",
      " 364/1562 [=====>........................] - ETA: 1:18 - loss: 0.9012 - acc: 0.71251562/1562 [==============================] - 107s 69ms/step - loss: 0.8976 - acc: 0.7123 - val_loss: 0.8216 - val_acc: 0.7362\n",
      "Epoch 10/200\n",
      "   8/1562 [..............................] - ETA: 1:37 - loss: 0.8882 - acc: 0.72661320/1562 [========================>.....] - ETA: 15s - loss: 0.8665 - acc: 0.71851562/1562 [==============================] - 108s 69ms/step - loss: 0.8676 - acc: 0.7178 - val_loss: 0.8314 - val_acc: 0.7309\n",
      "Epoch 11/200\n",
      " 389/1562 [======>.......................]1562/1562 [==============================] - 106s 68ms/step - loss: 0.8461 - acc: 0.7265 - val_loss: 0.7503 - val_acc: 0.7554\n",
      "Epoch 12/200\n",
      "  25/1562 [..............................] - ETA: 1:38 - loss: 0.7911 - acc: 0.75381280/1562 [=======================>......] - ETA: 18s - loss: 0.8229 - acc: 0.73511562/1562 [==============================] - 107s 69ms/step - loss: 0.8207 - acc: 0.7346 - val_loss: 0.7341 - val_acc: 0.7596\n",
      "Epoch 13/200\n",
      " 353/1562 [=====>........................] - ETA: 1:19 - loss: 0.8178 - acc: 0.73141562/1562 [==============================] - 107s 69ms/step - loss: 0.8071 - acc: 0.7360 - val_loss: 0.6993 - val_acc: 0.7803\n",
      "Epoch 14/200\n",
      "  13/1562 [..............................] - ETA: 1:33 - loss: 0.8048 - acc: 0.74041265/1562 [=======================>......] - ETA: 19s - loss: 0.7801 - acc: 0.74591562/1562 [==============================] - 106s 68ms/step - loss: 0.7797 - acc: 0.7460 - val_loss: 0.7300 - val_acc: 0.7641\n",
      "Epoch 15/200\n",
      " 351/1562 [=====>........................] - ETA: 1:17 - loss: 0.7619 - acc: 0.75061562/1562 [==============================] - 106s 68ms/step - loss: 0.7614 - acc: 0.7521 - val_loss: 0.7712 - val_acc: 0.7508\n",
      "Epoch 16/200\n",
      "   8/1562 [..............................] - ETA: 1:29 - loss: 0.7809 - acc: 0.75001295/1562 [=======================>......] - ETA: 17s - loss: 0.7506 - acc: 0.75671562/1562 [==============================] - 107s 69ms/step - loss: 0.7477 - acc: 0.7577 - val_loss: 0.6916 - val_acc: 0.7779\n",
      "Epoch 17/200\n",
      " 362/1562 [=====>........................] - ETA: 1:19 - loss: 0.7281 - acc: 0.75771562/1562 [==============================] - 107s 69ms/step - loss: 0.7364 - acc: 0.7593 - val_loss: 0.7170 - val_acc: 0.7613\n",
      "Epoch 18/200\n",
      "  16/1562 [..............................] - ETA: 1:30 - loss: 0.7370 - acc: 0.75391346/1562 [========================>.....] - ETA: 14s - loss: 0.7172 - acc: 0.76871562/1562 [==============================] - 106s 68ms/step - loss: 0.7179 - acc: 0.7684 - val_loss: 0.6573 - val_acc: 0.7888\n",
      "Epoch 19/200\n",
      " 380/1562 [======>.......................] - ETA: 1:18 - loss: 0.7003 - acc: 0.77181562/1562 [==============================] - 107s 69ms/step - loss: 0.7096 - acc: 0.7673 - val_loss: 0.6679 - val_acc: 0.7798\n",
      "Epoch 20/200\n",
      "  16/1562 [..............................] - ETA: 1:29 - loss: 0.6791 - acc: 0.77731422/1562 [==========================>...] - ETA: 9s - loss: 0.6907 - acc: 0.77391562/1562 [==============================] - 106s 68ms/step - loss: 0.6908 - acc: 0.7728 - val_loss: 0.6726 - val_acc: 0.7808\n",
      "Epoch 21/200\n",
      " 444/1562 [=======>......................] - ETA: 1:12 - loss: 0.6895 - acc: 0.77411562/1562 [==============================] - 105s 68ms/step - loss: 0.6882 - acc: 0.7748 - val_loss: 0.6077 - val_acc: 0.8018\n",
      "Epoch 22/200\n",
      "  55/1562 [>.............................] - ETA: 1:44 - loss: 0.6746 - acc: 0.76531302/1562 [========================>.....] - ETA: 17s - loss: 0.6707 - acc: 0.78071562/1562 [==============================] - 108s 69ms/step - loss: 0.6712 - acc: 0.7803 - val_loss: 0.6786 - val_acc: 0.7766\n",
      "Epoch 23/200\n",
      " 372/1562 [======>.......................] - ETA: 1:16 - loss: 0.6654 - acc: 0.77891562/1562 [==============================] - 108s 69ms/step - loss: 0.6630 - acc: 0.7824 - val_loss: 0.6039 - val_acc: 0.8065\n",
      "Epoch 24/200\n",
      "   9/1562 [..............................] - ETA: 2:09 - loss: 0.7001 - acc: 0.77781441/1562 [==========================>...] - ETA: 8s - loss: 0.6555 - acc: 0.78391562/1562 [==============================] - 108s 69ms/step - loss: 0.6570 - acc: 0.7835 - val_loss: 0.6123 - val_acc: 0.8019\n",
      "Epoch 25/200\n",
      " 408/1562 [======>.......................] - ETA: 1:18 - loss: 0.6431 - acc: 0.78491562/1562 [==============================] - 108s 69ms/step - loss: 0.6387 - acc: 0.7905 - val_loss: 0.5683 - val_acc: 0.8179\n",
      "Epoch 26/200\n",
      "  31/1562 [..............................] - ETA: 1:42 - loss: 0.6286 - acc: 0.80951335/1562 [========================>.....] - ETA: 14s - loss: 0.6348 - acc: 0.79261562/1562 [==============================] - 108s 69ms/step - loss: 0.6369 - acc: 0.7918 - val_loss: 0.5911 - val_acc: 0.8078\n",
      "Epoch 27/200\n",
      " 389/1562 [======>.......................] - ETA: 1:14 - loss: 0.6390 - acc: 0.79111562/1562 [==============================] - 106s 68ms/step - loss: 0.6244 - acc: 0.7959 - val_loss: 0.5778 - val_acc: 0.8109\n",
      "Epoch 28/200\n",
      "  37/1562 [..............................] - ETA: 1:34 - loss: 0.6378 - acc: 0.79651433/1562 [==========================>...] - ETA: 8s - loss: 0.6153 - acc: 0.79781562/1562 [==============================] - 106s 68ms/step - loss: 0.6148 - acc: 0.7976 - val_loss: 0.5730 - val_acc: 0.8120\n",
      "Epoch 29/200\n",
      " 428/1562 [=======>......................] - ETA: 1:13 - loss: 0.6061 - acc: 0.80261562/1562 [==============================] - 106s 68ms/step - loss: 0.6129 - acc: 0.7984 - val_loss: 0.5715 - val_acc: 0.8131\n",
      "Epoch 30/200\n",
      "  47/1562 [..............................] - ETA: 1:37 - loss: 0.6070 - acc: 0.79721394/1562 [=========================>....] - ETA: 11s - loss: 0.6010 - acc: 0.80251562/1562 [==============================] - 107s 69ms/step - loss: 0.6012 - acc: 0.8028 - val_loss: 0.6675 - val_acc: 0.7755\n",
      "Epoch 31/200\n",
      " 404/1562 [======>.......................] - ETA: 1:17 - loss: 0.5867 - acc: 0.80781562/1562 [==============================] - 107s 68ms/step - loss: 0.5870 - acc: 0.8070 - val_loss: 0.5533 - val_acc: 0.8175\n",
      "Epoch 32/200\n",
      "  44/1562 [..............................] - ETA: 1:41 - loss: 0.5498 - acc: 0.81821479/1562 [===========================>..] - ETA: 5s - loss: 0.5796 - acc: 0.80731562/1562 [==============================] - 107s 69ms/step - loss: 0.5805 - acc: 0.8072 - val_loss: 0.5687 - val_acc: 0.8142\n",
      "Epoch 33/200\n",
      " 433/1562 [=======>......................] - ETA: 1:13 - loss: 0.5851 - acc: 0.80721562/1562 [==============================] - 107s 68ms/step - loss: 0.5828 - acc: 0.8078 - val_loss: 0.5182 - val_acc: 0.8297\n",
      "Epoch 34/200\n",
      "  40/1562 [..............................] - ETA: 1:36 - loss: 0.5835 - acc: 0.80781325/1562 [========================>.....] - ETA: 15s - loss: 0.5711 - acc: 0.81131562/1562 [==============================] - 109s 70ms/step - loss: 0.5706 - acc: 0.8116 - val_loss: 0.5405 - val_acc: 0.8228\n",
      "Epoch 35/200\n",
      " 382/1562 [======>.......................] - ETA: 1:18 - loss: 0.5637 - acc: 0.81341562/1562 [==============================] - 110s 70ms/step - loss: 0.5694 - acc: 0.8129 - val_loss: 0.5985 - val_acc: 0.8017\n",
      "Epoch 36/200\n",
      "  27/1562 [..............................] - ETA: 1:40 - loss: 0.5318 - acc: 0.81601273/1562 [=======================>......] - ETA: 19s - loss: 0.5659 - acc: 0.81321562/1562 [==============================] - 111s 71ms/step - loss: 0.5623 - acc: 0.8149 - val_loss: 0.5587 - val_acc: 0.8166\n",
      "Epoch 37/200\n",
      " 361/1562 [=====>........................] - ETA: 1:19 - loss: 0.5714 - acc: 0.81101562/1562 [==============================] - 111s 71ms/step - loss: 0.5576 - acc: 0.8165 - val_loss: 0.5340 - val_acc: 0.8226\n",
      "Epoch 38/200\n",
      "   7/1562 [..............................] - ETA: 1:44 - loss: 0.5265 - acc: 0.82141303/1562 [========================>.....] - ETA: 17s - loss: 0.5492 - acc: 0.81891562/1562 [==============================] - 112s 72ms/step - loss: 0.5501 - acc: 0.8184 - val_loss: 0.5314 - val_acc: 0.8248\n",
      "Epoch 39/200\n",
      " 369/1562 [======>.......................] - ETA: 1:21 - loss: 0.5375 - acc: 0.82491562/1562 [==============================] - 111s 71ms/step - loss: 0.5474 - acc: 0.8181 - val_loss: 0.5420 - val_acc: 0.8218\n",
      "Epoch 40/200\n",
      "  11/1562 [..............................] - ETA: 1:36 - loss: 0.5036 - acc: 0.82391249/1562 [======================>.......] - ETA: 21s - loss: 0.5391 - acc: 0.82161562/1562 [==============================] - 112s 72ms/step - loss: 0.5395 - acc: 0.8222 - val_loss: 0.5301 - val_acc: 0.8285\n",
      "Epoch 41/200\n",
      " 353/1562 [=====>........................] - ETA: 1:23 - loss: 0.5190 - acc: 0.82811562/1562 [==============================] - 111s 71ms/step - loss: 0.5291 - acc: 0.8256 - val_loss: 0.5606 - val_acc: 0.8135\n",
      "Epoch 42/200\n",
      "   9/1562 [..............................] - ETA: 1:44 - loss: 0.5980 - acc: 0.79171321/1562 [========================>.....] - ETA: 16s - loss: 0.5274 - acc: 0.82591562/1562 [==============================] - 111s 71ms/step - loss: 0.5278 - acc: 0.8263 - val_loss: 0.5187 - val_acc: 0.8294\n",
      "Epoch 43/200\n",
      " 381/1562 [======>.......................] - ETA: 1:18 - loss: 0.5228 - acc: 0.82861562/1562 [==============================] - 110s 71ms/step - loss: 0.5177 - acc: 0.8298 - val_loss: 0.5150 - val_acc: 0.8297\n",
      "Epoch 44/200\n",
      "  19/1562 [..............................] - ETA: 1:41 - loss: 0.5206 - acc: 0.82731297/1562 [=======================>......] - ETA: 18s - loss: 0.5195 - acc: 0.82961562/1562 [==============================] - 111s 71ms/step - loss: 0.5184 - acc: 0.8298 - val_loss: 0.5109 - val_acc: 0.8332\n",
      "Epoch 45/200\n",
      " 353/1562 [=====>........................] - ETA: 1:22 - loss: 0.5048 - acc: 0.83451562/1562 [==============================] - 112s 72ms/step - loss: 0.5127 - acc: 0.8317 - val_loss: 0.5213 - val_acc: 0.8291\n",
      "Epoch 46/200\n",
      "   3/1562 [..............................] - ETA: 1:33 - loss: 0.5361 - acc: 0.80211228/1562 [======================>.......] - ETA: 22s - loss: 0.5088 - acc: 0.83011562/1562 [==============================] - 111s 71ms/step - loss: 0.5093 - acc: 0.8302 - val_loss: 0.5069 - val_acc: 0.8325\n",
      "Epoch 47/200\n",
      " 339/1562 [=====>........................] - ETA: 1:21 - loss: 0.5017 - acc: 0.83161562/1562 [==============================] - 110s 70ms/step - loss: 0.4982 - acc: 0.8365 - val_loss: 0.5374 - val_acc: 0.8166\n",
      "Epoch 48/200\n",
      "   1/1562 [..............................] - ETA: 1:29 - loss: 0.4595 - acc: 0.84381561/1562 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.83741562/1562 [==============================] - 111s 71ms/step - loss: 0.4949 - acc: 0.8374 - val_loss: 0.4912 - val_acc: 0.8406\n",
      "Epoch 49/200\n",
      " 471/1562 [========>.....................] - ETA: 1:12 - loss: 0.5011 - acc: 0.83461562/1562 [==============================] - 110s 70ms/step - loss: 0.4978 - acc: 0.8344 - val_loss: 0.4997 - val_acc: 0.8358\n",
      "Epoch 50/200\n",
      "  60/1562 [>.............................] - ETA: 1:34 - loss: 0.4772 - acc: 0.83591284/1562 [=======================>......] - ETA: 18s - loss: 0.4895 - acc: 0.83911562/1562 [==============================] - 108s 69ms/step - loss: 0.4905 - acc: 0.8379 - val_loss: 0.5280 - val_acc: 0.8274\n",
      "Epoch 51/200\n",
      " 367/1562 [======>.......................] - ETA: 1:18 - loss: 0.4836 - acc: 0.83911562/1562 [==============================] - 110s 70ms/step - loss: 0.4853 - acc: 0.8395 - val_loss: 0.5054 - val_acc: 0.8353\n",
      "Epoch 52/200\n",
      "  20/1562 [..............................] - ETA: 1:38 - loss: 0.4957 - acc: 0.82971370/1562 [=========================>....] - ETA: 12s - loss: 0.4869 - acc: 0.83701562/1562 [==============================] - 109s 70ms/step - loss: 0.4856 - acc: 0.8379 - val_loss: 0.5213 - val_acc: 0.8252\n",
      "Epoch 53/200\n",
      " 407/1562 [======>.......................] - ETA: 1:17 - loss: 0.4756 - acc: 0.84341562/1562 [==============================] - 110s 70ms/step - loss: 0.4737 - acc: 0.8449 - val_loss: 0.5282 - val_acc: 0.8254\n",
      "Epoch 54/200\n",
      "  29/1562 [..............................] - ETA: 1:28 - loss: 0.4710 - acc: 0.84591315/1562 [========================>.....] - ETA: 16s - loss: 0.4778 - acc: 0.84231562/1562 [==============================] - 108s 69ms/step - loss: 0.4751 - acc: 0.8431 - val_loss: 0.4969 - val_acc: 0.8382\n",
      "Epoch 55/200\n",
      " 369/1562 [======>.......................] - ETA: 1:20 - loss: 0.4666 - acc: 0.84541562/1562 [==============================] - 109s 70ms/step - loss: 0.4693 - acc: 0.8460 - val_loss: 0.5379 - val_acc: 0.8243\n",
      "Epoch 56/200\n",
      "  18/1562 [..............................] - ETA: 1:34 - loss: 0.4510 - acc: 0.85591280/1562 [=======================>......] - ETA: 19s - loss: 0.4623 - acc: 0.84621562/1562 [==============================] - 111s 71ms/step - loss: 0.4654 - acc: 0.8454 - val_loss: 0.5254 - val_acc: 0.8243\n",
      "Epoch 57/200\n",
      " 358/1562 [=====>........................] - ETA: 1:21 - loss: 0.4529 - acc: 0.85061562/1562 [==============================] - 108s 69ms/step - loss: 0.4624 - acc: 0.8473 - val_loss: 0.4825 - val_acc: 0.8417\n",
      "Epoch 58/200\n",
      "  18/1562 [..............................] - ETA: 2:02 - loss: 0.4684 - acc: 0.84901340/1562 [========================>.....] - ETA: 14s - loss: 0.4539 - acc: 0.85121562/1562 [==============================] - 110s 70ms/step - loss: 0.4541 - acc: 0.8511 - val_loss: 0.4866 - val_acc: 0.8390\n",
      "Epoch 59/200\n",
      " 393/1562 [======>.......................] - ETA: 1:17 - loss: 0.4639 - acc: 0.84651562/1562 [==============================] - 109s 70ms/step - loss: 0.4598 - acc: 0.8497 - val_loss: 0.4891 - val_acc: 0.8393\n",
      "Epoch 60/200\n",
      "  32/1562 [..............................] - ETA: 1:35 - loss: 0.4295 - acc: 0.86231383/1562 [=========================>....] - ETA: 11s - loss: 0.4486 - acc: 0.85221562/1562 [==============================] - 109s 70ms/step - loss: 0.4472 - acc: 0.8524 - val_loss: 0.5002 - val_acc: 0.8341\n",
      "Epoch 61/200\n",
      " 404/1562 [======>.......................] - ETA: 1:17 - loss: 0.4493 - acc: 0.85301562/1562 [==============================] - 109s 70ms/step - loss: 0.4462 - acc: 0.8532 - val_loss: 0.5168 - val_acc: 0.8277\n",
      "Epoch 62/200\n",
      "  35/1562 [..............................] - ETA: 1:36 - loss: 0.4727 - acc: 0.84291363/1562 [=========================>....] - ETA: 13s - loss: 0.4471 - acc: 0.85301562/1562 [==============================] - 108s 69ms/step - loss: 0.4458 - acc: 0.8534 - val_loss: 0.5081 - val_acc: 0.8304\n",
      "Epoch 63/200\n",
      " 398/1562 [======>.......................] - ETA: 1:17 - loss: 0.4413 - acc: 0.85571562/1562 [==============================] - 110s 70ms/step - loss: 0.4444 - acc: 0.8532 - val_loss: 0.4840 - val_acc: 0.8390\n",
      "Epoch 64/200\n",
      "  26/1562 [..............................] - ETA: 1:33 - loss: 0.4517 - acc: 0.84621314/1562 [========================>.....] - ETA: 16s - loss: 0.4404 - acc: 0.85331562/1562 [==============================] - 107s 69ms/step - loss: 0.4400 - acc: 0.8538 - val_loss: 0.4825 - val_acc: 0.8407\n",
      "Epoch 65/200\n",
      " 387/1562 [======>.......................] - ETA: 1:16 - loss: 0.4371 - acc: 0.85381562/1562 [==============================] - 109s 70ms/step - loss: 0.4354 - acc: 0.8559 - val_loss: 0.5569 - val_acc: 0.8179\n",
      "Epoch 66/200\n",
      "  27/1562 [..............................] - ETA: 1:27 - loss: 0.4336 - acc: 0.86111420/1562 [==========================>...] - ETA: 9s - loss: 0.4356 - acc: 0.85781562/1562 [==============================] - 109s 70ms/step - loss: 0.4365 - acc: 0.8572 - val_loss: 0.4726 - val_acc: 0.8438\n",
      "Epoch 67/200\n",
      " 412/1562 [======>.......................] - ETA: 1:16 - loss: 0.4262 - acc: 0.85851562/1562 [==============================] - 109s 70ms/step - loss: 0.4319 - acc: 0.8572 - val_loss: 0.5198 - val_acc: 0.8274\n",
      "Epoch 68/200\n",
      "  29/1562 [..............................] - ETA: 1:42 - loss: 0.4677 - acc: 0.83841243/1562 [======================>.......] - ETA: 21s - loss: 0.4284 - acc: 0.85711562/1562 [==============================] - 112s 72ms/step - loss: 0.4281 - acc: 0.8579 - val_loss: 0.4852 - val_acc: 0.8404\n",
      "Epoch 69/200\n",
      " 341/1562 [=====>........................] - ETA: 1:25 - loss: 0.4206 - acc: 0.86391562/1562 [==============================] - 78s 50ms/step - loss: 0.4274 - acc: 0.8579 - val_loss: 0.4782 - val_acc: 0.8463\n",
      "Epoch 70/200\n",
      " 324/1562 [=====>........................] - ETA: 49s - loss: 0.4131 - acc: 0.86411562/1562 [==============================] - 66s 42ms/step - loss: 0.4202 - acc: 0.8610 - val_loss: 0.5281 - val_acc: 0.8259\n",
      "Epoch 71/200\n",
      " 337/1562 [=====>........................] - ETA: 50s - loss: 0.4136 - acc: 0.86021562/1562 [==============================] - 66s 42ms/step - loss: 0.4206 - acc: 0.8594 - val_loss: 0.4918 - val_acc: 0.8393\n",
      "Epoch 72/200\n",
      " 345/1562 [=====>........................] - ETA: 49s - loss: 0.4095 - acc: 0.86391562/1562 [==============================] - 67s 43ms/step - loss: 0.4096 - acc: 0.8640 - val_loss: 0.4742 - val_acc: 0.8434\n",
      "Epoch 73/200\n",
      " 341/1562 [=====>........................] - ETA: 49s - loss: 0.4121 - acc: 0.86571562/1562 [==============================] - 66s 42ms/step - loss: 0.4130 - acc: 0.8646 - val_loss: 0.4667 - val_acc: 0.8437\n",
      "Epoch 74/200\n",
      " 344/1562 [=====>........................] - ETA: 49s - loss: 0.4024 - acc: 0.86861562/1562 [==============================] - 65s 42ms/step - loss: 0.4082 - acc: 0.8648 - val_loss: 0.4695 - val_acc: 0.8434\n",
      "Epoch 75/200\n",
      " 349/1562 [=====>........................] - ETA: 49s - loss: 0.4135 - acc: 0.86151562/1562 [==============================] - 65s 42ms/step - loss: 0.4128 - acc: 0.8639 - val_loss: 0.4615 - val_acc: 0.8482\n",
      "Epoch 76/200\n",
      " 360/1562 [=====>........................] - ETA: 49s - loss: 0.4021 - acc: 0.87021562/1562 [==============================] - 65s 42ms/step - loss: 0.3967 - acc: 0.8704 - val_loss: 0.4914 - val_acc: 0.8393\n",
      "Epoch 77/200\n",
      " 357/1562 [=====>........................] - ETA: 49s - loss: 0.4009 - acc: 0.86971562/1562 [==============================] - 66s 42ms/step - loss: 0.3999 - acc: 0.8685 - val_loss: 0.5024 - val_acc: 0.8371\n",
      "Epoch 78/200\n",
      " 358/1562 [=====>........................] - ETA: 49s - loss: 0.3975 - acc: 0.86851562/1562 [==============================] - 66s 42ms/step - loss: 0.3956 - acc: 0.8688 - val_loss: 0.4650 - val_acc: 0.8476\n",
      "Epoch 79/200\n",
      " 359/1562 [=====>........................] - ETA: 48s - loss: 0.3966 - acc: 0.86831562/1562 [==============================] - 65s 41ms/step - loss: 0.3950 - acc: 0.8696 - val_loss: 0.4978 - val_acc: 0.8382\n",
      "Epoch 80/200\n",
      " 358/1562 [=====>........................] - ETA: 49s - loss: 0.3872 - acc: 0.87511562/1562 [==============================] - 65s 42ms/step - loss: 0.3946 - acc: 0.8707 - val_loss: 0.4701 - val_acc: 0.8452\n",
      "Epoch 81/200\n",
      " 382/1562 [======>.......................] - ETA: 47s - loss: 0.3874 - acc: 0.87301562/1562 [==============================] - 65s 42ms/step - loss: 0.3875 - acc: 0.8725 - val_loss: 0.4713 - val_acc: 0.8444\n",
      "Epoch 82/200\n",
      " 376/1562 [======>.......................] - ETA: 47s - loss: 0.3777 - acc: 0.87701562/1562 [==============================] - 65s 42ms/step - loss: 0.3816 - acc: 0.8746 - val_loss: 0.4794 - val_acc: 0.8437\n",
      "Epoch 83/200\n",
      " 381/1562 [======>.......................] - ETA: 46s - loss: 0.3873 - acc: 0.87021562/1562 [==============================] - 66s 42ms/step - loss: 0.3850 - acc: 0.8727 - val_loss: 0.4751 - val_acc: 0.8452\n",
      "Epoch 84/200\n",
      " 358/1562 [=====>........................] - ETA: 48s - loss: 0.3803 - acc: 0.87391562/1562 [==============================] - 66s 42ms/step - loss: 0.3842 - acc: 0.8732 - val_loss: 0.4636 - val_acc: 0.8501\n",
      "Epoch 85/200\n",
      " 354/1562 [=====>........................] - ETA: 48s - loss: 0.3748 - acc: 0.87741562/1562 [==============================] - 65s 42ms/step - loss: 0.3788 - acc: 0.8748 - val_loss: 0.4863 - val_acc: 0.8412\n",
      "Epoch 86/200\n",
      " 359/1562 [=====>........................] - ETA: 48s - loss: 0.3746 - acc: 0.87771562/1562 [==============================] - 65s 42ms/step - loss: 0.3812 - acc: 0.8758 - val_loss: 0.4852 - val_acc: 0.8426\n",
      "Epoch 87/200\n",
      " 357/1562 [=====>........................] - ETA: 49s - loss: 0.3752 - acc: 0.87591562/1562 [==============================] - 66s 42ms/step - loss: 0.3739 - acc: 0.8774 - val_loss: 0.5106 - val_acc: 0.8341\n",
      "Epoch 88/200\n",
      " 354/1562 [=====>........................] - ETA: 48s - loss: 0.3692 - acc: 0.88211562/1562 [==============================] - 65s 41ms/step - loss: 0.3727 - acc: 0.8776 - val_loss: 0.5174 - val_acc: 0.8366\n",
      "Epoch 89/200\n",
      " 375/1562 [======>.......................] - ETA: 47s - loss: 0.3684 - acc: 0.87721562/1562 [==============================] - 65s 42ms/step - loss: 0.3738 - acc: 0.8752 - val_loss: 0.4841 - val_acc: 0.8410\n",
      "Epoch 90/200\n",
      " 367/1562 [======>.......................] - ETA: 48s - loss: 0.3672 - acc: 0.87671562/1562 [==============================] - 66s 42ms/step - loss: 0.3660 - acc: 0.8789 - val_loss: 0.4631 - val_acc: 0.8493\n",
      "Epoch 91/200\n",
      " 352/1562 [=====>........................] - ETA: 49s - loss: 0.3658 - acc: 0.87741562/1562 [==============================] - 67s 43ms/step - loss: 0.3660 - acc: 0.8791 - val_loss: 0.4562 - val_acc: 0.8472\n",
      "Epoch 92/200\n",
      " 329/1562 [=====>........................] - ETA: 50s - loss: 0.3505 - acc: 0.88251562/1562 [==============================] - 66s 42ms/step - loss: 0.3635 - acc: 0.8799 - val_loss: 0.5059 - val_acc: 0.8374\n",
      "Epoch 93/200\n",
      " 334/1562 [=====>........................] - ETA: 50s - loss: 0.3393 - acc: 0.88731562/1562 [==============================] - 65s 42ms/step - loss: 0.3626 - acc: 0.8801 - val_loss: 0.4868 - val_acc: 0.8428\n",
      "Epoch 94/200\n",
      " 352/1562 [=====>........................] - ETA: 48s - loss: 0.3613 - acc: 0.88081562/1562 [==============================] - 65s 42ms/step - loss: 0.3582 - acc: 0.8826 - val_loss: 0.4672 - val_acc: 0.8457\n",
      "Epoch 95/200\n",
      " 377/1562 [======>.......................] - ETA: 47s - loss: 0.3625 - acc: 0.87911562/1562 [==============================] - 65s 42ms/step - loss: 0.3603 - acc: 0.8815 - val_loss: 0.4632 - val_acc: 0.8489\n",
      "Epoch 96/200\n",
      " 379/1562 [======>.......................] - ETA: 46s - loss: 0.3620 - acc: 0.88201562/1562 [==============================] - 65s 41ms/step - loss: 0.3592 - acc: 0.8830 - val_loss: 0.4465 - val_acc: 0.8566\n",
      "Epoch 97/200\n",
      " 371/1562 [======>.......................] - ETA: 47s - loss: 0.3578 - acc: 0.88101562/1562 [==============================] - 66s 42ms/step - loss: 0.3554 - acc: 0.8811 - val_loss: 0.4803 - val_acc: 0.8453\n",
      "Epoch 98/200\n",
      " 358/1562 [=====>........................] - ETA: 48s - loss: 0.3401 - acc: 0.88701562/1562 [==============================] - 65s 42ms/step - loss: 0.3570 - acc: 0.8817 - val_loss: 0.4924 - val_acc: 0.8419\n",
      "Epoch 99/200\n",
      " 361/1562 [=====>........................] - ETA: 48s - loss: 0.3480 - acc: 0.88561562/1562 [==============================] - 65s 42ms/step - loss: 0.3481 - acc: 0.8861 - val_loss: 0.4728 - val_acc: 0.8420\n",
      "Epoch 100/200\n",
      " 352/1562 [=====>........................] - ETA: 48s - loss: 0.3535 - acc: 0.88531562/1562 [==============================] - 66s 42ms/step - loss: 0.3504 - acc: 0.8842 - val_loss: 0.4650 - val_acc: 0.8485\n",
      "Epoch 101/200\n",
      " 363/1562 [=====>........................] - ETA: 47s - loss: 0.3484 - acc: 0.88481562/1562 [==============================] - 65s 41ms/step - loss: 0.3494 - acc: 0.8857 - val_loss: 0.4853 - val_acc: 0.8429\n",
      "Epoch 102/200\n",
      " 376/1562 [======>.......................] - ETA: 47s - loss: 0.3442 - acc: 0.88411562/1562 [==============================] - 65s 42ms/step - loss: 0.3450 - acc: 0.8862 - val_loss: 0.4450 - val_acc: 0.8544\n",
      "Epoch 103/200\n",
      " 355/1562 [=====>........................] - ETA: 49s - loss: 0.3442 - acc: 0.88211562/1562 [==============================] - 66s 42ms/step - loss: 0.3436 - acc: 0.8848 - val_loss: 0.4692 - val_acc: 0.8438\n",
      "Epoch 104/200\n",
      " 358/1562 [=====>........................] - ETA: 48s - loss: 0.3446 - acc: 0.89001562/1562 [==============================] - 66s 42ms/step - loss: 0.3474 - acc: 0.8872 - val_loss: 0.4799 - val_acc: 0.8488\n",
      "Epoch 105/200\n",
      " 356/1562 [=====>........................] - ETA: 48s - loss: 0.3338 - acc: 0.89171562/1562 [==============================] - 64s 41ms/step - loss: 0.3422 - acc: 0.8884 - val_loss: 0.4698 - val_acc: 0.8451\n",
      "Epoch 106/200\n",
      " 382/1562 [======>.......................] - ETA: 46s - loss: 0.3297 - acc: 0.89301562/1562 [==============================] - 64s 41ms/step - loss: 0.3325 - acc: 0.8910 - val_loss: 0.4795 - val_acc: 0.8474\n",
      "Epoch 107/200\n",
      " 369/1562 [======>.......................] - ETA: 48s - loss: 0.3407 - acc: 0.88781562/1562 [==============================] - 65s 42ms/step - loss: 0.3365 - acc: 0.8890 - val_loss: 0.4880 - val_acc: 0.8445\n",
      "Epoch 108/200\n",
      " 345/1562 [=====>........................] - ETA: 49s - loss: 0.3200 - acc: 0.89591562/1562 [==============================] - 66s 42ms/step - loss: 0.3335 - acc: 0.8897 - val_loss: 0.4510 - val_acc: 0.8524\n",
      "Epoch 109/200\n",
      " 351/1562 [=====>........................] - ETA: 48s - loss: 0.3322 - acc: 0.88841562/1562 [==============================] - 65s 42ms/step - loss: 0.3294 - acc: 0.8912 - val_loss: 0.4709 - val_acc: 0.8468\n",
      "Epoch 110/200\n",
      " 367/1562 [======>.......................] - ETA: 48s - loss: 0.3284 - acc: 0.88991562/1562 [==============================] - 65s 42ms/step - loss: 0.3300 - acc: 0.8911 - val_loss: 0.4566 - val_acc: 0.8514\n",
      "Epoch 111/200\n",
      " 354/1562 [=====>........................] - ETA: 49s - loss: 0.3282 - acc: 0.89231562/1562 [==============================] - 66s 42ms/step - loss: 0.3353 - acc: 0.8916 - val_loss: 0.4299 - val_acc: 0.8591\n",
      "Epoch 112/200\n",
      " 345/1562 [=====>........................] - ETA: 49s - loss: 0.3291 - acc: 0.89151562/1562 [==============================] - 65s 42ms/step - loss: 0.3288 - acc: 0.8912 - val_loss: 0.4754 - val_acc: 0.8459\n",
      "Epoch 113/200\n",
      "1562/1562 [==============================] - 66s 42ms/step - loss: 0.3267 - acc: 0.8929 - val_loss: 0.4535 - val_acc: 0.8508\n",
      "Epoch 114/200\n",
      " 337/1562 [=====>........................] - ETA: 50s - loss: 0.3242 - acc: 0.89391562/1562 [==============================] - 65s 42ms/step - loss: 0.3258 - acc: 0.8926 - val_loss: 0.4484 - val_acc: 0.8561\n",
      "Epoch 115/200\n",
      " 374/1562 [======>.......................] - ETA: 47s - loss: 0.3256 - acc: 0.89011562/1562 [==============================] - 65s 42ms/step - loss: 0.3283 - acc: 0.8909 - val_loss: 0.4403 - val_acc: 0.8554\n",
      "Epoch 116/200\n",
      " 345/1562 [=====>........................] - ETA: 51s - loss: 0.3177 - acc: 0.89871562/1562 [==============================] - 66s 42ms/step - loss: 0.3197 - acc: 0.8961 - val_loss: 0.4538 - val_acc: 0.8542\n",
      "Epoch 117/200\n",
      " 361/1562 [=====>........................] - ETA: 48s - loss: 0.3226 - acc: 0.89671562/1562 [==============================] - 65s 42ms/step - loss: 0.3229 - acc: 0.8946 - val_loss: 0.4639 - val_acc: 0.8483\n",
      "Epoch 118/200\n",
      " 362/1562 [=====>........................] - ETA: 49s - loss: 0.3142 - acc: 0.89451562/1562 [==============================] - 66s 42ms/step - loss: 0.3179 - acc: 0.8957 - val_loss: 0.4535 - val_acc: 0.8538\n",
      "Epoch 119/200\n",
      " 370/1562 [======>.......................] - ETA: 47s - loss: 0.3130 - acc: 0.89801562/1562 [==============================] - 64s 41ms/step - loss: 0.3166 - acc: 0.8969 - val_loss: 0.4606 - val_acc: 0.8481\n",
      "Epoch 120/200\n",
      " 378/1562 [======>.......................] - ETA: 47s - loss: 0.3036 - acc: 0.90371562/1562 [==============================] - 65s 41ms/step - loss: 0.3149 - acc: 0.8965 - val_loss: 0.4521 - val_acc: 0.8496\n",
      "Epoch 121/200\n",
      " 384/1562 [======>.......................] - ETA: 45s - loss: 0.3122 - acc: 0.89431562/1562 [==============================] - 64s 41ms/step - loss: 0.3143 - acc: 0.8951 - val_loss: 0.4980 - val_acc: 0.8390\n",
      "Epoch 122/200\n",
      " 404/1562 [======>.......................] - ETA: 44s - loss: 0.3125 - acc: 0.89761562/1562 [==============================] - 64s 41ms/step - loss: 0.3115 - acc: 0.8973 - val_loss: 0.4856 - val_acc: 0.8472\n",
      "Epoch 123/200\n",
      " 378/1562 [======>.......................] - ETA: 47s - loss: 0.3020 - acc: 0.90131562/1562 [==============================] - 65s 42ms/step - loss: 0.3085 - acc: 0.8983 - val_loss: 0.5210 - val_acc: 0.8354\n",
      "Epoch 124/200\n",
      " 349/1562 [=====>........................] - ETA: 49s - loss: 0.3062 - acc: 0.891562/1562 [==============================] - 65s 42ms/step - loss: 0.3115 - acc: 0.8968 - val_loss: 0.4577 - val_acc: 0.8488\n",
      "Epoch 125/200\n",
      " 362/1562 [=====>........................] - ETA: 48s - loss: 0.2950 - acc: 0.90381562/1562 [==============================] - 65s 41ms/step - loss: 0.3082 - acc: 0.8987 - val_loss: 0.4355 - val_acc: 0.8564\n",
      "Epoch 126/200\n",
      " 373/1562 [======>.......................] - ETA: 47s - loss: 0.3016 - acc: 0.89961562/1562 [==============================] - 65s 42ms/step - loss: 0.3050 - acc: 0.9007 - val_loss: 0.5065 - val_acc: 0.8382\n",
      "Epoch 127/200\n",
      " 372/1562 [======>.......................] - ETA: 46s - loss: 0.2990 - acc: 0.89941562/1562 [==============================] - 64s 41ms/step - loss: 0.3078 - acc: 0.8975 - val_loss: 0.4594 - val_acc: 0.8501\n",
      "Epoch 128/200\n",
      " 348/1562 [=====>........................] - ETA: 49s - loss: 0.3068 - acc: 0.90281562/1562 [==============================] - 66s 42ms/step - loss: 0.3114 - acc: 0.8979 - val_loss: 0.4393 - val_acc: 0.8568\n",
      "Epoch 129/200\n",
      " 346/1562 [=====>........................] - ETA: 49s - loss: 0.2985 - acc: 0.90251562/1562 [==============================] - 65s 42ms/step - loss: 0.3004 - acc: 0.9013 - val_loss: 0.4401 - val_acc: 0.8557\n",
      "Epoch 130/200\n",
      " 356/1562 [=====>........................] - ETA: 48s - loss: 0.3008 - acc: 0.89981562/1562 [==============================] - 65s 41ms/step - loss: 0.3023 - acc: 0.9002 - val_loss: 0.4828 - val_acc: 0.8438\n",
      "Epoch 131/200\n",
      " 372/1562 [======>.......................] - ETA: 47s - loss: 0.3060 - acc: 0.89911562/1562 [==============================] - 66s 42ms/step - loss: 0.3009 - acc: 0.8997 - val_loss: 0.4418 - val_acc: 0.8560\n",
      "Epoch 132/200\n",
      " 359/1562 [=====>........................] - ETA: 48s - loss: 0.2935 - acc: 0.90441562/1562 [==============================] - 65s 42ms/step - loss: 0.3014 - acc: 0.9013 - val_loss: 0.4518 - val_acc: 0.8528\n",
      "Epoch 133/200\n",
      " 373/1562 [======>.......................] - ETA: 47s - loss: 0.3080 - acc: 0.89951562/1562 [==============================] - 66s 42ms/step - loss: 0.3004 - acc: 0.9022 - val_loss: 0.4465 - val_acc: 0.8526\n",
      "Epoch 134/200\n",
      " 363/1562 [=====>........................] - ETA: 47s - loss: 0.3102 - acc: 0.89701562/1562 [==============================] - 66s 42ms/step - loss: 0.3009 - acc: 0.9018 - val_loss: 0.4517 - val_acc: 0.8508\n",
      "Epoch 135/200\n",
      " 373/1562 [======>.......................] - ETA: 46s - loss: 0.2936 - acc: 0.90751562/1562 [==============================] - 66s 42ms/step - loss: 0.2909 - acc: 0.9053 - val_loss: 0.4562 - val_acc: 0.8527\n",
      "Epoch 136/200\n",
      " 355/1562 [=====>........................] - ETA: 49s - loss: 0.2980 - acc: 0.90291562/1562 [==============================] - 67s 43ms/step - loss: 0.2942 - acc: 0.9046 - val_loss: 0.4421 - val_acc: 0.8545\n",
      "Epoch 137/200\n",
      " 342/1562 [=====>........................] - ETA: 49s - loss: 0.2936 - acc: 0.90301562/1562 [==============================] - 65s 42ms/step - loss: 0.2942 - acc: 0.9044 - val_loss: 0.4330 - val_acc: 0.8552\n",
      "Epoch 138/200\n",
      " 349/1562 [=====>........................] - ETA: 48s - loss: 0.2875 - acc: 0.90421562/1562 [==============================] - 66s 42ms/step - loss: 0.2939 - acc: 0.9035 - val_loss: 0.4724 - val_acc: 0.8467\n",
      "Epoch 139/200\n",
      " 354/1562 [=====>........................] - ETA: 49s - loss: 0.2971 - acc: 0.90221562/1562 [==============================] - 65s 42ms/step - loss: 0.2902 - acc: 0.9052 - val_loss: 0.4391 - val_acc: 0.8571\n",
      "Epoch 140/200\n",
      " 345/1562 [=====>........................] - ETA: 50s - loss: 0.2896 - acc: 0.90341562/1562 [==============================] - 66s 42ms/step - loss: 0.2906 - acc: 0.9049 - val_loss: 0.4528 - val_acc: 0.8557\n",
      "Epoch 141/200\n",
      " 353/1562 [=====>........................] - ETA: 48s - loss: 0.2829 - acc: 0.90721562/1562 [==============================] - 66s 42ms/step - loss: 0.2857 - acc: 0.9064 - val_loss: 0.4438 - val_acc: 0.8548\n",
      "Epoch 142/200\n",
      " 336/1562 [=====>........................] - ETA: 50s - loss: 0.2859 - acc: 0.90721562/1562 [==============================] - 66s 42ms/step - loss: 0.2879 - acc: 0.9066 - val_loss: 0.4521 - val_acc: 0.8549\n",
      "Epoch 143/200\n",
      " 351/1562 [=====>........................] - ETA: 47s - loss: 0.2914 - acc: 0.90381562/1562 [==============================] - 65s 41ms/step - loss: 0.2863 - acc: 0.9065 - val_loss: 0.4751 - val_acc: 0.8493\n",
      "Epoch 144/200\n",
      " 371/1562 [======>.......................] - ETA: 48s - loss: 0.2880 - acc: 0.90511562/1562 [==============================] - 65s 42ms/step - loss: 0.2843 - acc: 0.9063 - val_loss: 0.4454 - val_acc: 0.8561\n",
      "Epoch 145/200\n",
      " 377/1562 [======>.......................] - ETA: 47s - loss: 0.2876 - acc: 0.90431562/1562 [==============================] - 65s 42ms/step - loss: 0.2847 - acc: 0.9054 - val_loss: 0.4655 - val_acc: 0.8497\n",
      "Epoch 146/200\n",
      " 353/1562 [=====>........................] - ETA: 49s - loss: 0.2698 - acc: 0.91281562/1562 [==============================] - 65s 41ms/step - loss: 0.2849 - acc: 0.9061 - val_loss: 0.4684 - val_acc: 0.8525\n",
      "Epoch 147/200\n",
      " 363/1562 [=====>........................] - ETA: 48s - loss: 0.2747 - acc: 0.90951562/1562 [==============================] - 65s 42ms/step - loss: 0.2754 - acc: 0.9105 - val_loss: 0.4858 - val_acc: 0.8461\n",
      "Epoch 148/200\n",
      " 356/1562 [=====>........................] - ETA: 49s - loss: 0.2916 - acc: 0.90351562/1562 [==============================] - 65s 42ms/step - loss: 0.2806 - acc: 0.9077 - val_loss: 0.4396 - val_acc: 0.8589\n",
      "Epoch 149/200\n",
      " 372/1562 [======>.......................] - ETA: 47s - loss: 0.2875 - acc: 0.90721562/1562 [==============================] - 65s 42ms/step - loss: 0.2818 - acc: 0.9076 - val_loss: 0.4584 - val_acc: 0.8543\n",
      "Epoch 150/200\n",
      " 367/1562 [======>.......................] - ETA: 47s - loss: 0.2847 - acc: 0.90831562/1562 [==============================] - 66s 42ms/step - loss: 0.2787 - acc: 0.9083 - val_loss: 0.4751 - val_acc: 0.8460\n",
      "Epoch 151/200\n",
      " 360/1562 [=====>........................] - ETA: 48s - loss: 0.2852 - acc: 0.90851562/1562 [==============================] - 66s 42ms/step - loss: 0.2826 - acc: 0.9074 - val_loss: 0.4484 - val_acc: 0.8565\n",
      "Epoch 152/200\n",
      " 358/1562 [=====>........................] - ETA: 49s - loss: 0.2787 - acc: 0.90801562/1562 [==============================] - 66s 42ms/step - loss: 0.2750 - acc: 0.9095 - val_loss: 0.4758 - val_acc: 0.8446\n",
      "Epoch 153/200\n",
      " 343/1562 [=====>........................] - ETA: 49s - loss: 0.2846 - acc: 0.90641562/1562 [==============================] - 66s 43ms/step - loss: 0.2769 - acc: 0.9079 - val_loss: 0.4883 - val_acc: 0.8456\n",
      "Epoch 154/200\n",
      " 348/1562 [=====>........................] - ETA: 49s - loss: 0.2792 - acc: 0.91041562/1562 [==============================] - 65s 42ms/step - loss: 0.2738 - acc: 0.9111 - val_loss: 0.4511 - val_acc: 0.8541\n",
      "Epoch 155/200\n",
      " 347/1562 [=====>........................] - ETA: 48s - loss: 0.2716 - acc: 0.91131562/1562 [==============================] - 66s 42ms/step - loss: 0.2709 - acc: 0.9121 - val_loss: 0.4389 - val_acc: 0.8556\n",
      "Epoch 156/200\n",
      " 325/1562 [=====>........................] - ETA: 51s - loss: 0.2625 - acc: 0.91341562/1562 [==============================] - 66s 42ms/step - loss: 0.2660 - acc: 0.9125 - val_loss: 0.4583 - val_acc: 0.8519\n",
      "Epoch 157/200\n",
      " 322/1562 [=====>........................] - ETA: 52s - loss: 0.2669 - acc: 0.90971562/1562 [==============================] - 67s 43ms/step - loss: 0.2754 - acc: 0.9081 - val_loss: 0.4555 - val_acc: 0.8549\n",
      "Epoch 158/200\n",
      " 299/1562 [====>.........................] - ETA: 52s - loss: 0.2578 - acc: 0.91741562/1562 [==============================] - 66s 42ms/step - loss: 0.2653 - acc: 0.9135 - val_loss: 0.4415 - val_acc: 0.8565\n",
      "Epoch 159/200\n",
      " 352/1562 [=====>........................] - ETA: 48s - loss: 0.2710 - acc: 0.91111562/1562 [==============================] - 66s 42ms/step - loss: 0.2702 - acc: 0.9109 - val_loss: 0.4535 - val_acc: 0.8542\n",
      "Epoch 160/200\n",
      " 354/1562 [=====>........................] - ETA: 48s - loss: 0.2635 - acc: 0.91291562/1562 [==============================] - 65s 42ms/step - loss: 0.2675 - acc: 0.9126 - val_loss: 0.4476 - val_acc: 0.8566\n",
      "Epoch 161/200\n",
      " 337/1562 [=====>........................] - ETA: 50s - loss: 0.2732 - acc: 0.91111562/1562 [==============================] - 67s 43ms/step - loss: 0.2702 - acc: 0.9103 - val_loss: 0.4811 - val_acc: 0.8446\n",
      "Epoch 162/200\n",
      " 339/1562 [=====>........................] - ETA: 49s - loss: 0.2603 - acc: 0.91651562/1562 [==============================] - 65s 42ms/step - loss: 0.2623 - acc: 0.9148 - val_loss: 0.4615 - val_acc: 0.8530\n",
      "Epoch 163/200\n",
      " 340/1562 [=====>........................] - ETA: 50s - loss: 0.2816 - acc: 0.90561562/1562 [==============================] - 66s 42ms/step - loss: 0.2714 - acc: 0.9093 - val_loss: 0.4412 - val_acc: 0.8536\n",
      "Epoch 164/200\n",
      " 365/1562 [======>.......................] - ETA: 47s - loss: 0.2552 - acc: 0.91761562/1562 [==============================] - 65s 42ms/step - loss: 0.2665 - acc: 0.9129 - val_loss: 0.4452 - val_acc: 0.8567\n",
      "Epoch 165/200\n",
      " 375/1562 [======>.......................] - ETA: 47s - loss: 0.2599 - acc: 0.91791562/1562 [==============================] - 65s 42ms/step - loss: 0.2639 - acc: 0.9141 - val_loss: 0.4296 - val_acc: 0.8595\n",
      "Epoch 166/200\n",
      " 359/1562 [=====>........................] - ETA: 48s - loss: 0.2684 - acc: 0.91231562/1562 [==============================] - 66s 43ms/step - loss: 0.2646 - acc: 0.9134 - val_loss: 0.4823 - val_acc: 0.8461\n",
      "Epoch 167/200\n",
      " 343/1562 [=====>........................] - ETA: 50s - loss: 0.2622 - acc: 0.91611562/1562 [==============================] - 66s 42ms/step - loss: 0.2585 - acc: 0.9155 - val_loss: 0.4687 - val_acc: 0.8491\n",
      "Epoch 168/200\n",
      " 350/1562 [=====>........................] - ETA: 49s - loss: 0.2657 - acc: 0.91571562/1562 [==============================] - 65s 42ms/step - loss: 0.2598 - acc: 0.9145 - val_loss: 0.4991 - val_acc: 0.8463\n",
      "Epoch 169/200\n",
      " 367/1562 [======>.......................] - ETA: 48s - loss: 0.2611 - acc: 0.91841562/1562 [==============================] - 65s 42ms/step - loss: 0.2590 - acc: 0.9166 - val_loss: 0.4439 - val_acc: 0.8590\n",
      "Epoch 170/200\n",
      " 358/1562 [=====>........................] - ETA: 49s - loss: 0.2519 - acc: 0.91731562/1562 [==============================] - 67s 43ms/step - loss: 0.2578 - acc: 0.9159 - val_loss: 0.4707 - val_acc: 0.8469\n",
      "Epoch 171/200\n",
      " 352/1562 [=====>........................] - ETA: 49s - loss: 0.2517 - acc: 0.91571562/1562 [==============================] - 66s 42ms/step - loss: 0.2568 - acc: 0.9153 - val_loss: 0.4265 - val_acc: 0.8631\n",
      "Epoch 172/200\n",
      " 344/1562 [=====>........................] - ETA: 50s - loss: 0.2579 - acc: 0.91501562/1562 [==============================] - 66s 42ms/step - loss: 0.2607 - acc: 0.9136 - val_loss: 0.4393 - val_acc: 0.8586\n",
      "Epoch 173/200\n",
      " 357/1562 [=====>........................] - ETA: 49s - loss: 0.2592 - acc: 0.91741562/1562 [==============================] - 65s 42ms/step - loss: 0.2511 - acc: 0.9183 - val_loss: 0.4468 - val_acc: 0.8545\n",
      "Epoch 174/200\n",
      " 360/1562 [=====>........................] - ETA: 48s - loss: 0.2580 - acc: 0.91721562/1562 [==============================] - 66s 42ms/step - loss: 0.2559 - acc: 0.9177 - val_loss: 0.4618 - val_acc: 0.8528\n",
      "Epoch 175/200\n",
      " 340/1562 [=====>........................] - ETA: 49s - loss: 0.2450 - acc: 0.92281562/1562 [==============================] - 65s 42ms/step - loss: 0.2534 - acc: 0.9178 - val_loss: 0.4435 - val_acc: 0.8571\n",
      "Epoch 176/200\n",
      " 350/1562 [=====>........................] - ETA: 49s - loss: 0.2545 - acc: 0.91501562/1562 [==============================] - 65s 42ms/step - loss: 0.2557 - acc: 0.9162 - val_loss: 0.4392 - val_acc: 0.8567\n",
      "Epoch 177/200\n",
      " 366/1562 [======>.......................] - ETA: 47s - loss: 0.2562 - acc: 0.92021562/1562 [==============================] - 66s 42ms/step - loss: 0.2533 - acc: 0.9170 - val_loss: 0.4495 - val_acc: 0.8560\n",
      "Epoch 178/200\n",
      " 360/1562 [=====>........................] - ETA: 47s - loss: 0.2550 - acc: 0.91581562/1562 [==============================] - 65s 42ms/step - loss: 0.2549 - acc: 0.9166 - val_loss: 0.4401 - val_acc: 0.8606\n",
      "Epoch 179/200\n",
      " 344/1562 [=====>........................] - ETA: 50s - loss: 0.2535 - acc: 0.91631562/1562 [==============================] - 66s 42ms/step - loss: 0.2493 - acc: 0.9186 - val_loss: 0.4406 - val_acc: 0.8584\n",
      "Epoch 180/200\n",
      " 324/1562 [=====>........................] - ETA: 50s - loss: 0.2431 - acc: 0.92431562/1562 [==============================] - 66s 42ms/step - loss: 0.2485 - acc: 0.9195 - val_loss: 0.4572 - val_acc: 0.8526\n",
      "Epoch 181/200\n",
      " 339/1562 [=====>........................] - ETA: 49s - loss: 0.2425 - acc: 0.92071562/1562 [==============================] - 65s 42ms/step - loss: 0.2474 - acc: 0.9195 - val_loss: 0.4620 - val_acc: 0.8514\n",
      "Epoch 182/200\n",
      " 362/1562 [=====>........................] - ETA: 48s - loss: 0.2487 - acc: 0.91861562/1562 [==============================] - 66s 42ms/step - loss: 0.2464 - acc: 0.9192 - val_loss: 0.4664 - val_acc: 0.8511\n",
      "Epoch 183/200\n",
      " 363/1562 [=====>........................] - ETA: 47s - loss: 0.2445 - acc: 0.91881562/1562 [==============================] - 65s 42ms/step - loss: 0.2458 - acc: 0.9206 - val_loss: 0.4331 - val_acc: 0.8579\n",
      "Epoch 184/200\n",
      " 351/1562 [=====>........................] - ETA: 49s - loss: 0.2416 - acc: 0.92191562/1562 [==============================] - 66s 42ms/step - loss: 0.2428 - acc: 0.9209 - val_loss: 0.4734 - val_acc: 0.8522\n",
      "Epoch 185/200\n",
      " 340/1562 [=====>........................] - ETA: 50s - loss: 0.2618 - acc: 0.91251562/1562 [==============================] - 66s 42ms/step - loss: 0.2477 - acc: 0.9184 - val_loss: 0.4465 - val_acc: 0.8569\n",
      "Epoch 186/200\n",
      " 356/1562 [=====>........................] - ETA: 48s - loss: 0.2519 - acc: 0.91751562/1562 [==============================] - 66s 42ms/step - loss: 0.2460 - acc: 0.9200 - val_loss: 0.4721 - val_acc: 0.8510\n",
      "Epoch 187/200\n",
      " 345/1562 [=====>........................] - ETA: 50s - loss: 0.2424 - acc: 0.92341562/1562 [==============================] - 67s 43ms/step - loss: 0.2438 - acc: 0.9205 - val_loss: 0.4453 - val_acc: 0.8555\n",
      "Epoch 188/200\n",
      " 344/1562 [=====>........................] - ETA: 49s - loss: 0.2389 - acc: 0.92171562/1562 [==============================] - 66s 42ms/step - loss: 0.2449 - acc: 0.9192 - val_loss: 0.4200 - val_acc: 0.8642\n",
      "Epoch 189/200\n",
      " 317/1562 [=====>........................] - ETA: 52s - loss: 0.2382 - acc: 0.92321562/1562 [==============================] - 66s 42ms/step - loss: 0.2403 - acc: 0.9227 - val_loss: 0.4583 - val_acc: 0.8509\n",
      "Epoch 190/200\n",
      " 333/1562 [=====>........................] - ETA: 50s - loss: 0.2393 - acc: 0.92081562/1562 [==============================] - 65s 42ms/step - loss: 0.2428 - acc: 0.9205 - val_loss: 0.4566 - val_acc: 0.8538\n",
      "Epoch 191/200\n",
      " 366/1562 [======>.......................] - ETA: 47s - loss: 0.2422 - acc: 0.92071562/1562 [==============================] - 66s 42ms/step - loss: 0.2430 - acc: 0.9201 - val_loss: 0.4386 - val_acc: 0.8570\n",
      "Epoch 192/200\n",
      " 338/1562 [=====>........................] - ETA: 50s - loss: 0.2316 - acc: 0.92001562/1562 [==============================] - 67s 43ms/step - loss: 0.2393 - acc: 0.9215 - val_loss: 0.4565 - val_acc: 0.8527\n",
      "Epoch 193/200\n",
      " 343/1562 [=====>........................] - ETA: 49s - loss: 0.2394 - acc: 0.92091562/1562 [==============================] - 66s 42ms/step - loss: 0.2384 - acc: 0.9227 - val_loss: 0.4508 - val_acc: 0.8564\n",
      "Epoch 194/200\n",
      " 342/1562 [=====>........................] - ETA: 49s - loss: 0.2466 - acc: 0.91841562/1562 [==============================] - 66s 42ms/step - loss: 0.2381 - acc: 0.9229 - val_loss: 0.4459 - val_acc: 0.8560\n",
      "Epoch 195/200\n",
      " 340/1562 [=====>........................] - ETA: 50s - loss: 0.2372 - acc: 0.92361562/1562 [==============================] - 66s 42ms/step - loss: 0.2344 - acc: 0.9239 - val_loss: 0.4300 - val_acc: 0.8605\n",
      "Epoch 196/200\n",
      " 343/1562 [=====>........................] - ETA: 49s - loss: 0.2509 - acc: 0.91961562/1562 [==============================] - 65s 42ms/step - loss: 0.2411 - acc: 0.9225 - val_loss: 0.4530 - val_acc: 0.8571\n",
      "Epoch 197/200\n",
      " 331/1562 [=====>........................] - ETA: 50s - loss: 0.2363 - acc: 0.92381562/1562 [==============================] - 66s 42ms/step - loss: 0.2391 - acc: 0.9236 - val_loss: 0.4318 - val_acc: 0.8596\n",
      "Epoch 198/200\n",
      " 319/1562 [=====>........................] - ETA: 51s - loss: 0.2288 - acc: 0.92561562/1562 [==============================] - 65s 42ms/step - loss: 0.2356 - acc: 0.9219 - val_loss: 0.4445 - val_acc: 0.8577\n",
      "Epoch 199/200\n",
      " 350/1562 [=====>........................] - ETA: 48s - loss: 0.2321 - acc: 0.92291562/1562 [==============================] - 65s 42ms/step - loss: 0.2348 - acc: 0.9234 - val_loss: 0.4280 - val_acc: 0.8625\n",
      "Epoch 200/200\n",
      " 360/1562 [=====>........................] - ETA: 48s - loss: 0.2345 - acc: 0.92381562/1562 [==============================] - 66s 42ms/step - loss: 0.2388 - acc: 0.9228 - val_loss: 0.4317 - val_acc: 0.8631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb56c6c6f98>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Archivo donde se almacenan los pesos\n",
    "archivo = 'drive/ColabNotebooks/pesos-v1.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "modelo_chk = ModelCheckpoint(archivo, monitor='val_loss', verbose=0,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "# Reducir tasa de aprendizaje cuando se estanca el aprendizaje\n",
    "reducir_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "              patience=5, min_lr=0.001)\n",
    "\n",
    "# Almacenar los resultados de cada epoca a un archivo CSV\n",
    "csv_log = CSVLogger('drive/ColabNotebooks/entrenamiento-v1.log')\n",
    "\n",
    "# Entrenar la Red\n",
    "modelo.fit_generator(datagen.flow(x_entrenamiento, y_entrenamiento, batch_size=tamaño_lote),\n",
    "                    steps_per_epoch=x_entrenamiento.shape[0] // tamaño_lote,\n",
    "                    epochs=epocas,\n",
    "                    validation_data=(x_prueba, y_prueba),\n",
    "                    callbacks=[reducir_lr, modelo_chk, csv_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4342,
     "status": "ok",
     "timestamp": 1525405255745,
     "user": {
      "displayName": "Wladimir Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102207768536882894468"
     },
     "user_tz": 240
    },
    "id": "FSZ4PncOoKoY",
    "outputId": "1ded4c35-4e53-4680-ddc5-982a13efd57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 286us/step\n",
      "Perdida prueba: 14.511121047973633\n",
      "Exactitud prueba: 0.0997\n"
     ]
    }
   ],
   "source": [
    "# cargar los pesos que generaron la mejor exactitud sobre el conjunto de validación \n",
    "modelo.load_weights('pesos-v1.188-0.4200.hdf5')\n",
    "\n",
    "scores = modelo.evaluate(x_prueba, y_prueba, verbose=1)\n",
    "print('Perdida prueba:', scores[0])\n",
    "print('Exactitud prueba:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "l4Z0pJpHPPbm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "30.EjemploUsoColaboratory.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
