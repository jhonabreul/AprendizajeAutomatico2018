{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los seres humanos no comienzan su pensamiento desde cero cada segundo. Al leer este ensayo, entiendes cada palabra según tu comprensión de las palabras anteriores. No tiras todo y comienzas a pensar de nuevo. Tus pensamientos tienen persistencia.\n",
    "\n",
    "Las redes neuronales tradicionales no pueden hacer esto, y parece una deficiencia importante. Por ejemplo, imagine que desea clasificar qué tipo de evento está sucediendo en cada punto de una película. No está claro cómo una red neuronal tradicional podría usar su razonamiento sobre eventos previos en la película para informar a los posteriores.\n",
    "\n",
    "Las redes neuronales recurrentes abordan este problema. Son redes con bucles que permiten que la información persista.\n",
    "\n",
    "<img src=\"figuras/nodo_rnr.png\" width=\"15%\">\n",
    "\n",
    "En la figura de arriba, un trozo de la red neuronal RN, observa una entrada $x_t$ y genera un valor $s_t$. Un ciclo permite que la información pase de un paso de la red al siguiente.\n",
    "\n",
    "Estos bucles hacen que las redes neuronales recurrentes parezcan misteriosas. Sin embargo, si piensa un poco más, resulta que no son tan diferentes a una red neuronal normal. Una red neuronal recurrente se puede considerar como copias múltiples de la misma red, cada una pasando un mensaje a un sucesor. Considere lo que sucede si desenrollamos el ciclo:\n",
    "\n",
    "<img src=\"figuras/rnr_desenrrollada.png\" width=\"75%\">\n",
    "\n",
    "Esta naturaleza de cadena revela que las redes neuronales recurrentes están íntimamente relacionadas con secuencias y listas. Son la arquitectura natural de una red neuronal para usar para tales datos.\n",
    "\n",
    "En los últimos años, ha habido un éxito increíble aplicando RNR a una variedad de problemas: reconocimiento de voz, modelado de lenguaje, traducción, subtitulado de imágenes ...\n",
    "\n",
    "Esencial para estos éxitos es el uso de LSTM (Long Short-Time Memory), un tipo muy especial de red neuronal recurrente que funciona, para muchas tareas, mucho mejor que la versión estándar. Casi todos los mejores resultados basados en redes neuronales recurrentes se logran con ellas. Son estas LSTM las que se explorará a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El Problema de las Dependencias a Largo Plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los atractivos de las RNR es la idea de que podrían ser capaces de conectar información previa a la tarea actual, por ejemplo, el uso de los cuadros de video previos podría ayudar a comprender el cuadro actual. Si las RNR pudieran hacer esto, serían extremadamente útiles.\n",
    "\n",
    "Algunas veces, solo necesitamos ver información reciente para realizar la tarea actual. Por ejemplo, considere un modelo de lenguaje que intente predecir la siguiente palabra en función de las anteriores. Si estamos tratando de predecir la última palabra en \"las nubes están en el *cielo*\", no necesitamos ningún contexto adicional; es bastante obvio que la próxima palabra será cielo. En tales casos, cuando la brecha entre la información relevante y el lugar que se necesita es pequeña, las RNR pueden aprender a usar la información pasada.\n",
    "\n",
    "<img src=\"figuras/dependencia_corta.png\" width=\"75%\">\n",
    "\n",
    "Pero también hay casos en los que necesitamos más contexto. Considere tratar de predecir la última palabra en el texto \"Crecí en Francia ... Hablo *francés* con fluidez\". La información reciente sugiere que la siguiente palabra es probablemente el nombre de un idioma, pero si queremos reducir el idioma, necesitamos el contexto de Francia, desde atrás. Es completamente posible que exista una brecha entre la información relevante y el punto en el que se necesita que sea muy grande.\n",
    "\n",
    "Desafortunadamente, a medida que crece esa brecha, las RNN no pueden aprender a conectar la información.\n",
    "\n",
    "<img src=\"figuras/dependencia_larga.png\" width=\"75%\">\n",
    "\n",
    "En teoría, las RNR son absolutamente capaces de manejar tales \"dependencias a largo plazo\". Un humano podría elegir cuidadosamente los parámetros para que resuelvan problemas de juguete de esta forma. Tristemente, en la práctica, las RNR no parecen ser capaces de aprenderlas. El problema fue explorado en profundidad por Hochreiter (1991) [alemán] y Bengio, et al. (1994), quienes encontraron algunas razones bastante fundamentales por las cuales podría ser difícil.\n",
    "\n",
    "Este problema se puede resolver con las LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes de memoria de corto y largo plazo, generalmente llamadas \"LSTM\" del ingles (Long Short Term Memory), son un tipo especial de RNR, capaz de aprender dependencias a largo plazo. Fueron introducidos por Hochreiter y Schmidhuber (1997), y fueron refinadas y popularizadas por muchas personas en trabajos posteriores. Funcionan muy bien en una gran variedad de problemas, y ahora son ampliamente utilizadas.\n",
    "\n",
    "Las LSTM están diseñados explícitamente para evitar el problema de dependencia a largo plazo. Recordar la información durante largos períodos de tiempo es prácticamente su comportamiento predeterminado.\n",
    "\n",
    "Todas las redes neuronales recurrentes tienen la forma de una cadena de módulos repetitivos de red neuronal. En RNN estándar, este módulo de repetición tendrá una estructura muy simple, como una sola capa de tanh.\n",
    "\n",
    "<img src=\"figuras/estandar_rnr.png\" width=\"75%\">\n",
    "\n",
    "Los LSTM también tienen esta estructura tipo cadena, pero el módulo de repetición tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, hay cuatro, interactuando de una manera muy especial.\n",
    "\n",
    "<img src=\"figuras/lstm_rnr.png\" width=\"100%\">\n",
    "\n",
    "Notación:\n",
    "\n",
    "<img src=\"figuras/notacion_lstm.png\" width=\"75%\">\n",
    "\n",
    "En la figura de arriba, cada línea lleva un vector completo, desde la salida de un nodo hasta las entradas de otros. Los círculos verdes representan operaciones puntuales, como la adición de vectores, mientras que los cuadros amarillos son capas entrenadas de redes neuronales. Las líneas que se unen denotan concatenación, mientras que una línea de bifurcación denota que su contenido se copia y las copias van a diferentes ubicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La idea central detrás de los LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave para las LSTM es el estado de la celda, la línea horizontal que se extiende por la parte superior de la figura.\n",
    "\n",
    "El estado de la celda es como una cinta transportadora. Corre directamente por toda la cadena, con solo algunas interacciones lineales menores. Es muy fácil que la información fluya sin cambios\n",
    "\n",
    "<img src=\"figuras/estado_celda.png\" width=\"50%\">\n",
    "\n",
    "La LSTM tiene la capacidad de eliminar o agregar información al estado de la celda, cuidadosamente regulado por estructuras llamadas compuertas.\n",
    "\n",
    "Las compuertas son una forma de dejar pasar la información. Se componen de una capa de red neuronal con activación sigmoide y una operación de multiplicación puntual.\n",
    "\n",
    "<img src=\"figuras/compuerta_lstm.png\" width=\"15%\">\n",
    "\n",
    "La salida de capa con activación sigmoide son números entre cero y uno, que describe la cantidad que se debe dejar pasar cada componente. Un valor de cero significa \"no dejar pasar nada\", mientras que un valor de uno significa \"dejar pasar todo\"\n",
    "\n",
    "Una LSTM tiene tres de estas compuertas, para proteger y controlar el estado de la celda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorrido paso a paso de una LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso en nuestra LSTM es decidir qué información vamos a olvidar/recordar del estado de la celda. Esta decisión es tomada por una capa con activación sigmoide llamada \"capa de compuerta de olvido\". Basado en los valores de $s_{t-1}$ y $x_t$, genera un número entre 0 y 1 para cada número del estado de la celda $C_{t-1}$. Un 1 representa \"mantener esto por completo\", mientras que un 0 representa \"deshacerse por completo de esto\".\n",
    "\n",
    "Regresemos a nuestro ejemplo de un modelo de lenguaje que intenta predecir la siguiente palabra en base a todas las anteriores. En tal problema, el estado de la celda puede incluir el género del sujeto presente, de modo que puedan usarse los pronombres correctos. Cuando vemos un sujeto nuevo, queremos olvidar el género del sujeto anterior.\n",
    "\n",
    "<img src=\"figuras/compuerta_olvidar.png\" width=\"75%\">\n",
    "\n",
    "El siguiente paso es decidir qué nueva información vamos a almacenar en el estado de la celda. Esto tiene dos partes. Primero, una capa sigmoide llamada \"compuerta capa de entrada\" decide qué valores actualizaremos. A continuación, una capa $tanh$ crea un vector de nuevos valores candidatos, $\\tilde{C}_t$, que podrían agregarse al estado. En el siguiente paso, combinaremos estos dos para crear la actualización del estado.\n",
    "\n",
    "En el ejemplo de nuestro modelo de lenguaje, nos gustaría agregar el género del nuevo sujeto al estado de la celda, para reemplazar el anterior que estamos olvidando.\n",
    "\n",
    "<img src=\"figuras/compuerta_actualizar.png\" width=\"75%\">\n",
    "\n",
    "Ahora es el momento de actualizar el estado de celda anterior, $C_{t-1}$, en el nuevo estado de celda $C_t$. Los pasos anteriores ya decidieron qué hacer, solo tenemos que hacerlo.\n",
    "\n",
    "Multiplicamos el estado anterior por $f_t$, olvidando las cosas que decidimos olvidar antes. Luego le sumamos $i_t * \\tilde{C}_t$. Estos son los nuevos valores candidatos, ajustados por cuánto decidimos actualizar cada valor de estado.\n",
    "\n",
    "En el caso del modelo de lenguaje, aquí es donde dejamos caer la información sobre el género del sujeto anterior y agregamos la nueva información, como decidimos en los pasos anteriores.\n",
    "\n",
    "<img src=\"figuras/actualizar_estado.png\" width=\"75%\">\n",
    "\n",
    "Finalmente, tenemos que decidir qué vamos a generar. Este resultado se basará en el estado de celda, pero será una versión filtrada. Primero, ejecutamos una capa con activación sigmoide que decide qué partes del estado de la celda vamos a generar. Luego, ponemos el estado de la celda a través de una función `tanh` (para ajustar los valores entre -1 y 1) y posteriormente se multiplican por la salida de la compuerta sigmoide, de modo que solo se genererarán las partes que decidamos.\n",
    "\n",
    "Para el ejemplo del modelo de lenguaje, dado que acaba de ver un tema, es posible que desee generar información relevante para un verbo, en caso de que eso sea lo que viene a continuación. Por ejemplo, podría mostrar si el sujeto es singular o plural, de modo que sabemos en qué forma debe conjugarse un verbo si eso es lo que sigue a continuación.\n",
    "\n",
    "<img src=\"figuras/salida_celda.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de una Red Neuronal Recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo adaptado del siguiente artículo en medium: https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construiremos un RNR sencilla que recuerde los datos de entrada y luego los repita después de unos pocos pasos de tiempo. Primero establezcamos algunas constantes que necesitaremos, lo que quieren decir se aclarará en un momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epocas = 100\n",
    "longitud_total_series = 50000\n",
    "longitud_truncada_backprop = 15\n",
    "tamaño_estado = 4\n",
    "num_clases = 2\n",
    "paso_eco = 3\n",
    "tamaño_lote = 5\n",
    "num_lotes = longitud_total_series//tamaño_lote//longitud_truncada_backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar los datos de entrenamiento, la entrada es básicamente un vector binario aleatorio. La salida será el \"eco\" de la entrada, pasos de `pasos_eco` desplazados a la derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, longitud_total_series, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, paso_eco)\n",
    "    y[0:paso_eco] = 0\n",
    "\n",
    "    x = x.reshape((tamaño_lote, -1))  # El primer indice es el que cambia más lento, las subseries como filas\n",
    "    y = y.reshape((tamaño_lote, -1))\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe el cambio de la forma de los datos en una matriz con filas `tamaño_lote`. Las redes neuronales se entrenan al aproximar el gradiente de la función de pérdida con respecto a los pesos neuronales, al observar solo un pequeño subconjunto de los datos, también conocido como *mini-lote*. La remodelación toma todo el conjunto de datos y lo pone en una matriz, que más tarde se dividirá en estos mini lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir el grafo computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow funciona construyendo primero un gráfico computacional, que especifica qué operaciones se realizarán. La entrada y la salida de este gráfico son típicamente matrices multidimensionales, también conocidas como tensores. El gráfico, o partes de él, se pueden ejecutar iterativamente en una sesión, esto se puede hacer en la CPU, GPU o incluso en un servidor remoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables y Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos estructuras de datos básicas de TensorFlow que se usarán en este ejemplo son los *placeholders* y las variables. En cada ejecución, los datos del lote se alimentan a los *placeholders*, que son *nodos de inicio* del gráfico computacional. Además, el estado RNR se suministra en un *placeholders*, que se guarda de la salida de la ejecución anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loteX_placeholder = tf.placeholder(tf.float32, [tamaño_lote, longitud_truncada_backprop])\n",
    "loteY_placeholder = tf.placeholder(tf.int32, [tamaño_lote, longitud_truncada_backprop])\n",
    "\n",
    "iniciar_estado = tf.placeholder(tf.float32, [tamaño_lote, tamaño_estado])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pesos y los sesgos de la red se declaran como variables de TensorFlow, lo que los hace persistentes en todas las ejecuciones y les permite actualizarse incrementalmente para cada lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(np.random.rand(tamaño_estado+1, tamaño_estado), dtype=tf.float32)\n",
    "b1 = tf.Variable(np.zeros((1,tamaño_estado)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(tamaño_estado, num_clases),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_clases)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figura siguiente muestra la matriz de datos de entrada, y el lote actual `loteX_placeholder` está en el rectángulo punteado. Como veremos más adelante, esta \"ventana lote\" se desliza `longitud_truncada_backprop` pasos a la derecha en cada ejecución, de ahí la flecha. En nuestro ejemplo a continuación `tamaño_lote = 3`, `longitud_truncada_backprop = 3` y `longitud_total_series = 36`. Tenga en cuenta que estos números son solo para fines de visualización, los valores son diferentes en el código. El índice de orden de serie se muestra como números en los puntos de datos.\n",
    "\n",
    "<img src=\"figuras/puntos_datos_entrenamiento.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempacando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es el momento de construir la parte del gráfico que se asemeja al cálculo real de la RNR, primero queremos dividir los datos del lote en pasos de tiempo adyacentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempacar columnas\n",
    "series_entradas = tf.unstack(loteX_placeholder, axis=1)\n",
    "series_etiquetas = tf.unstack(loteY_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver en la siguiente figura, se realiza desempacando las columnas (eje = 1) del lote en una lista de Python. El RNR entrenará simultáneamente en diferentes partes de la serie temporal; los pasos 4 a 6, 16 a 18 y 28 a 30 en el ejemplo de lote actual. La razón para usar los nombres de variables en \"plural\" tal como \"series\" es enfatizar que la variable es una lista que representa una serie temporal con múltiples entradas en cada paso.\n",
    "\n",
    "<img src=\"figuras/desempacado.png\" width=\"50%\">\n",
    "\n",
    "El hecho de que el entrenamiento se realice simultáneamente en tres lugares en nuestras series temporales, nos exige guardar tres instancias de estados cuando se propague hacia adelante. Eso ya se ha tomado en cuenta, ya que se ve que el *placeholder* `iniciar_estado` tiene filas `tamaño_lote`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagación Hacia Adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, construyamos la parte del grafo que hace el cálculo actual de la RNR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagación hacia adelante\n",
    "estado_actual = iniciar_estado\n",
    "series_estados = []\n",
    "for entrada_actual in series_entradas:\n",
    "    entrada_actual = tf.reshape(entrada_actual, [tamaño_lote, 1])\n",
    "    entrada_y_estado_concatenado = tf.concat([entrada_actual, estado_actual], 1)  # Incrementar número de columnas\n",
    "    proximo_estado = tf.tanh(tf.matmul(entrada_y_estado_concatenado, W1) + b1)  # Suma expandida\n",
    "    series_estados.append(proximo_estado)\n",
    "    estado_actual = proximo_estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe la concatenación en la línea 6, lo que realmente queremos hacer es calcular la suma de dos transformaciones afines entrada_actual * $W_a$ + estado_actual * $W_b$ en la figura siguiente. Al concatenar esos dos tensores, solo se usará una multiplicación de matrices. La adición del sesgo b se expande \"broadcast\" a todas las muestras del lote.\n",
    "\n",
    "<img src=\"figuras/concatenar.png\" width=\"75%\">\n",
    "\n",
    "¿Que significa el nombre de la variable longitud_truncada_backprop?. Cuando se entrena un RNR, en realidad se trata como una red neuronal profunda con pesos recurrentes en cada capa. Estas capas no se desenrollarán hasta el comienzo del tiempo, eso sería demasiado costoso desde el punto de vista informático y, por lo tanto, se truncarán en un número limitado de pasos de tiempo. En nuestro esquema de ejemplo anterior, el error se propaga hacia atrás por tres pasos para el lote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular la perdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la parte final del grafo, una capa de softmax completamente conectada desde el estado al resultado que hará que las clases sean codificadas _one-hot_, y luego calculando la pérdida del lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_series = [tf.matmul(estado, W2) + b2 for estado in series_estados] # Suma expandida \"Broadcasted\"\n",
    "predicciones_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "perdidas = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=etiquetas) for logits, etiquetas in zip(logits_series, series_etiquetas)]\n",
    "perdida_total = tf.reduce_mean(perdidas)\n",
    "\n",
    "paso_entrenamiento = tf.train.AdagradOptimizer(0.3).minimize(perdida_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe la llamada API a `sparse_softmax_cross_entropy_with_logits`, automáticamente calcula el softmax internamente y luego calcula la entropía cruzada. En nuestro ejemplo, las clases son mutuamente excluyentes (son cero o uno), que es la razón para usar el \"Sparse-softmax\", puede leer más sobre él en la API. El uso es tener `logits` de la forma `[tamaño_lote, num_clases]` y etiquetas de la forma `[tamaño_lote]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización del Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir una función de visualización para que podamos ver qué está sucediendo en la red mientras entrenamos. Trazará la pérdida a lo largo del tiempo, mostrará la entrada de entrenamiento, la salida de entrenamiento y las predicciones actuales de la red sobre diferentes series de ejemplos en un lote de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(lista_perdidas, predicciones_series, loteX, loteY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(lista_perdidas)\n",
    "\n",
    "    for lote_series_idx in range(5):\n",
    "        salida_one_hot_series = np.array(predicciones_series)[:, lote_series_idx, :]\n",
    "        salida_sencilla_series = np.array([(1 if salida[0] < 0.5 else 0) for salida in salida_one_hot_series])\n",
    "\n",
    "        plt.subplot(2, 3, lote_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, longitud_truncada_backprop, 0, 2])\n",
    "        left_offset = range(longitud_truncada_backprop)\n",
    "        plt.bar(left_offset, loteX[lote_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, loteY[lote_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, salida_sencilla_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar la Sesión de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es hora de concluir y entrenar la red, en TensorFlow el grafo se ejecuta en una sesión. Se generan nuevos datos en cada época (no es la forma habitual de hacerlo, pero funciona en este caso, ya que todo es predecible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva data, epoca 0\n",
      "Paso 0 Perdida 0.88473946\n",
      "Paso 100 Perdida 0.6960295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wladimir/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 200 Perdida 0.6931229\n",
      "Paso 300 Perdida 0.70290196\n",
      "Paso 400 Perdida 0.69108075\n",
      "Paso 500 Perdida 0.7008171\n",
      "Paso 600 Perdida 0.6925838\n",
      "Nueva data, epoca 1\n",
      "Paso 0 Perdida 0.6947787\n",
      "Paso 100 Perdida 0.7021264\n",
      "Paso 200 Perdida 0.69244677\n",
      "Paso 300 Perdida 0.7022199\n",
      "Paso 400 Perdida 0.6932236\n",
      "Paso 500 Perdida 0.69719785\n",
      "Paso 600 Perdida 0.6916017\n",
      "Nueva data, epoca 2\n",
      "Paso 0 Perdida 0.68926674\n",
      "Paso 100 Perdida 0.6926627\n",
      "Paso 200 Perdida 0.73770094\n",
      "Paso 300 Perdida 0.69454044\n",
      "Paso 400 Perdida 0.6960679\n",
      "Paso 500 Perdida 0.6976107\n",
      "Paso 600 Perdida 0.69326895\n",
      "Nueva data, epoca 3\n",
      "Paso 0 Perdida 0.69235027\n",
      "Paso 100 Perdida 0.69328535\n",
      "Paso 200 Perdida 0.69124395\n",
      "Paso 300 Perdida 0.693568\n",
      "Paso 400 Perdida 0.6935466\n",
      "Paso 500 Perdida 0.69307035\n",
      "Paso 600 Perdida 0.6991796\n",
      "Nueva data, epoca 4\n",
      "Paso 0 Perdida 0.7045314\n",
      "Paso 100 Perdida 0.6934795\n",
      "Paso 200 Perdida 0.6934856\n",
      "Paso 300 Perdida 0.693981\n",
      "Paso 400 Perdida 0.6967002\n",
      "Paso 500 Perdida 0.69299734\n",
      "Paso 600 Perdida 0.69358927\n",
      "Nueva data, epoca 5\n",
      "Paso 0 Perdida 0.7010321\n",
      "Paso 100 Perdida 0.6911572\n",
      "Paso 200 Perdida 0.6941172\n",
      "Paso 300 Perdida 0.6910371\n",
      "Paso 400 Perdida 0.6901536\n",
      "Paso 500 Perdida 0.6942947\n",
      "Paso 600 Perdida 0.6900411\n",
      "Nueva data, epoca 6\n",
      "Paso 0 Perdida 0.6948158\n",
      "Paso 100 Perdida 0.69834536\n",
      "Paso 200 Perdida 0.69662404\n",
      "Paso 300 Perdida 0.6929953\n",
      "Paso 400 Perdida 0.6875537\n",
      "Paso 500 Perdida 0.69679886\n",
      "Paso 600 Perdida 0.69256264\n",
      "Nueva data, epoca 7\n",
      "Paso 0 Perdida 0.699628\n",
      "Paso 100 Perdida 0.6910838\n",
      "Paso 200 Perdida 0.69473654\n",
      "Paso 300 Perdida 0.69516903\n",
      "Paso 400 Perdida 0.7006636\n",
      "Paso 500 Perdida 0.6963305\n",
      "Paso 600 Perdida 0.69241977\n",
      "Nueva data, epoca 8\n",
      "Paso 0 Perdida 0.6989978\n",
      "Paso 100 Perdida 0.69233173\n",
      "Paso 200 Perdida 0.693618\n",
      "Paso 300 Perdida 0.69292927\n",
      "Paso 400 Perdida 0.70474803\n",
      "Paso 500 Perdida 0.6978252\n",
      "Paso 600 Perdida 0.70676875\n",
      "Nueva data, epoca 9\n",
      "Paso 0 Perdida 0.69633013\n",
      "Paso 100 Perdida 0.6931075\n",
      "Paso 200 Perdida 0.69502807\n",
      "Paso 300 Perdida 0.6960625\n",
      "Paso 400 Perdida 0.70560175\n",
      "Paso 500 Perdida 0.6881325\n",
      "Paso 600 Perdida 0.69248676\n",
      "Nueva data, epoca 10\n",
      "Paso 0 Perdida 0.69741625\n",
      "Paso 100 Perdida 0.69407564\n",
      "Paso 200 Perdida 0.69274634\n",
      "Paso 300 Perdida 0.69132864\n",
      "Paso 400 Perdida 0.68545264\n",
      "Paso 500 Perdida 0.6873914\n",
      "Paso 600 Perdida 0.6940321\n",
      "Nueva data, epoca 11\n",
      "Paso 0 Perdida 0.6959443\n",
      "Paso 100 Perdida 0.69320196\n",
      "Paso 200 Perdida 0.69326335\n",
      "Paso 300 Perdida 0.6949402\n",
      "Paso 400 Perdida 0.6930775\n",
      "Paso 500 Perdida 0.69363654\n",
      "Paso 600 Perdida 0.69294924\n",
      "Nueva data, epoca 12\n",
      "Paso 0 Perdida 0.70343125\n",
      "Paso 100 Perdida 0.6937505\n",
      "Paso 200 Perdida 0.6940062\n",
      "Paso 300 Perdida 0.69348437\n",
      "Paso 400 Perdida 0.6940815\n",
      "Paso 500 Perdida 0.68686557\n",
      "Paso 600 Perdida 0.68762726\n",
      "Nueva data, epoca 13\n",
      "Paso 0 Perdida 0.6983263\n",
      "Paso 100 Perdida 0.69340056\n",
      "Paso 200 Perdida 0.6944935\n",
      "Paso 300 Perdida 0.69276446\n",
      "Paso 400 Perdida 0.6912928\n",
      "Paso 500 Perdida 0.68884873\n",
      "Paso 600 Perdida 0.69303197\n",
      "Nueva data, epoca 14\n",
      "Paso 0 Perdida 0.69800967\n",
      "Paso 100 Perdida 0.68957263\n",
      "Paso 200 Perdida 0.6974337\n",
      "Paso 300 Perdida 0.69180226\n",
      "Paso 400 Perdida 0.6960914\n",
      "Paso 500 Perdida 0.6891392\n",
      "Paso 600 Perdida 0.69325346\n",
      "Nueva data, epoca 15\n",
      "Paso 0 Perdida 0.69421965\n",
      "Paso 100 Perdida 0.6914512\n",
      "Paso 200 Perdida 0.68990827\n",
      "Paso 300 Perdida 0.69580823\n",
      "Paso 400 Perdida 0.6946864\n",
      "Paso 500 Perdida 0.6924893\n",
      "Paso 600 Perdida 0.6933227\n",
      "Nueva data, epoca 16\n",
      "Paso 0 Perdida 0.6945121\n",
      "Paso 100 Perdida 0.69470596\n",
      "Paso 200 Perdida 0.69144076\n",
      "Paso 300 Perdida 0.6943129\n",
      "Paso 400 Perdida 0.6924955\n",
      "Paso 500 Perdida 0.6886029\n",
      "Paso 600 Perdida 0.6909814\n",
      "Nueva data, epoca 17\n",
      "Paso 0 Perdida 0.69647044\n",
      "Paso 100 Perdida 0.69720894\n",
      "Paso 200 Perdida 0.6958926\n",
      "Paso 300 Perdida 0.6905531\n",
      "Paso 400 Perdida 0.69163996\n",
      "Paso 500 Perdida 0.6929081\n",
      "Paso 600 Perdida 0.69282794\n",
      "Nueva data, epoca 18\n",
      "Paso 0 Perdida 0.6953956\n",
      "Paso 100 Perdida 0.69034755\n",
      "Paso 200 Perdida 0.69648725\n",
      "Paso 300 Perdida 0.6936271\n",
      "Paso 400 Perdida 0.6936793\n",
      "Paso 500 Perdida 0.69263077\n",
      "Paso 600 Perdida 0.6929827\n",
      "Nueva data, epoca 19\n",
      "Paso 0 Perdida 0.6919596\n",
      "Paso 100 Perdida 0.6896456\n",
      "Paso 200 Perdida 0.69798017\n",
      "Paso 300 Perdida 0.6943712\n",
      "Paso 400 Perdida 0.69106305\n",
      "Paso 500 Perdida 0.6952058\n",
      "Paso 600 Perdida 0.69309604\n",
      "Nueva data, epoca 20\n",
      "Paso 0 Perdida 0.69205225\n",
      "Paso 100 Perdida 0.6949126\n",
      "Paso 200 Perdida 0.69723624\n",
      "Paso 300 Perdida 0.6940111\n",
      "Paso 400 Perdida 0.6946045\n",
      "Paso 500 Perdida 0.6931052\n",
      "Paso 600 Perdida 0.6951741\n",
      "Nueva data, epoca 21\n",
      "Paso 0 Perdida 0.6883813\n",
      "Paso 100 Perdida 0.7050086\n",
      "Paso 200 Perdida 0.6925019\n",
      "Paso 300 Perdida 0.6947444\n",
      "Paso 400 Perdida 0.6933089\n",
      "Paso 500 Perdida 0.6909489\n",
      "Paso 600 Perdida 0.6930124\n",
      "Nueva data, epoca 22\n",
      "Paso 0 Perdida 0.6922481\n",
      "Paso 100 Perdida 0.6935714\n",
      "Paso 200 Perdida 0.69242275\n",
      "Paso 300 Perdida 0.69503015\n",
      "Paso 400 Perdida 0.69290185\n",
      "Paso 500 Perdida 0.696048\n",
      "Paso 600 Perdida 0.69046664\n",
      "Nueva data, epoca 23\n",
      "Paso 0 Perdida 0.6921378\n",
      "Paso 100 Perdida 0.6928186\n",
      "Paso 200 Perdida 0.6930952\n",
      "Paso 300 Perdida 0.6869595\n",
      "Paso 400 Perdida 0.6930027\n",
      "Paso 500 Perdida 0.694087\n",
      "Paso 600 Perdida 0.69314003\n",
      "Nueva data, epoca 24\n",
      "Paso 0 Perdida 0.6908057\n",
      "Paso 100 Perdida 0.6865639\n",
      "Paso 200 Perdida 0.69215655\n",
      "Paso 300 Perdida 0.6901108\n",
      "Paso 400 Perdida 0.69016725\n",
      "Paso 500 Perdida 0.6933376\n",
      "Paso 600 Perdida 0.69775856\n",
      "Nueva data, epoca 25\n",
      "Paso 0 Perdida 0.6945585\n",
      "Paso 100 Perdida 0.69038624\n",
      "Paso 200 Perdida 0.6917664\n",
      "Paso 300 Perdida 0.6931121\n",
      "Paso 400 Perdida 0.6957701\n",
      "Paso 500 Perdida 0.68984103\n",
      "Paso 600 Perdida 0.69131374\n",
      "Nueva data, epoca 26\n",
      "Paso 0 Perdida 0.68896365\n",
      "Paso 100 Perdida 0.701124\n",
      "Paso 200 Perdida 0.6928155\n",
      "Paso 300 Perdida 0.69357973\n",
      "Paso 400 Perdida 0.6935703\n",
      "Paso 500 Perdida 0.69390684\n",
      "Paso 600 Perdida 0.69389933\n",
      "Nueva data, epoca 27\n",
      "Paso 0 Perdida 0.6910808\n",
      "Paso 100 Perdida 0.7035626\n",
      "Paso 200 Perdida 0.6930476\n",
      "Paso 300 Perdida 0.69294316\n",
      "Paso 400 Perdida 0.6932063\n",
      "Paso 500 Perdida 0.69323796\n",
      "Paso 600 Perdida 0.6931569\n",
      "Nueva data, epoca 28\n",
      "Paso 0 Perdida 0.6919291\n",
      "Paso 100 Perdida 0.6930128\n",
      "Paso 200 Perdida 0.6926643\n",
      "Paso 300 Perdida 0.690089\n",
      "Paso 400 Perdida 0.69009614\n",
      "Paso 500 Perdida 0.68753934\n",
      "Paso 600 Perdida 0.69205767\n",
      "Nueva data, epoca 29\n",
      "Paso 0 Perdida 0.690741\n",
      "Paso 100 Perdida 0.6939068\n",
      "Paso 200 Perdida 0.6957297\n",
      "Paso 300 Perdida 0.6919974\n",
      "Paso 400 Perdida 0.69278085\n",
      "Paso 500 Perdida 0.70262975\n",
      "Paso 600 Perdida 0.6927849\n",
      "Nueva data, epoca 30\n",
      "Paso 0 Perdida 0.69177496\n",
      "Paso 100 Perdida 0.69395185\n",
      "Paso 200 Perdida 0.6929361\n",
      "Paso 300 Perdida 0.6950661\n",
      "Paso 400 Perdida 0.69169515\n",
      "Paso 500 Perdida 0.6948542\n",
      "Paso 600 Perdida 0.6980976\n",
      "Nueva data, epoca 31\n",
      "Paso 0 Perdida 0.69412214\n",
      "Paso 100 Perdida 0.69294614\n",
      "Paso 200 Perdida 0.6992581\n",
      "Paso 300 Perdida 0.70223546\n",
      "Paso 400 Perdida 0.6967768\n",
      "Paso 500 Perdida 0.69277406\n",
      "Paso 600 Perdida 0.6931331\n",
      "Nueva data, epoca 32\n",
      "Paso 0 Perdida 0.692821\n",
      "Paso 100 Perdida 0.6940515\n",
      "Paso 200 Perdida 0.69348264\n",
      "Paso 300 Perdida 0.69202685\n",
      "Paso 400 Perdida 0.6930936\n",
      "Paso 500 Perdida 0.70464236\n",
      "Paso 600 Perdida 0.6861234\n",
      "Nueva data, epoca 33\n",
      "Paso 0 Perdida 0.69031936\n",
      "Paso 100 Perdida 0.69189775\n",
      "Paso 200 Perdida 0.69416565\n",
      "Paso 300 Perdida 0.6924787\n",
      "Paso 400 Perdida 0.6930903\n",
      "Paso 500 Perdida 0.6967405\n",
      "Paso 600 Perdida 0.6915631\n",
      "Nueva data, epoca 34\n",
      "Paso 0 Perdida 0.6940744\n",
      "Paso 100 Perdida 0.69158363\n",
      "Paso 200 Perdida 0.69338393\n",
      "Paso 300 Perdida 0.69433266\n",
      "Paso 400 Perdida 0.6811152\n",
      "Paso 500 Perdida 0.6935167\n",
      "Paso 600 Perdida 0.696831\n",
      "Nueva data, epoca 35\n",
      "Paso 0 Perdida 0.6966026\n",
      "Paso 100 Perdida 0.6912552\n",
      "Paso 200 Perdida 0.6922841\n",
      "Paso 300 Perdida 0.6928213\n",
      "Paso 400 Perdida 0.7025585\n",
      "Paso 500 Perdida 0.6933313\n",
      "Paso 600 Perdida 0.60481286\n",
      "Nueva data, epoca 36\n",
      "Paso 0 Perdida 0.45986083\n",
      "Paso 100 Perdida 0.10567653\n",
      "Paso 200 Perdida 0.06085762\n",
      "Paso 300 Perdida 0.03883303\n",
      "Paso 400 Perdida 0.028259737\n",
      "Paso 500 Perdida 0.034041557\n",
      "Paso 600 Perdida 0.027045056\n",
      "Nueva data, epoca 37\n",
      "Paso 0 Perdida 0.17024563\n",
      "Paso 100 Perdida 0.017882623\n",
      "Paso 200 Perdida 0.015745683\n",
      "Paso 300 Perdida 0.01305273\n",
      "Paso 400 Perdida 0.012805294\n",
      "Paso 500 Perdida 0.010130071\n",
      "Paso 600 Perdida 0.010849557\n",
      "Nueva data, epoca 38\n",
      "Paso 0 Perdida 0.18976115\n",
      "Paso 100 Perdida 0.009237579\n",
      "Paso 200 Perdida 0.008305625\n",
      "Paso 300 Perdida 0.0077673355\n",
      "Paso 400 Perdida 0.007579446\n",
      "Paso 500 Perdida 0.007338152\n",
      "Paso 600 Perdida 0.005714302\n",
      "Nueva data, epoca 39\n",
      "Paso 0 Perdida 0.46502677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 100 Perdida 0.0068217968\n",
      "Paso 200 Perdida 0.006232371\n",
      "Paso 300 Perdida 0.00631782\n",
      "Paso 400 Perdida 0.0057727112\n",
      "Paso 500 Perdida 0.0050912164\n",
      "Paso 600 Perdida 0.0046302285\n",
      "Nueva data, epoca 40\n",
      "Paso 0 Perdida 0.39851993\n",
      "Paso 100 Perdida 0.004748943\n",
      "Paso 200 Perdida 0.0048182933\n",
      "Paso 300 Perdida 0.00421176\n",
      "Paso 400 Perdida 0.0040582637\n",
      "Paso 500 Perdida 0.0043878634\n",
      "Paso 600 Perdida 0.0033850444\n",
      "Nueva data, epoca 41\n",
      "Paso 0 Perdida 0.1494795\n",
      "Paso 100 Perdida 0.004312147\n",
      "Paso 200 Perdida 0.0037264437\n",
      "Paso 300 Perdida 0.0033368755\n",
      "Paso 400 Perdida 0.0031226906\n",
      "Paso 500 Perdida 0.0036768257\n",
      "Paso 600 Perdida 0.0030562342\n",
      "Nueva data, epoca 42\n",
      "Paso 0 Perdida 0.2716089\n",
      "Paso 100 Perdida 0.00365569\n",
      "Paso 200 Perdida 0.0029176795\n",
      "Paso 300 Perdida 0.0029164318\n",
      "Paso 400 Perdida 0.0028526753\n",
      "Paso 500 Perdida 0.0029669427\n",
      "Paso 600 Perdida 0.0027289384\n",
      "Nueva data, epoca 43\n",
      "Paso 0 Perdida 0.10011464\n",
      "Paso 100 Perdida 0.0024367305\n",
      "Paso 200 Perdida 0.003460149\n",
      "Paso 300 Perdida 0.003198527\n",
      "Paso 400 Perdida 0.0026949514\n",
      "Paso 500 Perdida 0.002409308\n",
      "Paso 600 Perdida 0.0023253663\n",
      "Nueva data, epoca 44\n",
      "Paso 0 Perdida 0.15637466\n",
      "Paso 100 Perdida 0.0022727787\n",
      "Paso 200 Perdida 0.0021017648\n",
      "Paso 300 Perdida 0.0019395711\n",
      "Paso 400 Perdida 0.0020662777\n",
      "Paso 500 Perdida 0.0020987208\n",
      "Paso 600 Perdida 0.0020099601\n",
      "Nueva data, epoca 45\n",
      "Paso 0 Perdida 0.3207378\n",
      "Paso 100 Perdida 0.002118513\n",
      "Paso 200 Perdida 0.00210017\n",
      "Paso 300 Perdida 0.0019411234\n",
      "Paso 400 Perdida 0.001981557\n",
      "Paso 500 Perdida 0.0018838326\n",
      "Paso 600 Perdida 0.0017439636\n",
      "Nueva data, epoca 46\n",
      "Paso 0 Perdida 0.38801807\n",
      "Paso 100 Perdida 0.0016899153\n",
      "Paso 200 Perdida 0.0018510481\n",
      "Paso 300 Perdida 0.0017381124\n",
      "Paso 400 Perdida 0.001958872\n",
      "Paso 500 Perdida 0.0015736498\n",
      "Paso 600 Perdida 0.001768588\n",
      "Nueva data, epoca 47\n",
      "Paso 0 Perdida 0.23641357\n",
      "Paso 100 Perdida 0.0013983381\n",
      "Paso 200 Perdida 0.0013012725\n",
      "Paso 300 Perdida 0.001485347\n",
      "Paso 400 Perdida 0.0014239032\n",
      "Paso 500 Perdida 0.001496293\n",
      "Paso 600 Perdida 0.0014630526\n",
      "Nueva data, epoca 48\n",
      "Paso 0 Perdida 0.21145856\n",
      "Paso 100 Perdida 0.0014780009\n",
      "Paso 200 Perdida 0.0017968678\n",
      "Paso 300 Perdida 0.0014300687\n",
      "Paso 400 Perdida 0.0014415932\n",
      "Paso 500 Perdida 0.0016638285\n",
      "Paso 600 Perdida 0.0013206907\n",
      "Nueva data, epoca 49\n",
      "Paso 0 Perdida 0.33547476\n",
      "Paso 100 Perdida 0.001494694\n",
      "Paso 200 Perdida 0.0012029991\n",
      "Paso 300 Perdida 0.0015315769\n",
      "Paso 400 Perdida 0.0012097829\n",
      "Paso 500 Perdida 0.0012597464\n",
      "Paso 600 Perdida 0.0011413965\n",
      "Nueva data, epoca 50\n",
      "Paso 0 Perdida 0.24938041\n",
      "Paso 100 Perdida 0.0015199961\n",
      "Paso 200 Perdida 0.0014937841\n",
      "Paso 300 Perdida 0.0012041492\n",
      "Paso 400 Perdida 0.0013671865\n",
      "Paso 500 Perdida 0.0013026609\n",
      "Paso 600 Perdida 0.0013153964\n",
      "Nueva data, epoca 51\n",
      "Paso 0 Perdida 0.14631505\n",
      "Paso 100 Perdida 0.0011780001\n",
      "Paso 200 Perdida 0.0014338632\n",
      "Paso 300 Perdida 0.0012294059\n",
      "Paso 400 Perdida 0.0014699847\n",
      "Paso 500 Perdida 0.0011507046\n",
      "Paso 600 Perdida 0.0009918265\n",
      "Nueva data, epoca 52\n",
      "Paso 0 Perdida 0.18584724\n",
      "Paso 100 Perdida 0.0010974198\n",
      "Paso 200 Perdida 0.0011287273\n",
      "Paso 300 Perdida 0.0011867392\n",
      "Paso 400 Perdida 0.0010603267\n",
      "Paso 500 Perdida 0.0009897943\n",
      "Paso 600 Perdida 0.0010796827\n",
      "Nueva data, epoca 53\n",
      "Paso 0 Perdida 0.19504279\n",
      "Paso 100 Perdida 0.0009498196\n",
      "Paso 200 Perdida 0.000991029\n",
      "Paso 300 Perdida 0.0011100444\n",
      "Paso 400 Perdida 0.000943124\n",
      "Paso 500 Perdida 0.0011010339\n",
      "Paso 600 Perdida 0.0010043769\n",
      "Nueva data, epoca 54\n",
      "Paso 0 Perdida 0.25024903\n",
      "Paso 100 Perdida 0.0010975978\n",
      "Paso 200 Perdida 0.0021458704\n",
      "Paso 300 Perdida 0.0012707856\n",
      "Paso 400 Perdida 0.0009860387\n",
      "Paso 500 Perdida 0.0009986988\n",
      "Paso 600 Perdida 0.0009088579\n",
      "Nueva data, epoca 55\n",
      "Paso 0 Perdida 0.20500396\n",
      "Paso 100 Perdida 0.0009082675\n",
      "Paso 200 Perdida 0.0010418297\n",
      "Paso 300 Perdida 0.0009904254\n",
      "Paso 400 Perdida 0.0010185103\n",
      "Paso 500 Perdida 0.0007422961\n",
      "Paso 600 Perdida 0.0009086912\n",
      "Nueva data, epoca 56\n",
      "Paso 0 Perdida 0.3632387\n",
      "Paso 100 Perdida 0.0010323393\n",
      "Paso 200 Perdida 0.00087762804\n",
      "Paso 300 Perdida 0.0009983268\n",
      "Paso 400 Perdida 0.00089377724\n",
      "Paso 500 Perdida 0.0013574321\n",
      "Paso 600 Perdida 0.00078636495\n",
      "Nueva data, epoca 57\n",
      "Paso 0 Perdida 0.31528127\n",
      "Paso 100 Perdida 0.0011160106\n",
      "Paso 200 Perdida 0.00085615186\n",
      "Paso 300 Perdida 0.0008306095\n",
      "Paso 400 Perdida 0.0008071747\n",
      "Paso 500 Perdida 0.000747266\n",
      "Paso 600 Perdida 0.0007413757\n",
      "Nueva data, epoca 58\n",
      "Paso 0 Perdida 0.111184895\n",
      "Paso 100 Perdida 0.0010543453\n",
      "Paso 200 Perdida 0.000849942\n",
      "Paso 300 Perdida 0.00074141927\n",
      "Paso 400 Perdida 0.0010244866\n",
      "Paso 500 Perdida 0.0007156559\n",
      "Paso 600 Perdida 0.0009040844\n",
      "Nueva data, epoca 59\n",
      "Paso 0 Perdida 0.3974043\n",
      "Paso 100 Perdida 0.0008533281\n",
      "Paso 200 Perdida 0.00080181047\n",
      "Paso 300 Perdida 0.0008053404\n",
      "Paso 400 Perdida 0.0006974265\n",
      "Paso 500 Perdida 0.000866897\n",
      "Paso 600 Perdida 0.0007898709\n",
      "Nueva data, epoca 60\n",
      "Paso 0 Perdida 0.29175872\n",
      "Paso 100 Perdida 0.00081536477\n",
      "Paso 200 Perdida 0.00071520545\n",
      "Paso 300 Perdida 0.0007999534\n",
      "Paso 400 Perdida 0.0007332263\n",
      "Paso 500 Perdida 0.00080078177\n",
      "Paso 600 Perdida 0.00070830993\n",
      "Nueva data, epoca 61\n",
      "Paso 0 Perdida 0.23388164\n",
      "Paso 100 Perdida 0.0010085846\n",
      "Paso 200 Perdida 0.0007796501\n",
      "Paso 300 Perdida 0.0007455786\n",
      "Paso 400 Perdida 0.00073428015\n",
      "Paso 500 Perdida 0.000768324\n",
      "Paso 600 Perdida 0.00068489724\n",
      "Nueva data, epoca 62\n",
      "Paso 0 Perdida 0.47594813\n",
      "Paso 100 Perdida 0.0011261306\n",
      "Paso 200 Perdida 0.0010479699\n",
      "Paso 300 Perdida 0.0008427304\n",
      "Paso 400 Perdida 0.0009075462\n",
      "Paso 500 Perdida 0.0007455155\n",
      "Paso 600 Perdida 0.00083584624\n",
      "Nueva data, epoca 63\n",
      "Paso 0 Perdida 0.3485444\n",
      "Paso 100 Perdida 0.0008837637\n",
      "Paso 200 Perdida 0.00093504967\n",
      "Paso 300 Perdida 0.0008294381\n",
      "Paso 400 Perdida 0.0008598906\n",
      "Paso 500 Perdida 0.00070346997\n",
      "Paso 600 Perdida 0.0007699015\n",
      "Nueva data, epoca 64\n",
      "Paso 0 Perdida 0.20490088\n",
      "Paso 100 Perdida 0.00075752946\n",
      "Paso 200 Perdida 0.00068455795\n",
      "Paso 300 Perdida 0.00065338786\n",
      "Paso 400 Perdida 0.0006641407\n",
      "Paso 500 Perdida 0.00067791634\n",
      "Paso 600 Perdida 0.00071232853\n",
      "Nueva data, epoca 65\n",
      "Paso 0 Perdida 0.28294095\n",
      "Paso 100 Perdida 0.00063857116\n",
      "Paso 200 Perdida 0.00069660676\n",
      "Paso 300 Perdida 0.0007110589\n",
      "Paso 400 Perdida 0.00059439254\n",
      "Paso 500 Perdida 0.0006834788\n",
      "Paso 600 Perdida 0.0006255436\n",
      "Nueva data, epoca 66\n",
      "Paso 0 Perdida 0.23458442\n",
      "Paso 100 Perdida 0.0007222417\n",
      "Paso 200 Perdida 0.00061413203\n",
      "Paso 300 Perdida 0.000618109\n",
      "Paso 400 Perdida 0.0005903728\n",
      "Paso 500 Perdida 0.00064760825\n",
      "Paso 600 Perdida 0.0005602317\n",
      "Nueva data, epoca 67\n",
      "Paso 0 Perdida 0.30186436\n",
      "Paso 100 Perdida 0.0007317769\n",
      "Paso 200 Perdida 0.0006241417\n",
      "Paso 300 Perdida 0.00062030443\n",
      "Paso 400 Perdida 0.00064959604\n",
      "Paso 500 Perdida 0.00066690403\n",
      "Paso 600 Perdida 0.00055972533\n",
      "Nueva data, epoca 68\n",
      "Paso 0 Perdida 0.2641477\n",
      "Paso 100 Perdida 0.000561386\n",
      "Paso 200 Perdida 0.00070859137\n",
      "Paso 300 Perdida 0.00060276594\n",
      "Paso 400 Perdida 0.00048288633\n",
      "Paso 500 Perdida 0.0006237672\n",
      "Paso 600 Perdida 0.0005242483\n",
      "Nueva data, epoca 69\n",
      "Paso 0 Perdida 0.32592118\n",
      "Paso 100 Perdida 0.0005118375\n",
      "Paso 200 Perdida 0.00064989435\n",
      "Paso 300 Perdida 0.0005600673\n",
      "Paso 400 Perdida 0.0005539159\n",
      "Paso 500 Perdida 0.0005286723\n",
      "Paso 600 Perdida 0.0005527252\n",
      "Nueva data, epoca 70\n",
      "Paso 0 Perdida 0.27108964\n",
      "Paso 100 Perdida 0.00050104386\n",
      "Paso 200 Perdida 0.0005356802\n",
      "Paso 300 Perdida 0.0005894482\n",
      "Paso 400 Perdida 0.0006356032\n",
      "Paso 500 Perdida 0.00061578845\n",
      "Paso 600 Perdida 0.0004947929\n",
      "Nueva data, epoca 71\n",
      "Paso 0 Perdida 0.27494735\n",
      "Paso 100 Perdida 0.0005041372\n",
      "Paso 200 Perdida 0.00056871364\n",
      "Paso 300 Perdida 0.00053430063\n",
      "Paso 400 Perdida 0.00050122157\n",
      "Paso 500 Perdida 0.00057340245\n",
      "Paso 600 Perdida 0.00051126134\n",
      "Nueva data, epoca 72\n",
      "Paso 0 Perdida 0.27806488\n",
      "Paso 100 Perdida 0.00046381095\n",
      "Paso 200 Perdida 0.00050863065\n",
      "Paso 300 Perdida 0.00053269806\n",
      "Paso 400 Perdida 0.0004240353\n",
      "Paso 500 Perdida 0.00047611285\n",
      "Paso 600 Perdida 0.00049477955\n",
      "Nueva data, epoca 73\n",
      "Paso 0 Perdida 0.08524108\n",
      "Paso 100 Perdida 0.00045681838\n",
      "Paso 200 Perdida 0.00040965315\n",
      "Paso 300 Perdida 0.00046825895\n",
      "Paso 400 Perdida 0.00045636133\n",
      "Paso 500 Perdida 0.0004746616\n",
      "Paso 600 Perdida 0.00055812625\n",
      "Nueva data, epoca 74\n",
      "Paso 0 Perdida 0.29924053\n",
      "Paso 100 Perdida 0.0004031256\n",
      "Paso 200 Perdida 0.00047009095\n",
      "Paso 300 Perdida 0.000521381\n",
      "Paso 400 Perdida 0.00040243092\n",
      "Paso 500 Perdida 0.0004401188\n",
      "Paso 600 Perdida 0.00042090242\n",
      "Nueva data, epoca 75\n",
      "Paso 0 Perdida 0.20199367\n",
      "Paso 100 Perdida 0.00043547363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 200 Perdida 0.00044073383\n",
      "Paso 300 Perdida 0.00043541868\n",
      "Paso 400 Perdida 0.00048288886\n",
      "Paso 500 Perdida 0.0004237451\n",
      "Paso 600 Perdida 0.00042717945\n",
      "Nueva data, epoca 76\n",
      "Paso 0 Perdida 0.22697575\n",
      "Paso 100 Perdida 0.0004165829\n",
      "Paso 200 Perdida 0.00045084083\n",
      "Paso 300 Perdida 0.00045642193\n",
      "Paso 400 Perdida 0.0004444168\n",
      "Paso 500 Perdida 0.00047798504\n",
      "Paso 600 Perdida 0.00040807147\n",
      "Nueva data, epoca 77\n",
      "Paso 0 Perdida 0.14426617\n",
      "Paso 100 Perdida 0.000533844\n",
      "Paso 200 Perdida 0.00041615334\n",
      "Paso 300 Perdida 0.00045956214\n",
      "Paso 400 Perdida 0.00044448968\n",
      "Paso 500 Perdida 0.0004699735\n",
      "Paso 600 Perdida 0.00038268947\n",
      "Nueva data, epoca 78\n",
      "Paso 0 Perdida 0.2937089\n",
      "Paso 100 Perdida 0.00040170818\n",
      "Paso 200 Perdida 0.0004605206\n",
      "Paso 300 Perdida 0.00062933983\n",
      "Paso 400 Perdida 0.00036203506\n",
      "Paso 500 Perdida 0.00047860295\n",
      "Paso 600 Perdida 0.00043082263\n",
      "Nueva data, epoca 79\n",
      "Paso 0 Perdida 0.24287036\n",
      "Paso 100 Perdida 0.00045700377\n",
      "Paso 200 Perdida 0.0004999677\n",
      "Paso 300 Perdida 0.00038992104\n",
      "Paso 400 Perdida 0.00044895342\n",
      "Paso 500 Perdida 0.0022158392\n",
      "Paso 600 Perdida 0.001385718\n",
      "Nueva data, epoca 80\n",
      "Paso 0 Perdida 0.36796418\n",
      "Paso 100 Perdida 0.0005834448\n",
      "Paso 200 Perdida 0.0010230815\n",
      "Paso 300 Perdida 0.0008252437\n",
      "Paso 400 Perdida 0.00086163095\n",
      "Paso 500 Perdida 0.0006535156\n",
      "Paso 600 Perdida 0.0004861626\n",
      "Nueva data, epoca 81\n",
      "Paso 0 Perdida 0.26825002\n",
      "Paso 100 Perdida 0.0006519995\n",
      "Paso 200 Perdida 0.00055669853\n",
      "Paso 300 Perdida 0.00060537213\n",
      "Paso 400 Perdida 0.00055878185\n",
      "Paso 500 Perdida 0.0005867363\n",
      "Paso 600 Perdida 0.00049659185\n",
      "Nueva data, epoca 82\n",
      "Paso 0 Perdida 0.30443975\n",
      "Paso 100 Perdida 0.00047500475\n",
      "Paso 200 Perdida 0.000627527\n",
      "Paso 300 Perdida 0.0006551412\n",
      "Paso 400 Perdida 0.00043389757\n",
      "Paso 500 Perdida 0.0005216982\n",
      "Paso 600 Perdida 0.00045808012\n",
      "Nueva data, epoca 83\n",
      "Paso 0 Perdida 0.37908444\n",
      "Paso 100 Perdida 0.0005154905\n",
      "Paso 200 Perdida 0.0005112333\n",
      "Paso 300 Perdida 0.0006044517\n",
      "Paso 400 Perdida 0.00052354945\n",
      "Paso 500 Perdida 0.00045024877\n",
      "Paso 600 Perdida 0.0004278867\n",
      "Nueva data, epoca 84\n",
      "Paso 0 Perdida 0.34745649\n",
      "Paso 100 Perdida 0.00047631806\n",
      "Paso 200 Perdida 0.0005101605\n",
      "Paso 300 Perdida 0.00044081846\n",
      "Paso 400 Perdida 0.00048028032\n",
      "Paso 500 Perdida 0.00043230734\n",
      "Paso 600 Perdida 0.0004147362\n",
      "Nueva data, epoca 85\n",
      "Paso 0 Perdida 0.2538107\n",
      "Paso 100 Perdida 0.0005242224\n",
      "Paso 200 Perdida 0.00036647657\n",
      "Paso 300 Perdida 0.00043777714\n",
      "Paso 400 Perdida 0.00042576285\n",
      "Paso 500 Perdida 0.00043450837\n",
      "Paso 600 Perdida 0.00040499275\n",
      "Nueva data, epoca 86\n",
      "Paso 0 Perdida 0.2362378\n",
      "Paso 100 Perdida 0.00043873658\n",
      "Paso 200 Perdida 0.0004195637\n",
      "Paso 300 Perdida 0.00035130075\n",
      "Paso 400 Perdida 0.0004301556\n",
      "Paso 500 Perdida 0.00040547378\n",
      "Paso 600 Perdida 0.00047696813\n",
      "Nueva data, epoca 87\n",
      "Paso 0 Perdida 0.17017487\n",
      "Paso 100 Perdida 0.0005020033\n",
      "Paso 200 Perdida 0.00045156197\n",
      "Paso 300 Perdida 0.00040574302\n",
      "Paso 400 Perdida 0.00039262054\n",
      "Paso 500 Perdida 0.00034546808\n",
      "Paso 600 Perdida 0.00039169184\n",
      "Nueva data, epoca 88\n",
      "Paso 0 Perdida 0.16374011\n",
      "Paso 100 Perdida 0.00052404084\n",
      "Paso 200 Perdida 0.0005223028\n",
      "Paso 300 Perdida 0.00029518435\n",
      "Paso 400 Perdida 0.0003850803\n",
      "Paso 500 Perdida 0.00042519128\n",
      "Paso 600 Perdida 0.00040182887\n",
      "Nueva data, epoca 89\n",
      "Paso 0 Perdida 0.39183396\n",
      "Paso 100 Perdida 0.0004320193\n",
      "Paso 200 Perdida 0.00035462578\n",
      "Paso 300 Perdida 0.00035399199\n",
      "Paso 400 Perdida 0.0003644708\n",
      "Paso 500 Perdida 0.00032742758\n",
      "Paso 600 Perdida 0.0004317149\n",
      "Nueva data, epoca 90\n",
      "Paso 0 Perdida 0.2830815\n",
      "Paso 100 Perdida 0.00032243284\n",
      "Paso 200 Perdida 0.00037531156\n",
      "Paso 300 Perdida 0.00034027663\n",
      "Paso 400 Perdida 0.0003972749\n",
      "Paso 500 Perdida 0.00032013597\n",
      "Paso 600 Perdida 0.0003062103\n",
      "Nueva data, epoca 91\n",
      "Paso 0 Perdida 0.14379524\n",
      "Paso 100 Perdida 0.0002880942\n",
      "Paso 200 Perdida 0.00037081944\n",
      "Paso 300 Perdida 0.00040412016\n",
      "Paso 400 Perdida 0.0003192985\n",
      "Paso 500 Perdida 0.0003266386\n",
      "Paso 600 Perdida 0.00029155606\n",
      "Nueva data, epoca 92\n",
      "Paso 0 Perdida 0.21013819\n",
      "Paso 100 Perdida 0.00039712669\n",
      "Paso 200 Perdida 0.0004387278\n",
      "Paso 300 Perdida 0.00032165932\n",
      "Paso 400 Perdida 0.00033414067\n",
      "Paso 500 Perdida 0.0003014253\n",
      "Paso 600 Perdida 0.00038263222\n",
      "Nueva data, epoca 93\n",
      "Paso 0 Perdida 0.27987298\n",
      "Paso 100 Perdida 0.00034862064\n",
      "Paso 200 Perdida 0.0003220814\n",
      "Paso 300 Perdida 0.00031157068\n",
      "Paso 400 Perdida 0.00037526106\n",
      "Paso 500 Perdida 0.00030364047\n",
      "Paso 600 Perdida 0.00033992835\n",
      "Nueva data, epoca 94\n",
      "Paso 0 Perdida 0.1530091\n",
      "Paso 100 Perdida 0.0003596001\n",
      "Paso 200 Perdida 0.0003492511\n",
      "Paso 300 Perdida 0.00035772732\n",
      "Paso 400 Perdida 0.00035662818\n",
      "Paso 500 Perdida 0.00032395692\n",
      "Paso 600 Perdida 0.000341417\n",
      "Nueva data, epoca 95\n",
      "Paso 0 Perdida 0.2963922\n",
      "Paso 100 Perdida 0.00036738164\n",
      "Paso 200 Perdida 0.00033208873\n",
      "Paso 300 Perdida 0.0003623759\n",
      "Paso 400 Perdida 0.0003698852\n",
      "Paso 500 Perdida 0.0003775811\n",
      "Paso 600 Perdida 0.00032594398\n",
      "Nueva data, epoca 96\n",
      "Paso 0 Perdida 0.1876129\n",
      "Paso 100 Perdida 0.00037558388\n",
      "Paso 200 Perdida 0.00031551457\n",
      "Paso 300 Perdida 0.00031902638\n",
      "Paso 400 Perdida 0.00031472658\n",
      "Paso 500 Perdida 0.0003246375\n",
      "Paso 600 Perdida 0.00040645906\n",
      "Nueva data, epoca 97\n",
      "Paso 0 Perdida 0.31557074\n",
      "Paso 100 Perdida 0.0005089256\n",
      "Paso 200 Perdida 0.00042727127\n",
      "Paso 300 Perdida 0.00049918605\n",
      "Paso 400 Perdida 0.0004096438\n",
      "Paso 500 Perdida 0.00044218331\n",
      "Paso 600 Perdida 0.00036850892\n",
      "Nueva data, epoca 98\n",
      "Paso 0 Perdida 0.19861445\n",
      "Paso 100 Perdida 0.00036827804\n",
      "Paso 200 Perdida 0.00040493786\n",
      "Paso 300 Perdida 0.00032872843\n",
      "Paso 400 Perdida 0.00035059222\n",
      "Paso 500 Perdida 0.00035871693\n",
      "Paso 600 Perdida 0.00032156752\n",
      "Nueva data, epoca 99\n",
      "Paso 0 Perdida 0.21182083\n",
      "Paso 100 Perdida 0.00035417342\n",
      "Paso 200 Perdida 0.00042803728\n",
      "Paso 300 Perdida 0.00032942608\n",
      "Paso 400 Perdida 0.0003391895\n",
      "Paso 500 Perdida 0.00034731344\n",
      "Paso 600 Perdida 0.00036058115\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    lista_perdidas = []\n",
    "\n",
    "    for epoca_idx in range(num_epocas):\n",
    "        x,y = generateData()\n",
    "        _estado_actual = np.zeros((tamaño_lote, tamaño_estado))\n",
    "\n",
    "        print(\"Nueva data, epoca\", epoca_idx)\n",
    "\n",
    "        for lote_idx in range(num_lotes):\n",
    "            inicio_idx = lote_idx * longitud_truncada_backprop\n",
    "            fin_idx = inicio_idx + longitud_truncada_backprop\n",
    "\n",
    "            loteX = x[:,inicio_idx:fin_idx]\n",
    "            loteY = y[:,inicio_idx:fin_idx]\n",
    "\n",
    "            _perdida_total, _paso_entrenamiento, _estado_actual, _predicciones_series = sess.run(\n",
    "                [perdida_total, paso_entrenamiento, estado_actual, predicciones_series],\n",
    "                feed_dict={\n",
    "                    loteX_placeholder:loteX,\n",
    "                    loteY_placeholder:loteY,\n",
    "                    iniciar_estado:_estado_actual\n",
    "                })\n",
    "\n",
    "            lista_perdidas.append(_perdida_total)\n",
    "\n",
    "            if lote_idx%100 == 0:\n",
    "                print(\"Paso\", lote_idx, \"Perdida\", _perdida_total)\n",
    "                plot(lista_perdidas, _predicciones_series, loteX, loteY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figuras/Eco_RNN.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que estamos moviendo `longitud_truncada_backprop` pasos adelante en cada iteración (línea 15-19), pero es posible tener diferentes pasos.  La desventaja de hacer esto es que `longitud_truncada_backprop` necesita ser significativamente más grande que las dependencias de tiempo (tres pasos en nuestro caso) para encapsular los datos de entrenamiento relevantes. De lo contrario, podría haber muchas \"fallas\", como puede ver en la figura siguiente.\n",
    "\n",
    "<img src=\"figuras/serie_tiempo.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [How to build a Recurrent Neural Network in TensorFlow](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
