{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Transferencia (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea general del aprendizaje por transferencia es usar el conocimiento aprendido de las tareas para las cuales hay una gran cantidad de datos etiquetados disponibles en entornos donde solo hay pocos datos etiquetados disponibles. La creación de datos etiquetados es costosa, por lo que es iportante aprovechar al máximo los conjuntos de datos existentes.\n",
    "\n",
    "En un modelo tradicional de aprendizaje automático, el objetivo principal es generalizar sobre datos aún no vistos, basado en patrones aprendidos de los datos de entrenamiento. Con el aprendizaje por transferencia, se intenta iniciar este proceso de generalización partiendo de patrones que se han aprendido para una tarea diferente. Esencialmente, en lugar de comenzar el proceso de aprendizaje desde una hoja en blanco (a menudo inicializada aleatoriamente), se parte de patrones que se han aprendido para resolver una tarea diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Aprendizaje por Transferencia con el Conjunto de Datos CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un conjunto de datos estandarizado llamado CIFAR-10. CIFAR-10 consta de 60000 imágenes. Hay 10 categorías diferentes y 6000 imágenes por categoría. Cada imagen tiene un tamaño de solo 32 por 32 píxeles.\n",
    "\n",
    "<img src=\"figuras/CIFAR-10_examples.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos Pre-entrenados en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos disponibles en Keras (https://keras.io/applications/):\n",
    "- Xception\n",
    "- VGG16\n",
    "- VGG19\n",
    "- ResNet50\n",
    "- InceptionV3\n",
    "- InceptionResNetV2\n",
    "- MobileNet\n",
    "- DenseNet\n",
    "- NASNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo utilizaremos el modelo InceptionV3:\n",
    "    \n",
    " <img src=\"figuras/inception.png\" width=\"100%\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar keras, CIFAR-10 e InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wladimir/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import scipy\n",
    "from scipy import misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar los datos de CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datos cargados\n"
     ]
    }
   ],
   "source": [
    "# cargar los datos\n",
    "(x_entrenamiento, y_entrenamiento), (x_prueba, y_prueba) = cifar10.load_data()\n",
    "y_entrenamiento = np.squeeze(y_entrenamiento)\n",
    "print('datos cargados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el modelo InceptionV3 y remover las capas finales de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo cargado\n"
     ]
    }
   ],
   "source": [
    "# cargar modelo inceptionV3\n",
    "modelo = InceptionV3(weights='imagenet', include_top=False, input_shape=(139, 139, 3))\n",
    "print('modelo cargado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener los atributos del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este nuevo modelo ya no devolverá la predicción de la clase de la imagen, ya que la capa de clasificación se ha eliminado; sin embargo, la Red Neuronal Convolucional ahora almacenada en el modelo todavía nos proporciona una forma útil de extraer los atributos de las imágenes. Al pasar cada una de las imágenes CIFAR-10 a través de este modelo, podemos convertir cada imagen de su matriz de 32x32x3 de píxeles a un vector con 2048 entradas. En la práctica, nos referimos a este conjunto de datos de puntos de 2048 dimensiones como los atributos de cuello de botella de InceptionV3.\n",
    "\n",
    "Los atributos se almacerarán en un archivo de manera que subsiguientes corridas del código no tenga que volverlos a procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargar atributos del archivo de atributos existente (entrenamiento)\n",
      "atributos almacenados (entrenamiento)\n"
     ]
    }
   ],
   "source": [
    "# obtener atributos (entrenamiento)\n",
    "if os.path.exists('inception_atributos_entrenamiento.npz'):\n",
    "    print('cargar atributos del archivo de atributos existente (entrenamiento)')\n",
    "    atributos = np.load('inception_atributos_entrenamiento.npz')['atributos']\n",
    "else:\n",
    "    print('no hay archivo de atributos existente (entrenamiento)')\n",
    "    print('calculando ...')\n",
    "    # preprocesar los datos de entrenamiento\n",
    "    grande_x_entrenamiento = np.array([scipy.misc.imresize(x_entrenamiento[i], (139, 139, 3)) \n",
    "                            for i in range(0, len(x_entrenamiento))]).astype('float32')\n",
    "    inception_entrada_entrenamiento = preprocess_input(grande_x_entrenamiento)\n",
    "    print('datos de entrenamiento preprocesados')\n",
    "    # extraer, procesar, y almacenar los atributos\n",
    "    atributos = modelo.predict(inception_entrada_entrenamiento)\n",
    "    atributos = np.squeeze(atributos)\n",
    "    np.savez('inception_atributos_entrenamiento', atributos=atributos)\n",
    "print('atributos almacenados (entrenamiento)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener los atributos del conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se repite el mismo procedimiento para los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargar atributos del archivo de atributos existente (prueba)\n",
      "atributos almacenados (entrenamiento)\n"
     ]
    }
   ],
   "source": [
    "# obtener atributos (prueba)\n",
    "if os.path.exists('inception_atributos_prueba.npz'):\n",
    "    print('cargar atributos del archivo de atributos existente (prueba)')\n",
    "    atributos_prueba = np.load('inception_atributos_prueba.npz')['atributos_prueba']\n",
    "else:\n",
    "    print('no hay archivo de atributos existente (prueba)')\n",
    "    print('calculando ...')\n",
    "    # preprocesar los datos de prueba\n",
    "    grande_x_prueba = np.array([scipy.misc.imresize(x_prueba[i], (139, 139, 3)) \n",
    "                            for i in range(0, len(x_prueba))]).astype('float32')\n",
    "    inception_entrada_prueba = preprocess_input(grande_x_prueba)\n",
    "    print('datos de prueba preprocesados')\n",
    "    # extraer, procesar, y almacenar los atributos (prueba)\n",
    "    atributos_prueba = modelo.predict(inception_entrada_prueba)\n",
    "    atributos_prueba = np.squeeze(atributos_prueba)\n",
    "    np.savez('inception_atributos_prueba', atributos_prueba=atributos_prueba)\n",
    "print('atributos almacenados (prueba)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar una Red Neuronal sencilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificar etiquetas de clases usando codificación one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_entrenamiento = np_utils.to_categorical(y_entrenamiento, 10)\n",
    "y_prueba = np_utils.to_categorical(y_prueba, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear Red Neuronal usando Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wladimir/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_95 (Conv2D)           (None, 2, 2, 100)         819300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 100)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 820,310\n",
      "Trainable params: 820,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, GlobalAveragePooling2D\n",
    "\n",
    "modelo = Sequential()\n",
    "modelo.add(Conv2D(filters=100, kernel_size=2, input_shape=atributos.shape[1:]))\n",
    "modelo.add(Dropout(0.4))\n",
    "modelo.add(GlobalAveragePooling2D())\n",
    "modelo.add(Dropout(0.3))\n",
    "modelo.add(Dense(10, activation='softmax'))\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilar y entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wladimir/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "Epoch 00001: val_loss improved from inf to 3.23936, saving model to modelo.mejor.hdf5\n",
      " - 47s - loss: 4.3853 - acc: 0.6828 - val_loss: 3.2394 - val_acc: 0.7695\n",
      "Epoch 2/50\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 33s - loss: 3.3390 - acc: 0.7657 - val_loss: 3.3096 - val_acc: 0.7709\n",
      "Epoch 3/50\n",
      "Epoch 00003: val_loss improved from 3.23936 to 2.74615, saving model to modelo.mejor.hdf5\n",
      " - 31s - loss: 3.2314 - acc: 0.7784 - val_loss: 2.7461 - val_acc: 0.8093\n",
      "Epoch 4/50\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 21s - loss: 3.0612 - acc: 0.7905 - val_loss: 3.0611 - val_acc: 0.7947\n",
      "Epoch 5/50\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 3.0647 - acc: 0.7939 - val_loss: 3.0429 - val_acc: 0.7971\n",
      "Epoch 6/50\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 21s - loss: 2.9063 - acc: 0.8049 - val_loss: 3.0748 - val_acc: 0.7956\n",
      "Epoch 7/50\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 20s - loss: 2.9077 - acc: 0.8050 - val_loss: 2.8267 - val_acc: 0.8121\n",
      "Epoch 8/50\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 20s - loss: 2.8492 - acc: 0.8100 - val_loss: 3.1610 - val_acc: 0.7908\n",
      "Epoch 9/50\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 20s - loss: 2.8941 - acc: 0.8089 - val_loss: 3.1286 - val_acc: 0.7938\n",
      "Epoch 10/50\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 20s - loss: 2.7856 - acc: 0.8152 - val_loss: 2.8925 - val_acc: 0.8077\n",
      "Epoch 11/50\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 20s - loss: 2.7947 - acc: 0.8151 - val_loss: 2.8203 - val_acc: 0.8143\n",
      "Epoch 12/50\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 20s - loss: 2.7211 - acc: 0.8205 - val_loss: 2.8300 - val_acc: 0.8122\n",
      "Epoch 13/50\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 20s - loss: 2.7742 - acc: 0.8176 - val_loss: 2.8681 - val_acc: 0.8123\n",
      "Epoch 14/50\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 20s - loss: 2.8052 - acc: 0.8157 - val_loss: 2.7642 - val_acc: 0.8192\n",
      "Epoch 15/50\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 20s - loss: 2.7266 - acc: 0.8212 - val_loss: 2.7735 - val_acc: 0.8193\n",
      "Epoch 16/50\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 20s - loss: 2.7125 - acc: 0.8231 - val_loss: 2.9453 - val_acc: 0.8101\n",
      "Epoch 17/50\n",
      "Epoch 00017: val_loss improved from 2.74615 to 2.72919, saving model to modelo.mejor.hdf5\n",
      " - 20s - loss: 2.6877 - acc: 0.8243 - val_loss: 2.7292 - val_acc: 0.8236\n",
      "Epoch 18/50\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 20s - loss: 2.6904 - acc: 0.8246 - val_loss: 2.9570 - val_acc: 0.8075\n",
      "Epoch 19/50\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 20s - loss: 2.6552 - acc: 0.8274 - val_loss: 2.8561 - val_acc: 0.8143\n",
      "Epoch 20/50\n",
      "Epoch 00020: val_loss improved from 2.72919 to 2.71950, saving model to modelo.mejor.hdf5\n",
      " - 20s - loss: 2.6447 - acc: 0.8288 - val_loss: 2.7195 - val_acc: 0.8238\n",
      "Epoch 21/50\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 20s - loss: 2.6270 - acc: 0.8294 - val_loss: 3.0318 - val_acc: 0.8027\n",
      "Epoch 22/50\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 20s - loss: 2.6423 - acc: 0.8284 - val_loss: 2.9971 - val_acc: 0.8052\n",
      "Epoch 23/50\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 20s - loss: 2.6039 - acc: 0.8312 - val_loss: 2.7719 - val_acc: 0.8202\n",
      "Epoch 24/50\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 20s - loss: 2.6173 - acc: 0.8311 - val_loss: 2.8521 - val_acc: 0.8149\n",
      "Epoch 25/50\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 20s - loss: 2.6195 - acc: 0.8308 - val_loss: 2.8911 - val_acc: 0.8138\n",
      "Epoch 26/50\n",
      "Epoch 00026: val_loss improved from 2.71950 to 2.71810, saving model to modelo.mejor.hdf5\n",
      " - 20s - loss: 2.5972 - acc: 0.8323 - val_loss: 2.7181 - val_acc: 0.8249\n",
      "Epoch 27/50\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 20s - loss: 2.5503 - acc: 0.8356 - val_loss: 2.9578 - val_acc: 0.8089\n",
      "Epoch 28/50\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 20s - loss: 2.5513 - acc: 0.8349 - val_loss: 2.8111 - val_acc: 0.8184\n",
      "Epoch 29/50\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 20s - loss: 2.5734 - acc: 0.8342 - val_loss: 2.7762 - val_acc: 0.8204\n",
      "Epoch 30/50\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 20s - loss: 2.5420 - acc: 0.8364 - val_loss: 3.0592 - val_acc: 0.8046\n",
      "Epoch 31/50\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 20s - loss: 2.5993 - acc: 0.8326 - val_loss: 2.9139 - val_acc: 0.8125\n",
      "Epoch 32/50\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 20s - loss: 2.5806 - acc: 0.8337 - val_loss: 2.7932 - val_acc: 0.8203\n",
      "Epoch 33/50\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 20s - loss: 2.4915 - acc: 0.8400 - val_loss: 2.7864 - val_acc: 0.8218\n",
      "Epoch 34/50\n",
      "Epoch 00034: val_loss improved from 2.71810 to 2.68224, saving model to modelo.mejor.hdf5\n",
      " - 20s - loss: 2.5132 - acc: 0.8378 - val_loss: 2.6822 - val_acc: 0.8286\n",
      "Epoch 35/50\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 20s - loss: 2.5509 - acc: 0.8360 - val_loss: 2.7404 - val_acc: 0.8231\n",
      "Epoch 36/50\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 20s - loss: 2.5381 - acc: 0.8368 - val_loss: 2.7894 - val_acc: 0.8215\n",
      "Epoch 37/50\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 20s - loss: 2.5750 - acc: 0.8352 - val_loss: 2.7172 - val_acc: 0.8257\n",
      "Epoch 38/50\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 20s - loss: 2.5717 - acc: 0.8353 - val_loss: 2.7578 - val_acc: 0.8225\n",
      "Epoch 39/50\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 20s - loss: 2.5887 - acc: 0.8341 - val_loss: 2.7874 - val_acc: 0.8212\n",
      "Epoch 40/50\n",
      "Epoch 00040: val_loss improved from 2.68224 to 2.67385, saving model to modelo.mejor.hdf5\n",
      " - 20s - loss: 2.5214 - acc: 0.8385 - val_loss: 2.6738 - val_acc: 0.8285\n",
      "Epoch 41/50\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 20s - loss: 2.5109 - acc: 0.8389 - val_loss: 2.9200 - val_acc: 0.8146\n",
      "Epoch 42/50\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 20s - loss: 2.5727 - acc: 0.8356 - val_loss: 3.0116 - val_acc: 0.8087\n",
      "Epoch 43/50\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 20s - loss: 2.4954 - acc: 0.8405 - val_loss: 2.8549 - val_acc: 0.8177\n",
      "Epoch 44/50\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 20s - loss: 2.5125 - acc: 0.8392 - val_loss: 3.1934 - val_acc: 0.7962\n",
      "Epoch 45/50\n",
      "Epoch 00045: val_loss did not improve\n",
      " - 20s - loss: 2.5574 - acc: 0.8366 - val_loss: 2.9212 - val_acc: 0.8130\n",
      "Epoch 46/50\n",
      "Epoch 00046: val_loss did not improve\n",
      " - 20s - loss: 2.4938 - acc: 0.8409 - val_loss: 2.8640 - val_acc: 0.8182\n",
      "Epoch 47/50\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 20s - loss: 2.4648 - acc: 0.8425 - val_loss: 2.7924 - val_acc: 0.8219\n",
      "Epoch 48/50\n",
      "Epoch 00048: val_loss did not improve\n",
      " - 20s - loss: 2.4730 - acc: 0.8421 - val_loss: 2.7942 - val_acc: 0.8228\n",
      "Epoch 49/50\n",
      "Epoch 00049: val_loss did not improve\n",
      " - 20s - loss: 2.4982 - acc: 0.8411 - val_loss: 2.7264 - val_acc: 0.8266\n",
      "Epoch 50/50\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 20s - loss: 2.4630 - acc: 0.8428 - val_loss: 2.7895 - val_acc: 0.8236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a166d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='modelo.mejor.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "modelo.fit(atributos, y_entrenamiento, batch_size=50, epochs=50,\n",
    "          validation_split=0.2, callbacks=[checkpointer],\n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exactitud del modelo con el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud prueba: 82.4600%\n"
     ]
    }
   ],
   "source": [
    "# cargar los pesos que generaron la mejor exactitud sobre el conjunto de validación \n",
    "modelo.load_weights('modelo.mejor.hdf5')\n",
    "\n",
    "# evaluar la exactitud sobre conjunto de prueba\n",
    "score = modelo.evaluate(atributos_prueba, y_prueba, verbose=0)\n",
    "exactitud = 100 * score[1]\n",
    "\n",
    "# imprimir exactitud del conjunto de prueba\n",
    "print('Exactitud prueba: %.4f%%' % exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
